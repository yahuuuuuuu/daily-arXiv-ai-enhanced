{"id": "2511.21581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21581", "abs": "https://arxiv.org/abs/2511.21581", "authors": ["Alex Ning", "Yen-Ling Kuo", "Gabe Gomes"], "title": "Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning", "comment": "13 pages, 6 figures", "summary": "Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u957f\u5ea6\u6f5c\u5728\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u603b\u63a8\u7406\u957f\u5ea6\u964d\u4f4e52%", "motivation": "\u6f5c\u5728\u63a8\u7406\u76f8\u6bd4\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5177\u6709\u538b\u7f29\u63a8\u7406\u957f\u5ea6\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u76f4\u63a5\u4f20\u9012\u4fe1\u606f\u4e30\u5bcc\u7684\u6f5c\u5728\u72b6\u6001\u6765\u7a81\u7834\u4eba\u7c7b\u8bed\u8a00\u6807\u8bb0\u4f5c\u4e3a\u63a8\u7406\u5a92\u4ecb\u7684\u9650\u5236", "method": "\u5f00\u53d1\u81ea\u9002\u5e94\u957f\u5ea6\u6f5c\u5728\u63a8\u7406\u6a21\u578b\uff0c\u5f15\u5165\u540eSFT\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u63a8\u7406\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u6765\u4f18\u5316\u6f5c\u5728\u63a8\u7406\u957f\u5ea6", "result": "\u5728Llama 3.2 1B\u6a21\u578b\u548cGSM8K-Aug\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u603b\u63a8\u7406\u957f\u5ea6\u4e0b\u964d52%\u4e14\u51c6\u786e\u7387\u6ca1\u6709\u635f\u5931", "conclusion": "\u6f5c\u5728\u63a8\u7406\u6a21\u578b\u5728\u8ba1\u7b97\u4f7f\u7528\u548c\u538b\u7f29\u80fd\u529b\u65b9\u9762\u63d0\u5347\u4e86\u6807\u51c6\uff0c\u672a\u6765\u8ba1\u5212\u6269\u5c55\u5230\u66f4\u591a\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u5206\u6790\u8bad\u7ec3\u7cfb\u6570\u5173\u7cfb\uff0c\u5b9e\u9a8c\u67b6\u6784\u53d8\u4f53\uff0c\u5e76\u7ee7\u7eed\u77e5\u8bc6\u84b8\u998f\u5de5\u4f5c"}}
{"id": "2511.21594", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21594", "abs": "https://arxiv.org/abs/2511.21594", "authors": ["Alex Ning", "Vainateya Rangaraju"], "title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction", "comment": "24 pages, 16 figures", "summary": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.", "AI": {"tldr": "\u901a\u8fc7\u964d\u7ef4\u6280\u672f\u63d0\u53d6\u548c\u53ef\u89c6\u5316Transformer\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u7ec4\u4ef6\u5728\u4e2d\u95f4\u5c42\u5b58\u5728\u660e\u663e\u5206\u79bb\uff0c\u5e76\u63ed\u793a\u4e86\u4f4d\u7f6e\u5d4c\u5165\u7684\u9ad8\u7ef4\u87ba\u65cb\u7ed3\u6784\u7b49\u51e0\u4f55\u6a21\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6210\u679c\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u96be\u4ee5\u89e3\u91ca\uff0c\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u6027\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u65b9\u6cd5\u6765\u7406\u89e3Transformer\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u5747\u5300\u6d41\u5f62\u8fd1\u4f3c\uff08UMAP\uff09\u7b49\u964d\u7ef4\u6280\u672f\uff0c\u5728GPT-2\u548cLLaMa\u6a21\u578b\u7684Transformer\u5757\u4e2d\u6355\u83b7\u5c42\u95f4\u6fc0\u6d3b\u72b6\u6001\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u51e0\u4f55\u7ed3\u6784\u5206\u6790\u3002", "result": "\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u7ec4\u4ef6\u8f93\u51fa\u5728\u4e2d\u95f4\u5c42\u5b58\u5728\u660e\u663e\u5206\u79bb\uff0c\u8bc6\u522b\u4e86\u521d\u59cb\u5e8f\u5217\u4f4d\u7f6e\u6f5c\u5728\u72b6\u6001\u7684\u9ad8\u8303\u6570\u7279\u5f81\uff0c\u53ef\u89c6\u5316\u4e86\u4f4d\u7f6e\u5d4c\u5165\u7684\u9ad8\u7ef4\u87ba\u65cb\u7ed3\u6784\u4ee5\u53ca\u5e8f\u5217\u5c42\u9762\u7684\u51e0\u4f55\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aTransformer\u5185\u90e8\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u53ef\u590d\u73b0\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u4f7f\u7528\u3002"}}
