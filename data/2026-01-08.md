<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.LG](#cs.LG) [Total: 74]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [RelightAnyone: A Generalized Relightable 3D Gaussian Head Model](https://arxiv.org/abs/2601.03357)
*Yingyan Xu,Pramod Rao,Sebastian Weiss,Gaspard Zoss,Markus Gross,Christian Theobalt,Marc Habermann,Derek Bradley*

Main category: cs.CV

TL;DR: 提出一种新的可重光照3D高斯头部模型，能够从单张或多视角图像重建可重光照的头像，无需OLAT数据。采用两阶段设计：第一阶段从无OLAT数据中学习平光照3DGS头像，第二阶段学习到物理反射参数的映射。


<details>
  <summary>Details</summary>
Motivation: 现有高质量重光照方法需要复杂的时间复用光照数据（如OLAT），限制了应用范围。需要一种能从普通单/多视角图像重建可重光照头像的通用方法。

Method: 两阶段方法：1) 从无OLAT的多视角数据集学习平光照3DGS头像，使用数据集特定光照码进行自监督光照对齐；2) 在较小OLAT数据集上学习从平光照头像到物理反射参数的映射。

Result: 该方法能泛化到未见过的对象，仅需单张图像即可拟合，支持新颖视角合成和数字头像重光照应用。

Conclusion: 提出了一种无需OLAT数据的可重光照3D高斯头部模型，通过两阶段设计实现跨对象泛化，为数字头像重光照提供了更实用的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.

</details>


### [2] [Bayesian Monocular Depth Refinement via Neural Radiance Fields](https://arxiv.org/abs/2601.03869)
*Arun Muthukkumar*

Main category: cs.CV

TL;DR: MDENeRF：通过NeRF不确定性引导的贝叶斯融合，迭代优化单目深度估计，提升几何细节


<details>
  <summary>Details</summary>
Motivation: 当前单目深度估计方法生成的深度图通常过于平滑，缺乏精细的几何细节，这限制了在自动驾驶、扩展现实等应用中的准确场景理解能力

Method: 提出MDENeRF框架，包含三个组件：1) 初始单目深度估计提供全局结构；2) 在扰动视角下训练的NeRF，具有逐像素不确定性；3) 通过贝叶斯融合将噪声单目深度和NeRF深度结合，利用体积渲染过程推导NeRF不确定性来迭代注入高频细节

Result: 在SUN RGB-D室内场景数据集上，在关键指标和实验中表现出优越性能

Conclusion: MDENeRF通过结合单目深度估计的全局结构先验和NeRF的精细几何细节，能够生成更准确、细节更丰富的深度图，为场景理解任务提供了更好的解决方案

Abstract: Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.

</details>


### [3] [Choreographing a World of Dynamic Objects](https://arxiv.org/abs/2601.04194)
*Yanzhe Lyu,Chen Geng,Karthik Dharmarajan,Yunzhi Zhang,Hadi Alzayer,Shangzhe Wu,Jiajun Wu*

Main category: cs.CV

TL;DR: CHORD：基于视频生成模型的通用4D动态场景生成框架，通过蒸馏方法从2D视频中提取拉格朗日运动信息，实现多物体动态场景的生成


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态物体不断演化、变形和交互，传统基于规则的图形流水线需要类别特定启发式且劳动密集，而基于学习的方法需要大规模数据集且难以覆盖所有感兴趣类别

Method: 提出基于蒸馏的通用生成流水线，从2D视频的欧拉表示中提取隐藏的拉格朗日运动信息，实现类别无关的4D动态场景生成

Result: 能够生成多样化的多体4D动态场景，相比现有方法具有优势，并展示了在机器人操作策略生成中的应用

Conclusion: CHORD提供了一种通用、多功能、类别无关的方法来生成4D动态场景，继承了视频生成模型的普适性，避免了传统方法的局限性

Abstract: Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord

</details>


### [4] [HyperCLOVA X 32B Think](https://arxiv.org/abs/2601.03286)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CV

TL;DR: HyperCLOVA X 32B Think 是一个专注于韩语语言文化背景下推理能力和智能体能力的视觉语言模型，在韩文文本和视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对韩语语言文化背景的视觉语言模型，特别强调推理能力和智能体能力，以支持更广泛的研究和应用。

Method: 模型采用预训练和后训练两阶段方法：首先预训练以增强推理能力，然后后训练支持多模态理解、增强推理、智能体行为和人类偏好对齐。

Result: 在相似规模模型中，该模型在韩文文本到文本、视觉到文本基准测试以及智能体导向评估任务上表现出色。

Conclusion: 通过开源 HyperCLOVA X 32B Think，旨在促进更广泛的采用，并推动学术和工业界的进一步研究和创新。

Abstract: In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.

</details>


### [5] [CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception](https://arxiv.org/abs/2601.03302)
*Mohammad Rostami,Atik Faysal,Hongtao Xia,Hadi Kasasbeh,Ziang Gao,Huaxia Wang*

Main category: cs.CV

TL;DR: CDRF是一个大规模RF无人机检测与识别基准数据集，结合真实采集数据和系统生成的合成变体，解决现有RF数据集稀缺和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RF无人机数据集存在稀缺性和多样性有限的问题，限制了RF感知模型的发展。需要大规模、多样化的基准数据集来推动鲁棒、可泛化的RF感知模型研究。

Method: 1) 收集真实世界RF捕获数据（Rowan大学校园和受控RF笼设施）；2) 开发系统化增强流水线，包括精确控制SNR、注入干扰发射器、应用频率偏移和标签一致的边界框变换；3) 涵盖多种当代无人机模型；4) 提供开源工具用于数据生成、预处理、增强和评估。

Result: CDRF成为首个大规模RF无人机检测与识别基准，支持分类、开放集识别和目标检测任务的标准化评估。数据集与工具兼容现有公共基准，促进可重复的管道和严格比较。

Conclusion: 通过发布这一综合性基准和工具，CDRF旨在加速鲁棒、可泛化的RF感知模型的发展，为RF无人机检测研究提供标准化评估平台。

Abstract: We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.

</details>


### [6] [Mass Concept Erasure in Diffusion Models with Concept Hierarchy](https://arxiv.org/abs/2601.03305)
*Jiahang Tu,Ye Li,Yiming Wu,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.CV

TL;DR: 提出基于超类型-子类型概念层次的概念擦除方法，通过分组抑制和SuPLoRA技术高效擦除多个相关概念，同时保持整体生成质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型概念擦除方法在处理多个概念时效率低下且效果不佳，每个概念需要单独微调参数，且可能降低整体生成质量

Method: 构建超类型-子类型概念层次结构，将语义相关概念分组；提出分组抑制方法，共享学习参数；设计SuPLoRA技术，在冻结下投影矩阵中编码超类型信息，仅更新上投影矩阵

Result: 理论分析证明SuPLoRA能有效缓解生成性能退化；构建了包含名人、物体、色情内容等多领域概念的同时擦除基准

Conclusion: 提出的概念层次结构和SuPLoRA方法能高效擦除多个相关概念，同时保持整体生成能力，解决了现有方法在多概念擦除时的效率和效果问题

Abstract: The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.

</details>


### [7] [VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models](https://arxiv.org/abs/2601.03309)
*Jianke Zhang,Xiaoyu Chen,Qiuyue Wang,Mingsheng Li,Yanjiang Guo,Yucheng Hu,Jiajun Zhang,Shuai Bai,Junyang Lin,Jianyu Chen*

Main category: cs.CV

TL;DR: VLM4VLA：一个最小化适应流程，将通用视觉语言模型转换为视觉语言动作策略，研究发现VLM的通用能力不能预测下游控制性能，视觉模块是主要瓶颈


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型的选择和能力如何影响下游视觉语言动作策略的性能，这是一个基础但很少被系统研究的问题

Method: 提出VLM4VLA最小适应流程，仅使用少量可学习参数将通用VLM转换为VLA策略，在三个基准测试的多个下游任务上进行实证研究，并分析具体体现能力的微调影响

Result: VLM初始化比从头训练有优势，但VLM的通用能力不能预测下游任务性能；提升特定体现技能不一定改善控制性能；视觉模块是主要瓶颈，向VLM视觉编码器注入控制相关监督能带来持续增益

Conclusion: 当前VLM预训练目标与体现动作规划需求存在持续领域差距，标准VLM能力是必要但不充分的，视觉编码器是改进VLA策略的关键瓶颈

Abstract: Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

</details>


### [8] [Deep Learning-Based Image Recognition for Soft-Shell Shrimp Classification](https://arxiv.org/abs/2601.03317)
*Yun-Hao Zhang,I-Hsien Ting,Dario Liberona,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.CV

TL;DR: 使用深度学习图像识别技术自动分类刚捕捞的白虾，提高分类准确性、效率和一致性，保持虾的新鲜度并改善产品外观。


<details>
  <summary>Details</summary>
Motivation: 随着信息技术融入水产养殖，生产更加稳定且持续增长。消费者对高品质水产品需求增加，新鲜度和外观完整性是关键关注点。虾类加工食品在捕捞后新鲜度迅速下降，软壳虾在烹饪或冷冻后常出现头身分离，影响产品外观和消费者感知。

Method: 利用基于深度学习的图像识别技术，使用卷积神经网络（CNN）模型替代人工分拣，实现白虾的自动分类。

Result: 通过减少处理时间，该技术有助于保持虾的新鲜度，并确保虾类运输企业更有效地满足客户需求。

Conclusion: 深度学习图像识别技术能够提高虾类分类的准确性、效率和一致性，有助于解决新鲜度下降和外观完整性问题，提升产品质量和消费者满意度。

Abstract: With the integration of information technology into aquaculture, production has become more stable and continues to grow annually. As consumer demand for high-quality aquatic products rises, freshness and appearance integrity are key concerns. In shrimp-based processed foods, freshness declines rapidly post-harvest, and soft-shell shrimp often suffer from head-body separation after cooking or freezing, affecting product appearance and consumer perception. To address these issues, this study leverages deep learning-based image recognition for automated classification of white shrimp immediately after harvest. A convolutional neural network (CNN) model replaces manual sorting, enhancing classification accuracy, efficiency, and consistency. By reducing processing time, this technology helps maintain freshness and ensures that shrimp transportation businesses meet customer demands more effectively.

</details>


### [9] [Higher order PCA-like rotation-invariant features for detailed shape descriptors modulo rotation](https://arxiv.org/abs/2601.03326)
*Jarek Duda*

Main category: cs.CV

TL;DR: 提出高阶矩张量作为PCA的扩展，用于更精确的旋转不变形状描述，应用于分子形状描述、物体识别和形状相似性度量


<details>
  <summary>Details</summary>
Motivation: PCA使用协方差矩阵（二阶矩）只能将形状近似为椭球体，但真实形状通常更复杂，需要更高阶的描述方法

Method: 扩展PCA到高阶中心矩张量（如三阶张量p_abc）或多项式乘以高斯函数，构建可解码的高精度形状描述符及其旋转不变量

Result: 能够创建任意高精度的旋转不变形状描述符，避免了传统方法中昂贵的旋转优化计算

Conclusion: 高阶矩张量方法为旋转不变形状分析提供了更精确、计算高效的解决方案，在多个领域具有实际应用价值

Abstract: PCA can be used for rotation invariant features, describing a shape with its $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ covariance matrix approximating shape by ellipsoid, allowing for rotation invariants like its traces of powers. However, real shapes are usually much more complicated, hence there is proposed its extension to e.g. $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$ order-3 or higher tensors describing central moments, or polynomial times Gaussian allowing decodable shape descriptors of arbitrarily high accuracy, and their analogous rotation invariants. Its practical applications could be rotation-invariant features to include shape modulo rotation e.g. for molecular shape descriptors, or for up to rotation object recognition in 2D images/3D scans, or shape similarity metric allowing their inexpensive comparison (modulo rotation) without costly optimization over rotations.

</details>


### [10] [MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.03331)
*Yang Shi,Yifeng Xie,Minzhe Guo,Liangsi Lu,Mingxuan Huang,Jingchao Wang,Zhihong Zhu,Boyan Xu,Zhiqi Huang*

Main category: cs.CV

TL;DR: MMErroR是一个多模态基准测试，包含2,013个嵌入单一连贯推理错误的样本，用于评估视觉语言模型检测错误推理和分类错误类型的能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在多模态学习中的性能提升，需要评估这些模型是否真正理解处理的内容，特别是能否检测推理过程中的错误并识别错误类型。

Method: 构建MMErroR基准测试，包含2,013个样本，涵盖6个顶级领域的24个子领域，每个样本嵌入单一连贯推理错误。评估20个先进视觉语言模型检测错误推理和分类错误类型的能力。

Result: 即使最佳模型(Gemini-3.0-Pro)也只能正确分类66.47%的错误，表明识别错误推理具有挑战性。错误识别能力为多模态推理模型的能力提供了有价值的见解。

Conclusion: MMErroR基准测试揭示了当前视觉语言模型在检测错误推理方面的局限性，为模型理解和评估提供了新的视角，表明需要进一步研究提升模型的过程级理解能力。

Abstract: Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io

</details>


### [11] [Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views](https://arxiv.org/abs/2601.03362)
*Xiang Zhang,Yang Zhang,Lukas Mehl,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: HairGuard是一个3D视觉框架，专门用于恢复软边界（如头发）的精细细节，通过深度修复网络和生成式场景绘制器，在单目深度估计、立体图像/视频转换和新视角合成等任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 软边界（如细发）在自然和计算机生成图像中常见，但由于前景和背景线索的模糊混合，在3D视觉中仍然具有挑战性。现有方法难以准确恢复这些精细的软边界细节。

Method: 1) 提出数据整理流程，利用图像抠图数据集进行训练；2) 设计深度修复网络，通过门控残差模块自动识别软边界区域并精确修复深度；3) 基于深度前向变形保留高保真纹理；4) 使用生成式场景绘制器填充遮挡区域并消除软边界内的冗余背景伪影；5) 颜色融合器自适应结合变形和修复结果。

Result: HairGuard在单目深度估计、立体图像/视频转换和新视角合成等任务中实现了最先进的性能，特别是在软边界区域有显著改进。框架支持即插即用集成到现有深度模型中。

Conclusion: HairGuard框架有效解决了3D视觉中软边界细节恢复的挑战，通过创新的深度修复和生成式场景绘制技术，在多个3D视觉任务中实现了高质量的几何一致性和精细细节保留。

Abstract: Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.

</details>


### [12] [RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models](https://arxiv.org/abs/2601.03369)
*Sha Luo,Yogesh Prabhu,Tim Ossowski,Kaiping Chen,Junjie Hu*

Main category: cs.CV

TL;DR: 论文提出了RiskCueBench视频理解基准，要求模型从风险信号片段（而非完整视频）预测危险事件，揭示现有系统在早期风险预测上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 随着视频社交媒体的快速增长，从视觉数据预测危险事件对公共安全至关重要。现有研究通常让模型访问包含事故本身的完整视频序列，这大大降低了任务难度，无法反映真实世界条件。

Method: 引入RiskCueBench视频理解基准，通过精心标注识别"风险信号片段"——即最早表明潜在安全问题的时刻，要求模型基于这些早期视觉信号预测未来危险事件。

Result: 实验结果显示当前系统在理解动态演变情况和从早期视觉信号预测未来危险事件方面存在显著能力差距，突显了视频风险预测模型实际部署的重要挑战。

Conclusion: RiskCueBench基准更好地反映了真实世界条件，揭示了现有视频风险预测系统在早期风险识别方面的不足，为实际部署提出了重要挑战。

Abstract: With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.

</details>


### [13] [A Novel Unified Approach to Deepfake Detection](https://arxiv.org/abs/2601.03382)
*Lord Sen,Shyamapada Mukherjee*

Main category: cs.CV

TL;DR: 提出了一种新颖的深度伪造检测架构，结合空间域和频域特征的交叉注意力以及血液检测模块，在FF++和Celeb-DF数据集上取得了优于SOTA的结果（最高99.88% AUC）


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，深度伪造的合成和滥用威胁日益严重。为了维持数字时代的信任，检测和标记深度伪造变得非常必要。

Method: 提出了一种统一架构，利用空间域和频域特征之间的交叉注意力，并结合血液检测模块来分类图像的真伪。使用Swin Transformer和BERT或EfficientNet-B4和BERT作为基础模型。

Result: 在FF++数据集上达到99.80% AUC，在Celeb-DF数据集上达到99.88% AUC（使用Swin Transformer和BERT）。使用EfficientNet-B4和BERT时分别达到99.55%和99.38% AUC。该方法在跨数据集测试中也表现出良好的泛化能力。

Conclusion: 提出的深度伪造检测架构在多个数据集上取得了优于现有技术的结果，并展示了良好的泛化性能，为深度伪造检测提供了有效的解决方案。

Abstract: The advancements in the field of AI is increasingly giving rise to various threats. One of the most prominent of them is the synthesis and misuse of Deepfakes. To sustain trust in this digital age, detection and tagging of deepfakes is very necessary. In this paper, a novel architecture for Deepfake detection in images and videos is presented. The architecture uses cross attention between spatial and frequency domain features along with a blood detection module to classify an image as real or fake. This paper aims to develop a unified architecture and provide insights into each step. Though this approach we achieve results better than SOTA, specifically 99.80%, 99.88% AUC on FF++ and Celeb-DF upon using Swin Transformer and BERT and 99.55, 99.38 while using EfficientNet-B4 and BERT. The approach also generalizes very well achieving great cross dataset results as well.

</details>


### [14] [Better, But Not Sufficient: Testing Video ANNs Against Macaque IT Dynamics](https://arxiv.org/abs/2601.03392)
*Matteo Dunnhofer,Christian Micheloni,Kohitij Kar*

Main category: cs.CV

TL;DR: 比较恒河猴下颞叶皮层在观看自然视频时的神经响应与静态、循环和视频神经网络模型的差异，发现现有视频模型主要捕捉外观相关的动态，而非IT皮层表现的外观不变性时间计算。


<details>
  <summary>Details</summary>
Motivation: 虽然前馈神经网络是灵长类腹侧视觉流的主流模型，但它们本质上是静态计算模型。灵长类世界是动态的，下颞叶皮层不仅支持物体识别，还在自然视频观看中编码物体运动速度。需要探究IT的时间响应是否反映了超越帧级特征的时间计算。

Method: 通过比较恒河猴IT在观看自然视频时的神经响应与静态、循环和视频神经网络模型的预测能力。使用压力测试：在"外观消除"变体（保留运动但移除形状和纹理）上评估解码器的泛化能力。

Result: 视频模型在神经预测性方面有适度改进，特别是在后期响应阶段。IT群体活动在外观消除操作中表现出泛化能力，但所有ANN模型都失败了。视频模型主要捕捉外观相关的动态，而非IT表达的外观不变性时间计算。

Conclusion: 当前视频模型更好地捕捉外观相关的动态，而非IT皮层表达的外观不变性时间计算。需要新的目标函数来编码生物时间统计特性和不变性。

Abstract: Feedforward artificial neural networks (ANNs) trained on static images remain the dominant models of the the primate ventral visual stream, yet they are intrinsically limited to static computations. The primate world is dynamic, and the macaque ventral visual pathways, specifically the inferior temporal (IT) cortex not only supports object recognition but also encodes object motion velocity during naturalistic video viewing. Does IT's temporal responses reflect nothing more than time-unfolded feedforward transformations, framewise features with shallow temporal pooling, or do they embody richer dynamic computations? We tested this by comparing macaque IT responses during naturalistic videos against static, recurrent, and video-based ANN models. Video models provided modest improvements in neural predictivity, particularly at later response stages, raising the question of what kind of dynamics they capture. To probe this, we applied a stress test: decoders trained on naturalistic videos were evaluated on "appearance-free" variants that preserve motion but remove shape and texture. IT population activity generalized across this manipulation, but all ANN classes failed. Thus, current video models better capture appearance-bound dynamics rather than the appearance-invariant temporal computations expressed in IT, underscoring the need for new objectives that encode biological temporal statistics and invariances.

</details>


### [15] [Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning](https://arxiv.org/abs/2601.03400)
*Ali Najar,Alireza Mirrokni,Arshia Izadyari,Sadegh Mohammadian,Amir Homayoon Sharifizade,Asal Meskin,Mobin Bagherian,Ehsaneddin Asgari*

Main category: cs.CV

TL;DR: Eye-Q是一个多语言视觉文字谜题基准，旨在评估VLMs的深层推理能力而非表面识别，结果显示现有模型在抽象和跨语言谜题上表现不佳，最高准确率仅60.27%


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在标准基准测试中表现良好，但主要依赖表面识别而非深层推理。需要创建更具挑战性的评估方式，测试模型发现隐含视觉线索、生成和修正假设、将感知证据映射到非字面概念的能力

Method: 提出Eye-Q多语言基准，包含1,343个视觉文字谜题。每个谜题包含概念密集的场景和简短描述，模型需要推断特定目标词或短语。谜题设计为非结构化和线索隐含，包含干扰项和上下文关系，需要选择性注意、抽象和关联推理。基准涵盖英语、波斯语、阿拉伯语和跨语言谜题

Result: 评估最先进的视觉语言模型显示显著性能差距，特别是在抽象和跨语言谜题上。模型在构建和搜索适当概念表示以进行灵活图像到短语推理方面存在局限，最高准确率仅为60.27%

Conclusion: 视觉文字谜题揭示了当前视觉语言模型在深层推理能力上的重要限制，特别是在处理抽象概念和跨语言场景时。Eye-Q基准为评估模型复杂视觉理解能力提供了有价值的工具

Abstract: Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.

</details>


### [16] [GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models](https://arxiv.org/abs/2601.03416)
*Xiangdong Hu,Yangyang Jiang,Qin Hu,Xiaojun Jia*

Main category: cs.CV

TL;DR: GAMBIT是一种新型多模态越狱攻击框架，通过游戏化场景设计引导模型主动完成恶意查询，显著提高了对推理型和非推理型MLLM的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要增加视觉任务复杂度，未能充分利用模型自身推理激励，导致在推理模型上效果不佳。如果模型能像人类一样思考，能否通过影响其认知阶段决策来主动完成越狱？

Method: 提出GAMBIT框架：1)分解并重组有害视觉语义；2)构建游戏化场景驱动模型探索、重构意图；3)将模型定位为游戏参与者，使其在追求游戏目标时降低安全注意力，从而回答重构的恶意查询。

Result: 在主流推理和非推理MLLM上实现高攻击成功率：Gemini 2.5 Flash达到92.13%，QvQ-MAX达到91.20%，GPT-4o达到85.87%，显著优于基线方法。

Conclusion: 通过游戏化场景设计利用模型的推理激励机制，可以有效绕过MLLM的安全对齐机制，这揭示了当前安全防护的脆弱性，并为未来防御策略提供了重要启示。

Abstract: Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.

</details>


### [17] [WeedRepFormer: Reparameterizable Vision Transformers for Real-Time Waterhemp Segmentation and Gender Classification](https://arxiv.org/abs/2601.03431)
*Toqi Tahamid Sarker,Taminul Islam,Khaled R. Ahmed,Cristiana Bernardi Rankrape,Kaitlin E. Creager,Karla Gage*

Main category: cs.CV

TL;DR: WeedRepFormer是一个轻量级多任务视觉Transformer，用于同时进行水麻分割和性别分类，通过结构重参数化实现训练容量与推理延迟的解耦，在保持高性能的同时显著减少参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有农业模型难以平衡细粒度特征提取（用于生物属性分类）和实时部署所需的效率。需要一种既能准确识别水麻（一种杂草）并分类其性别，又能在资源受限设备上高效运行的模型。

Method: 1. 设计轻量级多任务Vision Transformer架构；2. 在整个架构（包括Vision Transformer骨干网络、Lite R-ASPP解码器和新的可重参数化分类头）中系统集成结构重参数化技术；3. 引入包含23株植物10,264个标注帧的全面水麻数据集。

Result: 在基准测试中：分割任务达到92.18% mIoU，性别分类达到81.91%准确率；仅使用3.59M参数和3.80 GFLOPs；推理速度108.95 FPS；相比最先进的iFormer-T，分类准确率提升4.40%，参数减少1.9倍，同时保持竞争力的分割性能。

Conclusion: WeedRepFormer通过结构重参数化有效解决了农业视觉任务中精度与效率的平衡问题，为实时杂草识别和性别分类提供了高效解决方案，在资源受限的农业环境中具有实际应用价值。

Abstract: We present WeedRepFormer, a lightweight multi-task Vision Transformer designed for simultaneous waterhemp segmentation and gender classification. Existing agricultural models often struggle to balance the fine-grained feature extraction required for biological attribute classification with the efficiency needed for real-time deployment. To address this, WeedRepFormer systematically integrates structural reparameterization across the entire architecture - comprising a Vision Transformer backbone, a Lite R-ASPP decoder, and a novel reparameterizable classification head - to decouple training-time capacity from inference-time latency. We also introduce a comprehensive waterhemp dataset containing 10,264 annotated frames from 23 plants. On this benchmark, WeedRepFormer achieves 92.18% mIoU for segmentation and 81.91% accuracy for gender classification using only 3.59M parameters and 3.80 GFLOPs. At 108.95 FPS, our model outperforms the state-of-the-art iFormer-T by 4.40% in classification accuracy while maintaining competitive segmentation performance and significantly reducing parameter count by 1.9x.

</details>


### [18] [FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder](https://arxiv.org/abs/2601.03460)
*Zeyu Dong,Yimin Zhu,Yu Wu,Yu Sun*

Main category: cs.CV

TL;DR: FROST-Drive提出一种新的端到端自动驾驶架构，通过冻结预训练视觉编码器来保留视觉语言模型的泛化能力，在长尾场景中显著优于完全微调的方法。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶模型通常对视觉编码器进行完全微调，这可能导致模型过度专注于训练数据而限制了泛化能力。本文质疑这种训练范式的必要性，旨在探索如何更好地利用预训练视觉语言模型的强大泛化能力。

Method: 提出FROST-Drive架构：1) 冻结预训练视觉语言模型的视觉编码器权重；2) 使用基于transformer的适配器进行多模态融合；3) 采用GRU-based解码器生成平滑路径点；4) 设计专门优化Rater Feedback Score的自定义损失函数。

Result: 在Waymo Open E2E数据集（专门包含长尾场景的大规模数据集）上的实验表明，冻结编码器方法显著优于完全微调的模型，证明了保留VLM广泛知识是更有效的策略。

Conclusion: 保留预训练视觉语言模型的泛化知识比密集的领域特定适应更有效，为开发能够更好处理现实世界复杂性的视觉模型提供了新途径。

Abstract: End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.

</details>


### [19] [Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets](https://arxiv.org/abs/2601.03463)
*Md. Hefzul Hossain Papon,Shadman Rabby*

Main category: cs.CV

TL;DR: 该研究表明，经过良好正则化的浅层CNN架构在多个孟加拉国视觉数据集上表现出色，可作为轻量级、可部署的基准模型，无需大型GPU或预训练模型。


<details>
  <summary>Details</summary>
Motivation: 为低资源环境（如孟加拉国）提供实用的计算机视觉解决方案，解决现实世界部署中需要大型GPU和预训练模型的问题，建立统一可复现的基准。

Method: 采用经过良好正则化的浅层卷积神经网络架构，在多个异构领域的孟加拉国视觉数据集上进行评估，包括智慧城市监控和农业品种分类等。

Result: 浅层架构在多个领域表现出高度竞争力，可作为统一的基准模型，在低资源设置下实现实际部署价值，无需大型计算资源。

Conclusion: 轻量级CNN在低资源环境中具有重要实用价值，为孟加拉国视觉数据集建立了可复现的基准，展示了浅层架构在现实世界部署中的潜力。

Abstract: Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.

</details>


### [20] [Latent Geometry of Taste: Scalable Low-Rank Matrix Factorization](https://arxiv.org/abs/2601.03466)
*Joshua Salako*

Main category: cs.CV

TL;DR: 基于MovieLens 32M数据集，通过并行化ALS框架实现高效协同过滤，发现约束低秩模型在泛化性能上优于高维模型，并可视化嵌入空间揭示语义聚类，最后验证了冷启动场景下的实用性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模交互数据集中协同过滤面临的可扩展性和数据稀疏性问题，探索用户偏好的潜在几何结构。

Method: 使用MovieLens 32M数据集，实现高性能并行化交替最小二乘(ALS)框架，进行广泛的超参数优化，可视化学习到的嵌入空间，并在冷启动场景中验证系统实用性。

Result: 约束低秩模型在泛化性能上显著优于高维对应模型，在RMSE和排序精度之间达到最佳平衡；嵌入空间可视化显示无监督地出现了语义流派聚类；冷启动场景中通过可调评分参数有效管理流行度偏差与个性化亲和力之间的权衡。

Conclusion: 通过并行化ALS框架和约束低秩建模，能够有效解决大规模协同过滤的可扩展性问题，模型不仅能捕获深层结构关系，还能在冷启动场景中实现实用平衡，为推荐系统提供了高效解决方案。

Abstract: Scalability and data sparsity remain critical bottlenecks for collaborative filtering on massive interaction datasets. This work investigates the latent geometry of user preferences using the MovieLens 32M dataset, implementing a high-performance, parallelized Alternating Least Squares (ALS) framework. Through extensive hyperparameter optimization, we demonstrate that constrained low-rank models significantly outperform higher dimensional counterparts in generalization, achieving an optimal balance between Root Mean Square Error (RMSE) and ranking precision. We visualize the learned embedding space to reveal the unsupervised emergence of semantic genre clusters, confirming that the model captures deep structural relationships solely from interaction data. Finally, we validate the system's practical utility in a cold-start scenario, introducing a tunable scoring parameter to manage the trade-off between popularity bias and personalized affinity effectively. The codebase for this research can be found here: https://github.com/joshsalako/recommender.git

</details>


### [21] [ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing](https://arxiv.org/abs/2601.03467)
*Hengjia Li,Liming Jiang,Qing Yan,Yizhi Song,Hao Kang,Zichuan Liu,Xin Lu,Boxi Wu,Deng Cai*

Main category: cs.CV

TL;DR: 提出ThinkRL-Edit框架，通过解耦视觉推理与图像合成，使用思维链推理采样和偏好分组策略，解决指令驱动图像编辑中的推理限制问题


<details>
  <summary>Details</summary>
Motivation: 当前指令驱动图像编辑模型在推理密集型编辑任务上表现不佳，现有强化学习方法面临推理探索有限、奖励融合偏差和不稳定VLM奖励三大挑战

Method: 1) 提出思维链推理采样，包含规划和反思阶段；2) 无偏链偏好分组策略；3) 用二元检查表替代区间VLM评分

Result: 在推理密集型图像编辑任务上显著优于先前工作，产生指令忠实、视觉连贯、语义基础的编辑结果

Conclusion: ThinkRL-Edit通过解耦推理与合成、扩展推理探索、改进奖励机制，有效提升了指令驱动图像编辑的推理能力

Abstract: Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.

</details>


### [22] [Understanding Reward Hacking in Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2601.03468)
*Yunqi Hong,Kuei-Chun Kao,Hengguang Zhou,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 该论文系统分析了文本到图像RL后训练中的奖励黑客行为，发现现有奖励设计存在缺陷，并提出轻量级自适应伪影奖励模型作为正则化器来缓解奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练中使用的奖励函数往往是不完美的人类判断代理，容易导致奖励黑客行为——模型生成不真实或低质量的图像却能获得高奖励分数。需要解决文本到图像RL后训练中的奖励黑客问题。

Method: 1. 系统分析美学/人类偏好奖励和提示-图像一致性奖励各自如何导致奖励黑客；2. 发现集成多个奖励只能部分缓解问题；3. 提出轻量级自适应伪影奖励模型，在小型精选数据集上训练；4. 将该模型集成到现有RL流程中作为正则化器。

Result: 实验表明，加入伪影奖励能显著提高视觉真实感，减少多个文本到图像RL设置中的奖励黑客行为。轻量级奖励增强作为奖励黑客的安全防护措施是有效的。

Conclusion: 奖励黑客是文本到图像RL后训练中的常见问题，伪影生成是跨不同奖励模型的共同失败模式。提出的轻量级自适应伪影奖励模型能有效缓解奖励黑客，提高生成质量，为现有RL流程提供了实用的正则化解决方案。

Abstract: Reinforcement learning (RL) has become a standard approach for post-training large language models and, more recently, for improving image generation models, which uses reward functions to enhance generation quality and human preference alignment. However, existing reward designs are often imperfect proxies for true human judgment, making models prone to reward hacking--producing unrealistic or low-quality images that nevertheless achieve high reward scores. In this work, we systematically analyze reward hacking behaviors in text-to-image (T2I) RL post-training. We investigate how both aesthetic/human preference rewards and prompt-image consistency rewards individually contribute to reward hacking and further show that ensembling multiple rewards can only partially mitigate this issue. Across diverse reward models, we identify a common failure mode: the generation of artifact-prone images. To address this, we propose a lightweight and adaptive artifact reward model, trained on a small curated dataset of artifact-free and artifact-containing samples. This model can be integrated into existing RL pipelines as an effective regularizer for commonly used reward models. Experiments demonstrate that incorporating our artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating the effectiveness of lightweight reward augment serving as a safeguard against reward hacking.

</details>


### [23] [CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2601.03490)
*Yuzhe Sun,Zhe Dong,Haochen Jiang,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出不确定性引导框架，通过像素级参考不确定性图作为空间先验，自适应调节语言融合和局部细化，提升遥感图像分割的鲁棒性和几何保真度。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像分割方法采用均匀的融合和细化策略，但在视觉清晰区域引入不必要的语言扰动，在混淆区域又无法提供足够的消歧能力。这是由于跨模态对齐存在显著的空间非均匀性，源于尺度变化大、相似干扰物密集和复杂边界结构。

Method: 提出不确定性引导框架：1) 参考不确定性评分器(RUS)，通过在线误差一致性监督预测参考歧义的空间分布；2) 不确定性门控融合(UGF)，动态调节语言注入强度；3) 不确定性驱动局部细化(UDLR)，利用不确定性软掩码聚焦细化误差易发边界和细节。

Result: 大量实验表明，该方法作为统一的即插即用解决方案，在不改变骨干架构的情况下，显著提升了复杂遥感场景中的鲁棒性和几何保真度。

Conclusion: 通过显式建模参考不确定性并以此引导自适应推理，能够有效解决遥感图像分割中的空间非均匀性问题，提高分割精度和可靠性。

Abstract: Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.

</details>


### [24] [SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2601.03500)
*Yuxuan Xia,Siheng Wang,Peng Li*

Main category: cs.CV

TL;DR: 提出SDCD方法，通过结构扰乱对比解码来缓解大视觉语言模型中的物体幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注缓解语言先验或高层统计偏差，但忽视了视觉编码过程中的内部复杂性。作者发现视觉统计偏差（源于视觉编码器在弱结构监督下的Bag-of-Patches行为）是导致物体幻觉的重要因素。

Method: 提出无需训练的结构扰乱对比解码（SDCD）算法，通过引入打乱的结构扰乱视图来对比校准输出分布，惩罚在结构缺失视图中仍保持高置信度的token，从而抑制纹理驱动的偏差。

Result: 实验结果表明，SDCD在多个基准测试中显著缓解了幻觉问题，并提升了大视觉语言模型的多模态能力。

Conclusion: 视觉统计偏差是导致物体幻觉的重要因素，通过SDCD方法可以有效抑制纹理驱动的偏差，提升模型的可靠性和多模态理解能力。

Abstract: Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

</details>


### [25] [REFA: Real-time Egocentric Facial Animations for Virtual Reality](https://arxiv.org/abs/2601.03507)
*Qiang Zhang,Tong Xiao,Haroun Habeeb,Larissa Laich,Sofien Bouaziz,Patrick Snape,Wenjing Zhang,Matthew Cioffi,Peizhao Zhang,Pavel Pidlypenskyi,Winnie Lin,Luming Ma,Mengjiao Wang,Kunpeng Li,Chengjiang Long,Steven Song,Martin Prazak,Alexander Sjoholm,Ajinkya Deogade,Jaebong Lee,Julio Delgado Mangas,Amaury Aubel*

Main category: cs.CV

TL;DR: 提出一种基于VR头显红外摄像头的实时面部表情追踪系统，无需繁琐校准即可驱动虚拟角色表情


<details>
  <summary>Details</summary>
Motivation: 解决虚拟环境中面部表情追踪的侵入性问题和校准复杂性，为虚拟沟通、游戏娱乐等应用提供自然的表情交互方式

Method: 采用蒸馏学习方法训练机器学习模型，整合多源异构数据（合成和真实图像）；开发轻量级采集系统（手机+定制VR头显）收集18k多样本数据；建立鲁棒的可微分渲染管道自动提取表情标签

Result: 实现了非侵入式的实时面部表情追踪系统，无需冗长校准步骤，能够准确驱动虚拟角色表情

Conclusion: 该系统为虚拟环境中的沟通和表达开辟了新途径，在视频会议、游戏娱乐和远程协作等领域具有广泛应用前景

Abstract: We present a novel system for real-time tracking of facial expressions using egocentric views captured from a set of infrared cameras embedded in a virtual reality (VR) headset. Our technology facilitates any user to accurately drive the facial expressions of virtual characters in a non-intrusive manner and without the need of a lengthy calibration step. At the core of our system is a distillation based approach to train a machine learning model on heterogeneous data and labels coming form multiple sources, \eg synthetic and real images. As part of our dataset, we collected 18k diverse subjects using a lightweight capture setup consisting of a mobile phone and a custom VR headset with extra cameras. To process this data, we developed a robust differentiable rendering pipeline enabling us to automatically extract facial expression labels. Our system opens up new avenues for communication and expression in virtual environments, with applications in video conferencing, gaming, entertainment, and remote collaboration.

</details>


### [26] [G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2601.03510)
*Hojun Song,Chae-yeong Song,Jeong-hun Hong,Chaewon Moon,Dong-hwi Kim,Gahyeon Kim,Soo Ye Kim,Yiyi Liao,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: 提出Gaussian-to-Point (G2P)方法，将3D高斯泼溅的外观感知属性转移到点云，实现更区分性和外观一致的语义分割


<details>
  <summary>Details</summary>
Motivation: 点云稀疏不规则分布提供有限的外观证据，仅依赖几何特征难以区分形状相似但外观不同的物体（如颜色、纹理、材质）

Method: 通过建立点级对应关系解决优化高斯与原始点几何不对齐问题；利用高斯不透明度属性解决几何歧义；使用高斯尺度属性实现复杂3D场景中的精确边界定位

Result: 在标准基准测试中取得优越性能，在几何挑战性类别上显示显著改进，无需任何2D或语言监督

Conclusion: G2P方法通过将外观感知属性从3D高斯泼溅转移到点云，实现了更区分性和外观一致的语义分割，解决了现有模型受限于几何歧义的问题

Abstract: Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.

</details>


### [27] [Semantic Belief-State World Model for 3D Human Motion Prediction](https://arxiv.org/abs/2601.03517)
*Sarim Chaudhry*

Main category: cs.CV

TL;DR: SBWM将人体运动预测重构为人体流形上的潜在动力学模拟，通过分离观测重建与动力学建模，解决了传统方法的长时程漂移和均值塌陷问题。


<details>
  <summary>Details</summary>
Motivation: 传统人体运动预测方法将问题视为序列回归，直接外推关节坐标。这种方法不分离观测重建与动力学建模，缺乏对运动潜在因果的显式表示，导致长时程预测出现漂移、均值塌陷和不确定性校准不良等问题。

Method: 提出语义信念状态世界模型(SBWM)，将运动预测重构为人体流形上的潜在动力学模拟。模型维护循环概率信念状态，其演化独立于姿态重建学习，并与SMPL-X解剖参数化显式对齐。采用结构信息瓶颈防止潜在状态编码静态几何或传感器噪声，强制其捕获运动动力学、意图和控制相关结构。借鉴基于模型的强化学习中的信念状态世界模型，采用随机潜在转移和以推演为中心的训练策略。

Result: SBWM实现了连贯的长时程推演，在显著降低计算成本的同时保持了竞争性的预测精度。相比专注于重建保真度的RSSM、transformer和扩散方法，SBWM优先考虑稳定的前向模拟。

Conclusion: 将人体视为世界模型状态空间的一部分而非其输出，从根本上改变了运动模拟和预测的方式。这种分离观测重建与动力学建模的方法为解决传统运动预测的局限性提供了新思路。

Abstract: Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.

</details>


### [28] [Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution](https://arxiv.org/abs/2601.03526)
*Zhicheng Zhao,Fengjiao Peng,Jinquan Yan,Wei Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: PCNet提出了一种物理约束的光学引导热成像超分辨率方法，通过跨分辨率互增强模块和热传导物理模型，解决现有方法中高频信息丢失和物理不一致伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有光学引导的热成像超分辨率方法通常压缩光学特征以匹配热成像特征维度，这导致高频信息丢失，并因忽略模态间成像物理差异而引入纹理扭曲、边缘模糊等物理不一致伪影。

Method: 提出PCNet框架：1) 跨分辨率互增强模块(CRME)联合优化热成像超分辨率和光学到热成像模态转换，实现双向特征交互；2) 物理驱动的热传导模块(PDTM)将二维热传导融入光学引导，建模空间变化的热传导特性；3) 温度一致性损失确保生成的热图像符合真实热辐射原理。

Result: 在VGTSR2.0和DroneVehicle数据集上的大量实验表明，PCNet在重建质量和下游任务（语义分割和目标检测）上显著优于现有最先进方法。

Conclusion: PCNet通过物理约束的光学引导和跨分辨率互增强，实现了鲁棒的热成像超分辨率，解决了现有方法中的高频信息丢失和物理不一致伪影问题，在无人机监控应用中具有重要价值。

Abstract: Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.

</details>


### [29] [CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection](https://arxiv.org/abs/2601.03528)
*Jiayi Zhao,Changlu Chen,Jingsheng Li,Tianxiang Xue,Kun Zhan*

Main category: cs.CV

TL;DR: CloudMatch是一个半监督云检测框架，通过视图一致性学习和场景混合增强有效利用未标注遥感影像，在云检测任务上取得良好性能。


<details>
  <summary>Details</summary>
Motivation: 由于像素级标注成本高昂，需要开发半监督学习方法。云模式在不同场景和同一场景类别中表现出结构多样性和上下文变异性，这为利用未标注数据提供了机会。

Method: 提出CloudMatch框架，为每个未标注图像生成一个弱增强视图和两个互补的强增强视图：一个通过场景间混合模拟上下文变化，另一个通过场景内混合保持语义一致性。通过跨多样化增强视图的预测一致性学习，引导伪标签生成并增强泛化能力。

Result: 大量实验表明CloudMatch取得了良好性能，证明了其有效利用未标注数据的能力，并推进了半监督云检测的发展。

Conclusion: CloudMatch通过视图一致性学习和场景混合增强，成功解决了半监督云检测中利用未标注数据的挑战，为遥感图像分析提供了有效解决方案。

Abstract: Due to the high cost of annotating accurate pixel-level labels, semi-supervised learning has emerged as a promising approach for cloud detection. In this paper, we propose CloudMatch, a semi-supervised framework that effectively leverages unlabeled remote sensing imagery through view-consistency learning combined with scene-mixing augmentations. An observation behind CloudMatch is that cloud patterns exhibit structural diversity and contextual variability across different scenes and within the same scene category. Our key insight is that enforcing prediction consistency across diversely augmented views, incorporating both inter-scene and intra-scene mixing, enables the model to capture the structural diversity and contextual richness of cloud patterns. Specifically, CloudMatch generates one weakly augmented view along with two complementary strongly augmented views for each unlabeled image: one integrates inter-scene patches to simulate contextual variety, while the other employs intra-scene mixing to preserve semantic coherence. This approach guides pseudolabel generation and enhances generalization. Extensive experiments show that CloudMatch achieves good performance, demonstrating its capability to utilize unlabeled data efficiently and advance semi-supervised cloud detection.

</details>


### [30] [EASLT: Emotion-Aware Sign Language Translation](https://arxiv.org/abs/2601.03549)
*Guobin Tu,Di Weng*

Main category: cs.CV

TL;DR: EASLT是一个情感感知手语翻译框架，通过专门的情感编码器和情感感知融合模块，将面部表情作为语义锚点来解决手语翻译中的歧义问题，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无词表手语翻译方法主要关注手动信号，而忽略了面部表情的语义重要性，导致当不同概念具有相同手动表达时产生歧义。面部表情作为非手动信号对于完整理解手语语义至关重要。

Method: 提出EASLT框架，包含专门的情感编码器捕捉连续情感动态，通过新颖的情感感知融合模块自适应地重新校准时空手语特征，将情感信息作为语义锚点来解析语义歧义。

Result: 在PHOENIX14T和CSL-Daily基准测试中，EASLT在无词表方法中取得了先进性能，分别获得26.15和22.80的BLEU-4分数，以及61.0和57.8的BLEURT分数。消融研究证实情感建模有效解耦了情感语义和手动动态。

Conclusion: 将面部情感作为语义锚点而非辅助信息，通过专门的情感建模显著提升手语翻译的准确性，为解决手语翻译中的语义歧义问题提供了有效框架。

Abstract: Sign Language Translation (SLT) is a complex cross-modal task requiring the integration of Manual Signals (MS) and Non-Manual Signals (NMS). While recent gloss-free SLT methods have made strides in translating manual gestures, they frequently overlook the semantic criticality of facial expressions, resulting in ambiguity when distinct concepts share identical manual articulations. To address this, we present **EASLT** (**E**motion-**A**ware **S**ign **L**anguage **T**ranslation), a framework that treats facial affect not as auxiliary information, but as a robust semantic anchor. Unlike methods that relegate facial expressions to a secondary role, EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics. These representations are integrated via a novel *Emotion-Aware Fusion* (EAF) module, which adaptively recalibrates spatio-temporal sign features based on affective context to resolve semantic ambiguities. Extensive evaluations on the PHOENIX14T and CSL-Daily benchmarks demonstrate that EASLT establishes advanced performance among gloss-free methods, achieving BLEU-4 scores of 26.15 and 22.80, and BLEURT scores of 61.0 and 57.8, respectively. Ablation studies confirm that explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity. Code is available at https://github.com/TuGuobin/EASLT.

</details>


### [31] [SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization](https://arxiv.org/abs/2601.03579)
*Tianyi Shang,Pengjie Xu,Zhaojun Deng,Zhenyu Li,Zhicong Chen,Lijun Wu*

Main category: cs.CV

TL;DR: SpatiaLoc是一个利用粗到细策略的跨模态定位框架，通过实例级和全局级的空间关系建模，显著提升了基于文本和点云的定位性能。


<details>
  <summary>Details</summary>
Motivation: 跨模态定位使用文本和点云使机器人能够通过自然语言描述定位自身，应用于自主导航和人机交互。由于物体在文本和点云中经常重复出现，空间关系成为最具有区分性的定位线索。

Method: 采用粗到细策略：粗阶段使用BEOSE（贝塞尔增强对象空间编码器）通过二次贝塞尔曲线建模实例级空间关系，以及FAE（频率感知编码器）在频域生成全局级空间表示；细阶段使用UGFL（不确定性感知高斯精细定位器）通过建模为高斯分布并采用不确定性感知损失函数回归2D位置。

Result: 在KITTI360Pose数据集上的大量实验表明，SpatiaLoc显著优于现有的最先进方法。

Conclusion: 通过强调实例级和全局级的空间关系，SpatiaLoc框架在跨模态定位任务中取得了卓越性能，验证了空间关系作为关键定位线索的重要性。

Abstract: Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.

</details>


### [32] [Detecting AI-Generated Images via Distributional Deviations from Real Images](https://arxiv.org/abs/2601.03586)
*Yakun Niu,Yingjian Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 提出基于掩码预训练模型微调(MPFT)策略，通过纹理感知掩码(TAM)机制提升CLIP-ViT对AI生成图像的检测泛化能力，在少量数据微调下显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展提高了AI生成图像质量，引发对错误信息和公众信任侵蚀的担忧。检测AI生成图像成为关键挑战，特别是在泛化到未见生成模型方面。现有使用冻结预训练CLIP模型的方法虽然显示出泛化潜力，但仅将图像编码器视为基本特征提取器，未能充分利用其潜力。

Method: 提出掩码预训练模型微调(MPFT)策略，引入纹理感知掩码(TAM)机制，在微调过程中掩码包含生成模型特定模式的纹理区域。这种方法迫使CLIP-ViT关注AI生成图像相对于真实图像的"分布偏差"，从而提升检测泛化性能。

Result: 在GenImage和UniversalFakeDetect数据集上的大量实验表明，该方法仅需少量图像微调即可显著超越现有方法，在两个数据集上分别达到98.2%和94.6%的平均准确率。

Conclusion: 通过深入分析冻结CLIP图像编码器，发现其能有效聚类真实图像但缺乏区分真实与AI生成图像的能力。提出的MPFT策略通过TAM机制成功提升了检测泛化性能，为解决AI生成图像检测的泛化问题提供了有效方案。

Abstract: The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the "distributional deviations"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.

</details>


### [33] [Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions](https://arxiv.org/abs/2601.03590)
*Zhongbin Guo,Zhen Yang,Yushan Li,Xinyue Zhang,Wenyu Gao,Jiacheng Wang,Chengzhi Li,Xiangrui Liu,Ping Jian*

Main category: cs.CV

TL;DR: SiT-Bench是一个评估纯文本大语言模型空间智能的新基准，包含3800多个专家标注项目，涵盖5大类17个子任务，通过将视觉场景转换为坐标感知的文本描述来测试LLM的符号推理能力而非视觉模式匹配。


<details>
  <summary>Details</summary>
Motivation: 当前空间智能研究主要依赖视觉语言模型，但关键问题在于：空间理解究竟源于视觉编码器还是基础推理架构？为了探究大语言模型在没有像素输入情况下的空间智能能力，需要专门的评估基准。

Method: 将单视角/多视角场景转换为高保真、坐标感知的文本描述，构建包含3800多个专家标注项目的SiT-Bench基准，涵盖自我中心导航、视角转换、精细机器人操作等5大类17个子任务，通过纯文本推理测试LLM的空间智能。

Result: 评估显示，最先进的LLM在局部语义任务上表现良好，但在全局一致性方面存在显著的"空间差距"。研究发现，显式的空间推理能显著提升性能，表明LLM具有潜在的世界建模能力。

Conclusion: SiT-Bench作为基础资源，将促进未来视觉语言模型和具身智能体的空间感知LLM骨干网络的发展。研究揭示了LLM的空间推理潜力，为开发不依赖视觉输入的空间智能系统提供了新方向。

Abstract: Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant "spatial gap" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .

</details>


### [34] [Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations](https://arxiv.org/abs/2601.03596)
*Qianyu Guo,Jingrong Wu,Jieji Ren,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出环境鲁棒的少样本分割(ER-FSS)设置和自适应注意力蒸馏(AAD)方法，解决真实复杂环境下少样本分割性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有少样本分割研究忽视真实场景中的复杂环境因素(光照、背景、视角等)，导致实验室训练的模型在实际部署中表现不佳

Method: 提出自适应注意力蒸馏(AAD)方法，通过反复对比和蒸馏已知(支持)与未知(查询)图像间的关键共享语义，推导新类别的类特定注意力

Result: AAD方法在所有数据集和设置下将mIoU提高了3.3%-8.5%，表现出优越性能和强泛化能力

Conclusion: 提出的ER-FSS设置和AAD方法有效增强了少样本分割模型在复杂真实环境下的鲁棒性，为实际应用提供了解决方案

Abstract: Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model's robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model's ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.

</details>


### [35] [Unveiling Text in Challenging Stone Inscriptions: A Character-Context-Aware Patching Strategy for Binarization](https://arxiv.org/abs/2601.03609)
*Pratyush Jena,Amal Joseph,Arnav Sharma,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 提出一种针对印度石刻铭文的鲁棒自适应分块策略，结合注意力U-Net进行二值化，显著提升性能并展示跨文字的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 石刻铭文图像由于字符与石质背景对比度差、表面退化不均匀、干扰伪影多、文字密度和布局变化大，现有二值化方法难以有效分离字符区域。

Method: 提出自适应分块策略，通过动态采样和分块选择训练注意力U-Net模型，注意力机制聚焦细微结构特征，分块方法克服表面噪声和布局不规则性。

Result: 新型分块机制显著提升二值化性能，尽管仅在单一印度文字数据集上训练，模型展现出对印度和非印度文字的强零样本泛化能力。

Conclusion: 该方法为石刻铭文生成干净、结构化的内容表示，为下游任务如文字识别、OCR和历史文本分析奠定基础。

Abstract: Binarization is a popular first step towards text extraction in historical artifacts. Stone inscription images pose severe challenges for binarization due to poor contrast between etched characters and the stone background, non-uniform surface degradation, distracting artifacts, and highly variable text density and layouts. These conditions frequently cause existing binarization techniques to fail and struggle to isolate coherent character regions. Many approaches sub-divide the image into patches to improve text fragment resolution and improve binarization performance. With this in mind, we present a robust and adaptive patching strategy to binarize challenging Indic inscriptions. The patches from our approach are used to train an Attention U-Net for binarization. The attention mechanism allows the model to focus on subtle structural cues, while our dynamic sampling and patch selection method ensures that the model learns to overcome surface noise and layout irregularities. We also introduce a carefully annotated, pixel-precise dataset of Indic stone inscriptions at the character-fragment level. We demonstrate that our novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, our model exhibits strong zero-shot generalization to other Indic and non-indic scripts, highlighting its robustness and script-agnostic generalization capabilities. By producing clean, structured representations of inscription content, our method lays the foundation for downstream tasks such as script identification, OCR, and historical text analysis. Project page: https://ihdia.iiit.ac.in/shilalekhya-binarization/

</details>


### [36] [Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection](https://arxiv.org/abs/2601.03617)
*Samson Oseiwe Ajadalu*

Main category: cs.CV

TL;DR: 单目3D目标检测中，深度估计主干网络的选择比特征增强更重要，NeWCRFs在伪激光雷达管道中优于Depth Anything V2，语义特征仅带来边际增益。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测相比激光雷达成本更低，但由于从单张图像估计度量深度的困难，其精度仍然较低。研究旨在系统评估深度主干网络和特征工程如何影响单目伪激光雷达管道的性能。

Method: 在KITTI验证集上比较NeWCRFs（监督度量深度）和Depth Anything V2 Metric-Outdoor（Base）在相同伪激光雷达生成和PointRCNN检测协议下的表现。测试了点云增强方法，包括外观线索（灰度强度）和语义线索（实例分割置信度），并进行了深度精度与距离的诊断分析。

Result: NeWCRFs在中等难度分割上使用灰度强度达到10.50% AP3D（IoU=0.7）。语义特征仅带来边际增益，基于掩码的采样可能通过移除上下文几何而降低性能。深度精度与距离分析显示，粗略深度正确性不能完全预测严格的3D IoU性能。

Conclusion: 在使用现成的激光雷达检测器时，深度主干网络的选择和几何保真度主导性能，超过了次要特征注入的影响。深度估计质量比语义特征增强更为关键。

Abstract: Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.

</details>


### [37] [Shape Classification using Approximately Convex Segment Features](https://arxiv.org/abs/2601.03625)
*Bimal Kumar Ray*

Main category: cs.CV

TL;DR: 该论文提出了一种无需对象对齐的物体分类方法，通过特征排序替代对齐需求，使用边界分段和特征袋进行相似度度量。


<details>
  <summary>Details</summary>
Motivation: 现有基于描述性特征的物体分类技术需要依赖对象对齐来计算相似度，这限制了方法的适用性和效率。论文旨在消除对象对齐的必要性，开发更灵活的分类方法。

Method: 1. 对物体边界进行归一化和分割，得到近似凸分段；2. 按长度降序排列分段；3. 提取分段长度、极值点数量、面积、基部和宽度等特征组成特征袋；4. 使用这些特征度量图像边界之间的相似度。

Result: 在多个数据集上测试了该方法，观察到了可接受的结果，表明无需对象对齐的特征排序方法在物体分类中是可行的。

Conclusion: 通过特征排序可以成功替代对象对齐，基于边界分段和特征袋的方法为物体分类提供了新的有效途径，具有实际应用价值。

Abstract: The existing object classification techniques based on descriptive features rely on object alignment to compute the similarity of objects for classification. This paper replaces the necessity of object alignment through sorting of feature. The object boundary is normalized and segmented into approximately convex segments and the segments are then sorted in descending order of their length. The segment length, number of extreme points in segments, area of segments, the base and the width of the segments - a bag of features - is used to measure the similarity between image boundaries. The proposed method is tested on datasets and acceptable results are observed.

</details>


### [38] [MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction](https://arxiv.org/abs/2601.03633)
*Wenjie Luo,Chuanhu Deng,Chaorong Li,Rongyao Deng,Qiang Yang*

Main category: cs.CV

TL;DR: MFC-RFNet：一种结合多尺度特征通信、空间变换对齐和小波引导融合的生成式框架，用于高分辨率雷达回波序列的降水临近预报，在多个数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 雷达回波序列的高精度降水临近预报对灾害缓解和经济规划至关重要，但面临多尺度演化建模、帧间特征错位校正、长程时空上下文高效捕获等挑战。

Method: 提出MFC-RFNet生成框架：1) 小波引导跳跃连接(WGSC)保留高频细节；2) 特征通信模块(FCM)促进双向跨尺度交互；3) 条件引导空间变换融合(CGSTF)校正帧间位移；4) 采用整流流训练实现少步采样；5) 轻量级Vision-RWKV块捕获长程依赖。

Result: 在SEVIR、MeteoNet、Shanghai和CIKM四个公开数据集上评估，相比强基线有持续改进，在高雨率阈值下产生更清晰的回波形态，在更长预见期保持预测技能。

Conclusion: 整流流训练与尺度感知通信、空间对齐和频率感知融合的协同作用，为基于雷达的临近预报提供了有效且稳健的方法。

Abstract: Accurate and high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, yet it remains a significant challenge. Key difficulties include modeling complex multi-scale evolution, correcting inter-frame feature misalignment caused by displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity. To address these issues, we present the Multi-scale Feature Communication Rectified Flow (RF) Network (MFC-RFNet), a generative framework that integrates multi-scale communication with guided feature fusion. To enhance multi-scale fusion while retaining fine detail, a Wavelet-Guided Skip Connection (WGSC) preserves high-frequency components, and a Feature Communication Module (FCM) promotes bidirectional cross-scale interaction. To correct inter-frame displacement, a Condition-Guided Spatial Transform Fusion (CGSTF) learns spatial transforms from conditioning echoes to align shallow features. The backbone adopts rectified flow training to learn near-linear probability-flow trajectories, enabling few-step sampling with stable fidelity. Additionally, lightweight Vision-RWKV (RWKV) blocks are placed at the encoder tail, the bottleneck, and the first decoder layer to capture long-range spatiotemporal dependencies at low spatial resolutions with moderate compute. Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) demonstrate consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times. These results suggest that the proposed synergy of RF training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.

</details>


### [39] [CrackSegFlow: Controllable Flow-Matching Synthesis for Generalizable Crack Segmentation with the CSF-50K Benchmark](https://arxiv.org/abs/2601.03637)
*Babak Asadi,Peiyang Wu,Mani Golparvar-Fard,Ramez Hajj*

Main category: cs.CV

TL;DR: CrackSegFlow：基于流匹配的可控裂缝图像合成框架，通过生成真实裂缝图像和掩码对解决裂缝分割中的标注稀缺和领域偏移问题，显著提升分割性能并发布5万对数据集。


<details>
  <summary>Details</summary>
Motivation: 自动裂缝分割对于基础设施状态评估至关重要，但实际部署受到像素级标注稀缺以及传感器、光照、纹理和标注规范等跨领域偏移的限制。

Method: 提出CrackSegFlow框架，包含两个流匹配模型：1）基于二元掩码生成真实裂缝图像的生成器，采用拓扑保持掩码注入和边界门控调制；2）类别条件流匹配模型生成裂缝掩码，可控制裂缝覆盖率。还将裂缝掩码注入无裂缝背景以增加光照和表面伪影多样性。

Result: 在5个基准测试（4个沥青数据集和1个混凝土数据集）上，域内性能平均提升5.37 mIoU和5.13 F1；目标导向跨域合成平均提升13.12 mIoU和14.82 F1。相比基于扩散的语义合成，提供更快的确定性采样和更好的薄结构几何对齐。

Conclusion: CrackSegFlow通过可控合成解决了裂缝分割的数据稀缺和领域偏移问题，显著提升性能，并发布CSF-50K数据集（5万对裂缝图像和像素级精确掩码）用于大规模基准测试。

Abstract: Automated crack segmentation is essential for scalable condition assessment of pavements and civil infrastructure, yet practical deployment is limited by scarce pixel-level labels and severe domain shift across sensors, illumination, textures, and annotation conventions. This paper presents CrackSegFlow, a controllable flow-matching synthesis framework that generates photorealistic crack images conditioned on binary masks while preserving strict mask-image alignment. The generator combines topology-preserving mask injection with boundary-gated modulation to maintain thin-structure continuity and suppress texture-driven false positives. A second class-conditional flow-matching model synthesizes crack masks with explicit control over crack coverage, enabling balanced, topology-diverse paired data without additional manual annotation. We further inject crack masks into crack-free backgrounds to diversify illumination and surface artifacts and reduce false positives caused by shadows, joints, and pavement markings. Experiments on five benchmarks spanning four asphalt datasets and the crack class of a concrete-domain dataset demonstrate consistent improvements under an established hybrid CNN--Transformer segmentation backbone and a fixed training protocol. With real plus synthesized pairs, in-domain performance improves on average by 5.37 mIoU and 5.13 F1, and target-guided cross-domain synthesis yields average gains of 13.12 mIoU and 14.82 F1 using only limited target mask statistics. Compared with diffusion-based semantic synthesis, CrackSegFlow provides substantially faster deterministic sampling and improves fidelity and mask-image alignment for thin-structure crack geometry. Finally, we release CSF-50K, a public dataset of 50,000 paired crack images and pixel-accurate masks for large-scale benchmarking of generalizable crack segmentation.

</details>


### [40] [VideoMemory: Toward Consistent Video Generation via Memory Integration](https://arxiv.org/abs/2601.03655)
*Jinsong Zhou,Yihua Du,Xinli Xu,Luozhou Wang,Zijie Zhuang,Yehang Zhang,Shuaibo Li,Xiaojun Hu,Bolan Su,Ying-cong Chen*

Main category: cs.CV

TL;DR: VideoMemory是一个实体中心框架，通过动态记忆库实现跨多镜头视频生成中的实体一致性，解决了现有模型在场景变化或长时间间隔后无法保持实体身份和外观的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然能产生高质量短片，但在叙事视频生成中难以保持角色、道具和环境在不同镜头间的身份和外观一致性，特别是在场景变化或实体长时间间隔后重新出现时。

Method: 采用动态记忆库存储实体视觉和语义描述符，多智能体系统分解叙事脚本，从记忆库检索实体表示，基于检索状态合成关键帧和视频，并在每个镜头后更新记忆库以反映故事驱动变化。

Result: 在包含54个案例的多镜头一致性基准测试中，VideoMemory在角色、道具和背景持续性场景中展现出强大的实体级连贯性和高感知质量。

Conclusion: VideoMemory通过动态记忆库的检索-更新机制，成功实现了跨远距离镜头的实体一致性描绘，支持连贯的长格式视频生成。

Abstract: Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.

</details>


### [41] [MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding](https://arxiv.org/abs/2601.03660)
*Jiangyuan Liu,Hongxuan Ma,Yuhao Zhao,Zhe Liu,Jian Wang,Wei Zou*

Main category: cs.CV

TL;DR: MGPC是一个可泛化的多模态点云补全框架，通过整合点云、RGB图像和文本，使用模态丢弃策略、Transformer融合模块和渐进生成器，在包含100万训练对的大规模数据集MGPC-1M上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法（包括3D CNN、点云和Transformer方法）在合成基准上表现良好，但由于模态限制、可扩展性和生成能力不足，在新物体和真实场景中的泛化能力有限。

Method: 提出MGPC框架：1）整合点云、RGB图像和文本的统一架构；2）创新的模态丢弃策略提高鲁棒性；3）基于Transformer的融合模块；4）新颖的渐进生成器提升几何建模能力；5）构建MGPC-1M大规模数据集（1000+类别，100万训练对）。

Result: 在MGPC-1M数据集和真实世界数据上的广泛实验表明，该方法持续优于现有基线方法，在真实世界条件下表现出强大的泛化能力。

Conclusion: MGPC通过多模态整合和创新的架构设计，解决了点云补全在新物体和真实场景中的泛化挑战，为点云补全领域提供了有效的解决方案。

Abstract: Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.

</details>


### [42] [PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance](https://arxiv.org/abs/2601.03665)
*Siddarth Nilol Kundur Satish,Devesh Jaiswal,Hongyu Chen,Abhishek Bakshi*

Main category: cs.CV

TL;DR: 提出PhysVideoGenerator框架，将可学习的物理先验嵌入视频生成过程，通过预测器网络从扩散潜变量恢复物理特征，并注入到DiT生成器中，验证了联合训练的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型能产生高质量美学视频，但难以学习真实世界物理动态表示，导致物体碰撞不自然、重力不一致、时间闪烁等伪影。

Method: 提出PhysVideoGenerator框架：1) 使用轻量级预测器网络PredictorP从噪声扩散潜变量回归预训练V-JEPA 2提取的高级物理特征；2) 通过专门的交叉注意力机制将预测的物理token注入到DiT生成器(Latte)的时间注意力层中。

Result: 证明了该联合训练范式的技术可行性：扩散潜变量包含足够信息来恢复V-JEPA 2物理表示，多任务优化在训练过程中保持稳定。

Conclusion: 该工作建立了物理感知生成模型未来大规模评估的基础，验证了在视频生成中嵌入可学习物理先验的架构设计和训练稳定性。

Abstract: Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.

</details>


### [43] [TRec: Egocentric Action Recognition using 2D Point Tracks](https://arxiv.org/abs/2601.03667)
*Dennis Holzmann,Sven Wachsmuth*

Main category: cs.CV

TL;DR: 提出了一种利用2D点轨迹作为额外运动线索的自我中心动作识别新方法，通过随机采样图像点并跟踪其轨迹，结合Transformer模型显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心动作识别方法主要依赖RGB外观、人体姿态估计或其组合，缺乏有效的运动信息表示。本文探索如何利用简单的2D点轨迹作为轻量级但有效的运动线索来提升识别性能。

Method: 使用CoTracker跟踪每个视频中随机初始化的点集，将得到的轨迹与对应图像帧一起输入基于Transformer的识别模型。无需检测手、物体或交互区域，仅利用初始帧及其关联的点轨迹即可工作。

Result: 实验结果表明，集成2D点轨迹相比不使用运动信息的相同模型能持续提升性能。即使仅提供初始帧及其点轨迹（不含完整视频序列），也能获得显著性能增益。

Conclusion: 2D点轨迹作为轻量级但有效的运动表示，能显著提升自我中心动作识别性能，为动作理解提供了新的简单而强大的线索来源。

Abstract: We present a novel approach for egocentric action recognition that leverages 2D point tracks as an additional motion cue. While most existing methods rely on RGB appearance, human pose estimation, or their combination, our work demonstrates that tracking randomly sampled image points across video frames can substantially improve recognition accuracy. Unlike prior approaches, we do not detect hands, objects, or interaction regions. Instead, we employ CoTracker to follow a set of randomly initialized points through each video and use the resulting trajectories, together with the corresponding image frames, as input to a Transformer-based recognition model. Surprisingly, our method achieves notable gains even when only the initial frame and its associated point tracks are provided, without incorporating the full video sequence. Experimental results confirm that integrating 2D point tracks consistently enhances performance compared to the same model trained without motion information, highlighting their potential as a lightweight yet effective representation for egocentric action understanding.

</details>


### [44] [BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion](https://arxiv.org/abs/2601.03713)
*Qingyao Tian,Bingyu Yang,Huai Liao,Xinyan Huang,Junyong Li,Dong Yi,Hongbin Liu*

Main category: cs.CV

TL;DR: 提出BREATH-VL框架，结合视觉语言模型（VLMs）的语义理解和视觉配准的几何信息，用于内窥镜6自由度相机定位，在BREATH数据集上实现25.5%的平移误差降低。


<details>
  <summary>Details</summary>
Motivation: 解决VLMs应用于内窥镜6-DoF定位的三个挑战：缺乏大规模高质量医疗定位数据集、细粒度姿态回归能力有限、提取时序特征计算延迟高。利用VLMs的通用语义理解和配准方法的精确几何对齐的互补优势。

Method: 1) 构建BREATH数据集（目前最大的体内内窥镜定位数据集）；2) 提出BREATH-VL混合框架，集成VLMs语义线索和视觉配准几何信息；3) 引入轻量级上下文学习机制，将运动历史编码为语言提示，实现高效时序推理。

Result: 视觉语言模块在挑战性手术场景中实现鲁棒的语义定位。BREATH-VL在准确性和泛化性上优于最先进的视觉定位方法，平移误差比最佳基线降低25.5%，同时保持竞争力的计算延迟。

Conclusion: 通过结合视觉语言模型的语义理解和传统配准方法的几何精度，BREATH-VL框架显著提升了内窥镜6-DoF定位性能，为医疗导航任务提供了有效的解决方案。

Abstract: Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.

</details>


### [45] [Towards Real-world Lens Active Alignment with Unlabeled Data via Domain Adaptation](https://arxiv.org/abs/2601.03718)
*Wenyong Lia,Qi Jiang,Weijian Hu,Kailun Yang,Zhanjun Zhang,Wenjun Tian,Kaiwei Wang,Jian Bai*

Main category: cs.CV

TL;DR: 提出DA3方法，通过少量无标签真实图像进行域适应，解决光学模拟与真实图像间的域差异问题，显著提升主动对准精度，减少98.7%的设备数据收集时间。


<details>
  <summary>Details</summary>
Motivation: 主动对准是大规模自动化组装高精度光学系统的关键技术。传统基于光学模拟的数字孪生管道能生成大规模标注数据，但复杂成像条件导致模拟与真实图像间存在域差异，限制了模拟训练模型的泛化能力。

Method: 提出域自适应主动对准(DA3)方法：1) 使用自回归域变换生成器；2) 采用对抗性特征对齐策略；3) 通过自监督学习提取真实域信息；4) 提取域不变图像退化特征以实现鲁棒的对准预测。

Result: 在两个镜头类型上的实验显示：1) DA3比纯模拟管道精度提升46%；2) 接近使用3个镜头样本精确标注真实数据的性能；3) 设备数据收集时间减少98.7%。

Conclusion: 域适应能有效赋予模拟训练模型鲁棒的真实世界性能，验证了数字孪生管道作为显著提升大规模光学组装效率的实用解决方案。

Abstract: Active Alignment (AA) is a key technology for the large-scale automated assembly of high-precision optical systems. Compared with labor-intensive per-model on-device calibration, a digital-twin pipeline built on optical simulation offers a substantial advantage in generating large-scale labeled data. However, complex imaging conditions induce a domain gap between simulation and real-world images, limiting the generalization of simulation-trained models. To address this, we propose augmenting a simulation baseline with minimal unlabeled real-world images captured at random misalignment positions, mitigating the gap from a domain adaptation perspective. We introduce Domain Adaptive Active Alignment (DA3), which utilizes an autoregressive domain transformation generator and an adversarial-based feature alignment strategy to distill real-world domain information via self-supervised learning. This enables the extraction of domain-invariant image degradation features to facilitate robust misalignment prediction. Experiments on two lens types reveal that DA3 improves accuracy by 46% over a purely simulation pipeline. Notably, it approaches the performance achieved with precisely labeled real-world data collected on 3 lens samples, while reducing on-device data collection time by 98.7%. The results demonstrate that domain adaptation effectively endows simulation-trained models with robust real-world performance, validating the digital-twin pipeline as a practical solution to significantly enhance the efficiency of large-scale optical assembly.

</details>


### [46] [CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval](https://arxiv.org/abs/2601.03728)
*Zhipeng Qian,Zihan Liang,Yufei Ma,Ben Chen,Huangyu Dai,Yiwei Ma,Jiayi Ji,Chenyi Lei,Han Li,Xiaoshuai Sun*

Main category: cs.CV

TL;DR: CSMCIR提出统一表示框架解决组合图像检索中的表示空间碎片化问题，通过多级思维链提示、对称双塔架构和动态记忆库实现高效查询-目标对齐，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在表示空间碎片化问题：查询和目标由异构模态组成，由不同编码器处理，迫使模型仅通过事后对齐来桥接未对齐的表示空间，这从根本上限制了检索性能。这种架构不对称性在特征空间中表现为三个分离的聚类，直接展示了异构模态如何从初始化就创建了根本未对齐的表示空间。

Method: 1. 多级思维链提示策略：引导多模态大语言模型为目标图像生成具有区分性、语义兼容的标题，建立模态对称性；2. 对称双塔架构：查询和目标两侧使用相同的共享参数Q-Former进行跨模态编码，确保一致的特征表示；3. 基于熵的时间动态记忆库策略：提供高质量负样本，同时保持与演化模型状态的一致性。

Result: 在四个基准数据集上的广泛实验表明，CSMCIR实现了最先进的性能，并具有优越的训练效率。全面的消融研究进一步验证了每个提出组件的有效性。

Conclusion: CSMCIR通过统一表示框架解决了组合图像检索中的表示空间碎片化问题，实现了高效的查询-目标对齐，显著提升了检索性能。

Abstract: Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.

</details>


### [47] [MATANet: A Multi-context Attention and Taxonomy-Aware Network for Fine-Grained Underwater Recognition of Marine Species](https://arxiv.org/abs/2601.03729)
*Donghwan Lee,Byeongjin Kim,Geunhee Kim,Hyukjin Kwon,Nahyeon Maeng,Wooju Kim*

Main category: cs.CV

TL;DR: 提出MATANet模型，通过多上下文环境注意力模块和分层分离诱导学习模块，结合环境上下文和生物分类学层次结构，提升海洋物种细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视环境上下文交互，且未能充分融入海洋生物分类学的层次结构，限制了海洋动物细粒度分类的效果，而这对生态学、生物多样性保护和政策制定至关重要。

Method: 提出MATANet模型，包含两个核心组件：1) 多上下文环境注意力模块(MCEAM)，学习感兴趣区域与周围环境的关系；2) 分层分离诱导学习模块(HSLM)，将分类学层次结构编码到特征空间。模型结合实例特征、环境特征和分类结构进行细粒度分类。

Result: 在FathomNet2025、FAIR1M和LifeCLEF2015-Fish数据集上实现了最先进的性能。

Conclusion: MATANet通过模拟专家策略，利用分类学知识和环境上下文来解释水下动物的模糊特征，有效提升了海洋物种细粒度分类的准确性。

Abstract: Fine-grained classification of marine animals supports ecology, biodiversity and habitat conservation, and evidence-based policy-making. However, existing methods often overlook contextual interactions from the surrounding environment and insufficiently incorporate the hierarchical structure of marine biological taxonomy. To address these challenges, we propose MATANet (Multi-context Attention and Taxonomy-Aware Network), a novel model designed for fine-grained marine species classification. MATANet mimics expert strategies by using taxonomy and environmental context to interpret ambiguous features of underwater animals. It consists of two key components: a Multi-Context Environmental Attention Module (MCEAM), which learns relationships between regions of interest (ROIs) and their surrounding environments, and a Hierarchical Separation-Induced Learning Module (HSLM), which encodes taxonomic hierarchy into the feature space. MATANet combines instance and environmental features with taxonomic structure to enhance fine-grained classification. Experiments on the FathomNet2025, FAIR1M, and LifeCLEF2015-Fish datasets demonstrate state-of-the-art performance. The source code is available at: https://github.com/dhlee-work/fathomnet-cvpr2025-ssl

</details>


### [48] [RadDiff: Describing Differences in Radiology Image Sets with Natural Language](https://arxiv.org/abs/2601.03733)
*Xiaoxian Shen,Yuhui Zhang,Sahithi Ankireddy,Xiaohan Wang,Maya Varma,Henry Guo,Curtis Langlotz,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: RadDiff是一个多模态智能系统，通过放射科医生风格的比较推理来描述配对放射学研究之间的临床意义差异，在RadDiffBench基准上达到47%准确率。


<details>
  <summary>Details</summary>
Motivation: 理解放射学图像集之间的差异对于生成临床见解和解释医疗AI系统至关重要。目前缺乏系统的方法和基准来揭示放射学数据中有意义的差异。

Method: 基于VisDiff的提议者-排序者框架，结合四个创新：1) 通过领域适应视觉语言模型注入医学知识；2) 整合图像与临床报告的多模态推理；3) 多轮推理的迭代假设细化；4) 定位并放大显著区域的针对性视觉搜索。

Result: 在RadDiffBench基准（57个专家验证的放射学研究对）上，RadDiff达到47%准确率，使用真实报告引导时达到50%，显著优于通用领域VisDiff基线。系统在COVID-19表型比较、种族亚组分析和生存相关影像特征发现等任务中表现出多功能性。

Conclusion: RadDiff和RadDiffBench为系统揭示放射学数据中有意义的差异提供了首个方法和基准基础，展示了在多样化临床任务中的实用性。

Abstract: Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.

</details>


### [49] [HyperCOD: The First Challenging Benchmark and Baseline for Hyperspectral Camouflaged Object Detection](https://arxiv.org/abs/2601.03736)
*Shuyan Bai,Tingfa Xu,Peifu Liu,Yuhao Qiu,Huiyan Bai,Huan Chen,Yanyan Peng,Jianan Li*

Main category: cs.CV

TL;DR: 提出首个大规模高光谱伪装目标检测基准HyperCOD，并开发了基于SAM的HSC-SAM模型，通过空间-光谱解耦方法有效解决模态差异问题


<details>
  <summary>Details</summary>
Motivation: RGB图像在颜色和纹理线索模糊的真实场景中检测伪装目标存在困难，而高光谱图像虽能提供精细光谱特征，但缺乏专门的大规模基准数据集阻碍了高光谱伪装目标检测的发展

Method: 提出HyperSpectral Camouflage-aware SAM (HSC-SAM)，将高光谱图像解耦为空间图（输入SAM图像编码器）和光谱显著性图（作为自适应提示），有效桥接模态差异

Result: HSC-SAM在HyperCOD基准上达到新的SOTA性能，并在其他公开HSI数据集上展现出强大的泛化能力

Conclusion: HyperCOD数据集和HSC-SAM基线为这一新兴领域提供了坚实基础，将推动未来研究发展

Abstract: RGB-based camouflaged object detection struggles in real-world scenarios where color and texture cues are ambiguous. While hyperspectral image offers a powerful alternative by capturing fine-grained spectral signatures, progress in hyperspectral camouflaged object detection (HCOD) has been critically hampered by the absence of a dedicated, large-scale benchmark. To spur innovation, we introduce HyperCOD, the first challenging benchmark for HCOD. Comprising 350 high-resolution hyperspectral images, It features complex real-world scenarios with minimal objects, intricate shapes, severe occlusions, and dynamic lighting to challenge current models. The advent of foundation models like the Segment Anything Model (SAM) presents a compelling opportunity. To adapt the Segment Anything Model (SAM) for HCOD, we propose HyperSpectral Camouflage-aware SAM (HSC-SAM). HSC-SAM ingeniously reformulates the hyperspectral image by decoupling it into a spatial map fed to SAM's image encoder and a spectral saliency map that serves as an adaptive prompt. This translation effectively bridges the modality gap. Extensive experiments show that HSC-SAM sets a new state-of-the-art on HyperCOD and generalizes robustly to other public HSI datasets. The HyperCOD dataset and our HSC-SAM baseline provide a robust foundation to foster future research in this emerging area.

</details>


### [50] [I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing](https://arxiv.org/abs/2601.03741)
*Jinghan Yu,Junhao Xiao,Chenyu Zhu,Jiaming Li,Jia Li,HanMing Deng,Xirui Wang,Guoli Jia,Jianjun Li,Zhiyuan Ma,Xiang Bai,Bowen Zhou*

Main category: cs.CV

TL;DR: I2E提出了一种新的"分解-行动"图像编辑范式，将图像分解为可操作的对象层，通过物理感知的视觉-语言-动作代理将复杂指令解析为原子操作序列，显著提升了组合编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本引导的图像编辑方法主要依赖端到端的像素级修复范式，在需要精确局部控制和复杂多对象空间推理的组合编辑任务中存在严重局限性：1) 规划与执行的隐式耦合；2) 缺乏对象级控制粒度；3) 依赖非结构化、像素中心的建模。

Method: 提出I2E框架，采用"分解-行动"范式：1) 使用分解器将非结构化图像转换为离散、可操作的对象层；2) 引入物理感知的视觉-语言-动作代理，通过思维链推理将复杂指令解析为一系列原子操作；3) 构建I2E-Bench基准测试集。

Result: 在I2E-Bench和多个公共基准测试上的实验结果表明，I2E在处理复杂组合指令、保持物理合理性以及确保多轮编辑稳定性方面显著优于现有最先进方法。

Conclusion: I2E通过结构化环境中的可操作交互过程重新构想图像编辑，解决了现有像素级修复范式的根本局限性，为复杂组合编辑任务提供了更精确、更可控的解决方案。

Abstract: Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel "Decompose-then-Action" paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.

</details>


### [51] [MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction](https://arxiv.org/abs/2601.03781)
*Xiaokun Sun,Zezhong Wu,Zewen Ding,Linli Xu*

Main category: cs.CV

TL;DR: 提出MVP（Masked Video Prediction）作为VideoLLMs的后训练目标，通过重建被遮蔽的连续视频片段来增强模型对时序逻辑和因果关系的理解，使用GRPO和细粒度奖励函数进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的VideoLLMs后训练方法主要关注整体内容理解（如字幕生成或视频问答），缺乏对内在时序连贯性和帧间相关性的显式监督，限制了模型捕捉复杂动态和细粒度视觉因果关系的能力。

Method: 提出MVP后训练目标，要求模型从一组具有挑战性的干扰项中重建被遮蔽的连续视频片段；开发可扩展的数据合成管道，将任意视频语料库转换为MVP训练样本；采用Group Relative Policy Optimization（GRPO）和细粒度奖励函数来增强模型对视频上下文和时序属性的理解。

Result: 综合评估表明，MVP通过直接强化时序推理和因果理解，显著提升了视频推理能力。

Conclusion: MVP作为一种新颖的后训练目标，能够有效弥补现有VideoLLMs在时序连贯性和因果理解方面的不足，为视频大语言模型提供了更强大的时序推理能力。

Abstract: Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.

</details>


### [52] [A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products](https://arxiv.org/abs/2601.03784)
*Steven Moonen,Rob Salaets,Kenneth Batstone,Abdellatif Bey-Temsamani,Nick Michiels*

Main category: cs.CV

TL;DR: 本文探讨在农业行业中，当缺乏CAD模型时，如何通过替代技术生成合成数据来训练AI物体检测模型，用于区分石头和马铃薯。


<details>
  <summary>Details</summary>
Motivation: 在农业行业中，由于缺乏现成的CAD模型，难以利用合成数据来训练AI视觉系统。传统制造业中可以通过CAD模型生成合成数据来减少真实数据的需求，但农业领域缺乏这种资源，需要寻找替代方案。

Method: 提出了多种替代CAD文件的合成数据生成技术：1）使用扫描获得的高代表性3D模型；2）使用图像到3D的方法生成模型。将这些技术生成的合成数据用于训练物体检测模型，并在小型真实数据集上进行微调。

Result: 实验表明，使用高代表性的3D模型（通过扫描或图像到3D方法获得）可以生成有效的合成数据用于训练物体检测模型。即使使用代表性较低的模型，通过在小规模真实数据集上进行微调，也能显著提升模型性能，甚至达到相似的效果。

Conclusion: 在农业等缺乏CAD模型的行业中，通过扫描或图像到3D技术获得3D模型来生成合成数据是可行的解决方案。结合小规模真实数据的微调，可以有效训练AI物体检测模型，降低数据获取成本。

Abstract: In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.

</details>


### [53] [From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs](https://arxiv.org/abs/2601.03808)
*Usha Shrestha,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 该论文提出了一种性能感知的闭环解决方案，让LLM能够通过内化经验性能线索来自主设计最优的代码增强变换，相比暴力搜索减少了600倍的候选评估，同时保持竞争性峰值准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码合成方面表现突出，但数据增强仍然依赖于启发式设计或暴力方法，需要更智能、性能感知的自动化解决方案。

Method: 在NNGPT生态系统中，使用低秩适应（LoRA）微调LLM，训练数据包含6000多个经验评估的PyTorch增强函数，每个仅用下游模型准确率标注。采用成对性能排序（更好-更差变换）进行训练，无需强化学习、奖励模型或符号目标。

Result: 相比暴力搜索，该方法减少了高达600倍的候选评估次数，同时保持竞争性峰值准确率。模型从随机合成转向任务对齐设计，能够内化语义性能线索而非记忆语法。

Conclusion: LLM可以通过非文本反馈循环展现任务级推理能力，绕过显式符号奖励。结构化思维链提示会引入语法噪声并降低性能，而直接提示在性能关键代码任务中能确保稳定优化。

Abstract: Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.

</details>


### [54] [EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging](https://arxiv.org/abs/2601.03811)
*Jan Tagscherer,Sarah de Boer,Lena Philipp,Fennie van der Graaf,Dré Peeters,Joeran Bosma,Lars Leijten,Bogdan Obreja,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: EvalBlocks是一个模块化、即插即用的框架，用于在医学影像基础模型开发过程中进行高效评估，解决了手动跟踪实验的繁琐问题。


<details>
  <summary>Details</summary>
Motivation: 医学影像基础模型开发需要持续监控下游性能，但研究人员通常依赖临时的手动工作流程来跟踪大量实验、设计选择及其对性能的影响，这种方法既缓慢又容易出错。

Method: 基于Snakemake构建的模块化框架，支持无缝集成新数据集、基础模型、聚合方法和评估策略，具有集中跟踪、单命令可重现、高效缓存和并行执行等特点。

Result: 在五个最先进的基础模型和三个医学影像分类任务上进行了演示，EvalBlocks能够简化模型评估流程，使研究人员能够更快地迭代，专注于模型创新而非评估后勤工作。

Conclusion: EvalBlocks框架作为开源软件发布，为医学影像基础模型开发提供了高效、可扩展的评估解决方案，显著提升了研究效率。

Abstract: Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.

</details>


### [55] [IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting](https://arxiv.org/abs/2601.03824)
*Wei Long,Haifeng Wu,Shiyin Jiang,Jinhua Zhang,Xinchun Ji,Shuhang Gu*

Main category: cs.CV

TL;DR: IDESplat提出迭代深度概率提升方法，通过级联warp操作和多层DPBU单元，逐步优化深度估计，实现更精确的3D高斯均值预测，在重建质量和泛化能力上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的泛化方法通常依赖单一warp操作估计深度概率，这限制了跨视图几何线索的充分利用，导致深度图不稳定且粗糙，进而影响高斯均值的预测精度。

Method: 提出IDESplat框架：1) 引入深度概率提升单元(DPBU)，通过级联warp操作以乘法方式集成极线注意力图；2) 堆叠多个DPBU构建迭代深度估计过程，逐步识别高概率深度候选；3) 迭代提升深度概率估计并更新深度候选，逐步细化深度图。

Result: 在RealEstate10K上比DepthSplat提升0.33dB PSNR，仅使用10.7%参数和70%内存；在DTU跨数据集实验中比DepthSplat提升2.95dB PSNR；在ACID和DL3DV数据集上也取得最先进性能，同时保持实时效率。

Conclusion: IDESplat通过迭代深度概率提升机制有效解决了单warp操作的不稳定性问题，显著提高了深度估计精度，从而实现了更准确的高斯均值预测，在重建质量、泛化能力和计算效率方面均表现出色。

Abstract: Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.

</details>


### [56] [FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images](https://arxiv.org/abs/2601.03884)
*Sanidhya Ghosal,Anurag Sharma,Sushil Ghildiyal,Mukesh Saini*

Main category: cs.CV

TL;DR: 本文提出FLNet深度学习架构，通过超分辨率将Sentinel-2卫星图像从10米提升到3米分辨率，用于洪水后农作物损害评估，在BFCD-22数据集上显著提高了"完全损害"类别的F1分数。


<details>
  <summary>Details</summary>
Motivation: 洪水后政府救灾面临挑战，印度农作物受洪水影响广泛，需要快速准确的损害评估。传统人工调查速度慢且有偏差，现有卫星方法面临云层覆盖和低空间分辨率问题。

Method: 提出FLNet深度学习架构，结合超分辨率技术将Sentinel-2卫星图像的10米空间分辨率提升到3米，然后进行损害分类。在Bihar Flood Impacted Croplands Dataset (BFCD-22)上测试。

Result: 模型将关键的"完全损害"F1分数从0.83提高到0.89，几乎与商业高分辨率图像的0.89分数相当，展示了成本效益和可扩展性。

Conclusion: 这项工作提供了成本效益高且可扩展的解决方案，为从人工评估向自动化、高保真损害评估的全国性转变铺平了道路。

Abstract: Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical "Full Damage" F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.

</details>


### [57] [HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis](https://arxiv.org/abs/2601.03915)
*Julie van Logtestijn,Petru Manescu*

Main category: cs.CV

TL;DR: HemBLIP是一个视觉语言模型，用于生成外周血细胞的可解释形态学描述，在白血病诊断中提供透明分析，优于现有模型且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前白血病诊断中的深度学习模型多为黑盒，缺乏可解释性，限制了临床信任和应用。需要开发能够生成形态学描述的可解释模型来提高诊断透明度。

Method: 构建了包含1.4万个健康与白血病细胞及专家属性标注的新数据集，采用通用视觉语言模型进行全微调和LoRA参数高效训练，并与生物医学基础模型MedGEMMA进行基准比较。

Result: HemBLIP在描述质量和形态准确性方面表现更优，LoRA适应进一步提升了性能并显著降低了计算成本。

Conclusion: 视觉语言模型在血液学诊断中具有实现透明、可扩展分析的潜力，为临床信任和采用提供了有前景的解决方案。

Abstract: Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.

</details>


### [58] [FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection](https://arxiv.org/abs/2601.03928)
*Mingyu Ouyang,Kevin Qinghong Lin,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: FocusUI：一个高效的UI grounding框架，通过选择与指令最相关的视觉补丁并保持位置连续性，显著减少视觉token数量，提升推理速度和降低内存使用


<details>
  <summary>Details</summary>
Motivation: 当前VLM在UI grounding任务中处理高分辨率截图时会产生数千个视觉token，导致计算开销大且注意力稀释。而人类通常只关注UI中的感兴趣区域，因此需要更高效的UI grounding方法

Method: 提出FocusUI框架：1) 通过融合指令条件分数和基于规则的UI图分数构建补丁级监督，消除冗余视觉token；2) 引入PosPad策略，将连续丢弃的视觉token压缩为特殊标记以保持位置连续性

Result: 在四个grounding基准测试中超越GUI特定基线。在ScreenSpot-Pro基准上，FocusUI-7B比GUI-Actor-7B性能提升3.7%。即使只保留30%视觉token，性能仅下降3.2%，同时推理速度提升1.44倍，峰值GPU内存降低17%

Conclusion: FocusUI通过选择性视觉token保留和位置连续性保持，实现了高效且准确的UI grounding，为实际应用提供了可行的解决方案

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.

</details>


### [59] [ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.03955)
*Xu Zhang,Cheng Da,Huan Yang,Kun Gai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: ResTok提出了一种新的视觉tokenizer，通过构建层次化残差表示来改进自回归图像生成，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型设计的1D视觉tokenizer忽略了视觉数据的层次性和残差结构特性，导致表示能力不足。需要将视觉特有的层次残差先验重新引入视觉tokenization中。

Method: 提出ResTok，构建图像token和潜在token的层次化残差表示，通过渐进合并实现跨层次特征融合，同时语义残差防止信息重叠。还设计了层次化自回归生成器，一次预测整个层次的潜在token来加速生成。

Result: 在ImageNet-256上达到gFID 2.34，仅需9个采样步骤，显著优于现有方法。代码已开源。

Conclusion: 在视觉tokenization中恢复层次残差先验能显著提升自回归图像生成性能，ResTok为视觉tokenizer设计提供了新思路。

Abstract: Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.

</details>


### [60] [FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion](https://arxiv.org/abs/2601.03959)
*Enes Duran,Nikos Athanasiou,Muhammed Kocabas,Michael J. Black,Omid Taheri*

Main category: cs.CV

TL;DR: 提出了FUSION，首个基于扩散模型的无条件全身运动先验，联合建模身体和手部运动，解决了现有方法忽略手部或任务范围狭窄的问题。


<details>
  <summary>Details</summary>
Motivation: 手部对于与环境交互和传达手势至关重要，但现有运动合成方法要么忽略手部运动，要么只能在高度受限的设置下生成狭窄任务的全身运动。主要障碍是缺乏大规模数据集来联合捕捉多样化的全身运动和详细的手部关节。

Method: 1) 整理并统一现有的手部运动数据集与大规模身体运动数据，生成包含手部和身体的全身序列；2) 提出FUSION，首个基于扩散模型的无条件全身运动先验，联合建模身体和手部运动；3) 开发优化流程，通过精炼扩散模型的潜在空间来生成特定任务运动。

Result: 在HumanML3D数据集的关键点跟踪任务上超越了最先进的骨骼控制模型，并实现了更优越的运动自然性。在应用方面：1) 能够在给定物体运动时生成包含手指交互的详细全身运动；2) 使用LLM将自然语言线索转化为可操作的运动约束，生成自我交互运动。

Conclusion: FUSION能够精确控制手部运动，同时保持合理的全身协调性，为全身运动合成提供了更全面的解决方案，代码将公开。

Abstract: Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.

</details>


### [61] [PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography](https://arxiv.org/abs/2601.03993)
*Junle Liu,Peirong Zhang,Yuyi Zhang,Pengyu Yan,Hui Zhou,Xinyue Zhou,Fengjun Guo,Lianwen Jin*

Main category: cs.CV

TL;DR: PosterVerse：全流程商业级海报生成方法，通过LLM蓝图创建、扩散模型背景生成和MLLM驱动的HTML引擎，解决现有系统设计流程不完整、文本渲染精度差等问题，并引入首个中文HTML排版数据集PosterDNA。


<details>
  <summary>Details</summary>
Motivation: 商业级海报设计需要美学吸引力与精确信息传递的完美结合。现有自动化海报生成系统存在设计流程不完整、文本渲染精度差、商业应用灵活性不足等显著限制。

Method: PosterVerse采用三阶段全流程方法：1) 使用微调LLM从用户需求提取关键设计元素创建蓝图；2) 通过定制扩散模型生成视觉吸引人的图形背景；3) 使用MLLM驱动的HTML引擎进行统一布局-文本渲染，确保高文本精度和灵活定制。同时引入PosterDNA数据集，这是首个引入HTML排版文件的中文海报生成数据集。

Result: 实验结果表明，PosterVerse能持续生成商业级海报，具有吸引人的视觉效果、准确的文本对齐和可定制的布局，为自动化商业海报设计提供了有前景的解决方案。

Conclusion: PosterVerse通过全流程自动化解决了商业海报生成的关键挑战，结合PosterDNA数据集的高密度文本渲染能力，为商业级海报设计自动化提供了有效解决方案。

Abstract: Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.

</details>


### [62] [Padé Neurons for Efficient Neural Models](https://arxiv.org/abs/2601.04005)
*Onur Keleş,A. Murat Tekalp*

Main category: cs.CV

TL;DR: 提出Padé神经元(Paons)，一种基于Padé逼近的新型非线性神经元模型，比传统McCulloch-Pitts神经元具有更强的非线性能力，能在更少层数下实现更好性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络使用McCulloch-Pitts神经元模型（线性模型加逐点非线性激活），虽然已有二次神经元、广义运算神经元等非线性模型，但仍需更强的非线性表达能力。作者希望开发一种更优的非线性神经元模型。

Method: 受Padé逼近启发，提出Padé神经元(Paons)模型。Paons学习输入的不同非线性函数，提供多样化的非线性。该模型包含所有先前提出的神经元模型作为特例，可将任何网络中的神经元替换为Paons。

Result: 在基于ResNet架构的图像超分辨率、压缩和分类模型中，用Paons替换经典神经元。实验结果表明，Paons构建的神经模型在更少层数下提供优于或等于经典对应模型的性能。

Conclusion: Padé神经元(Paons)是一种优越的非线性神经元模型，具有非线性多样性、层效率高等优点，能提升神经网络性能并减少所需层数，已开源实现。

Abstract: Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.

</details>


### [63] [Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model](https://arxiv.org/abs/2601.04033)
*Yuan Wang,Borui Liao,Huijuan Huang,Jinda Lu,Ouxiang Li,Kuien Liu,Meng Wang,Xiang Wang*

Main category: cs.CV

TL;DR: 提出REACT框架级奖励模型，专门评估生成视频中的结构失真问题，包括异常物体外观和交互，通过两阶段训练和动态采样机制实现精准评估。


<details>
  <summary>Details</summary>
Motivation: 现有视频奖励模型主要关注视觉质量、运动质量和文本对齐，但忽略了关键的结构失真问题（如异常物体外观和交互），这些失真会降低生成视频的整体质量。

Method: 1) 构建大规模人类偏好数据集，基于提出的结构失真分类法进行标注；2) 使用高效的思维链合成管道生成额外数据；3) 采用两阶段训练框架：监督微调（带掩码损失）注入领域知识，然后强化学习（GRPO和成对奖励）增强推理能力；4) 推理时引入动态采样机制聚焦最可能失真的帧。

Result: REACT在评估结构失真方面补充了现有奖励模型，实现了准确的定量评估和可解释的归因分析，并提出了REACT-Bench基准用于生成视频失真评估。

Conclusion: REACT框架级奖励模型能有效识别和评估生成视频中的结构失真问题，为视频生成质量评估提供了重要补充，具有准确的定量评估和可解释的归因分析能力。

Abstract: Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.

</details>


### [64] [Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation](https://arxiv.org/abs/2601.04065)
*Raül Pérez-Gonzalo,Riccardo Magro,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种标注高效的叶片分割方法，将像素级任务重构为二值区域分类问题，使用无监督区域生长和区域混合增强技术


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机需要频繁检查，传统基于深度学习的像素级分割方法需要大量标注数据，存在可扩展性挑战

Method: 将像素级分割重构为二值区域分类，使用无监督的模块化自适应区域生长技术生成区域，结合自适应阈值和区域合并，并引入RegionMix增强策略

Result: 实现了最先进的分割精度和强大的跨站点泛化能力，在不同风电场中都能稳定分割涡轮叶片

Conclusion: 提出的标注高效分割框架在减少标注需求的同时，实现了高精度和强泛化能力，为风力涡轮机自动化检查提供了可行方案

Abstract: Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.

</details>


### [65] [Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models](https://arxiv.org/abs/2601.04068)
*Zitong Huang,Kaidong Zhang,Yukang Ding,Chao Gao,Rui Ding,Ying Chen,Wangmeng Zuo*

Main category: cs.CV

TL;DR: LocalDPO：一种新颖的文本到视频扩散模型后训练框架，通过在时空区域级别构建局部偏好对来优化对齐，无需外部评判模型或人工标注，显著提升视频质量和人类偏好评分。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化（DPO）方法依赖多样本排序和特定任务的评判模型，效率低下且常产生模糊的全局监督。需要一种更高效、细粒度的视频生成器对齐方法。

Method: 提出LocalDPO框架：1）自动化构建局部偏好对数据，将高质量真实视频作为正样本，通过随机时空掩码局部破坏并仅用冻结基础模型恢复掩码区域生成负样本；2）引入区域感知DPO损失，将偏好学习限制在破坏区域以实现快速收敛。

Result: 在Wan2.1和CogVideoX上的实验表明，LocalDPO相比其他后训练方法，在视频保真度、时间连贯性和人类偏好评分方面均有持续提升。

Conclusion: LocalDPO为视频生成器对齐建立了一个更高效、细粒度的范式，通过局部偏好学习和自动化数据收集，显著提升了文本到视频扩散模型与人类偏好的对齐效果。

Abstract: Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.

</details>


### [66] [Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts](https://arxiv.org/abs/2601.04073)
*Zhihao Zhu,Jiafeng Liang,Shixin Jiang,Jinlan Fu,Ming Liu,Guanglu Sun,See-Kiong Ng,Bing Qin*

Main category: cs.CV

TL;DR: 论文发现大型多模态模型在视频推理中存在"文本惯性"问题：一旦推理链中出现文本幻觉，模型会盲目坚持错误文本而忽略冲突的视觉证据。作者提出LogicGraph扰动协议评估模型自反思能力，并开发了无需训练的推理范式来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在视频推理中表现出强大的链式思维能力，但其推理链的鲁棒性存在问题。研究发现模型存在"文本惯性"的失败模式：当推理过程中出现文本幻觉时，模型会盲目坚持错误文本而忽略冲突的视觉证据，这严重影响了推理的可靠性。

Method: 1. 提出LogicGraph扰动协议，在多样化LMMs的推理链中结构化注入扰动，评估其自反思能力；2. 提出主动视觉上下文精炼方法，这是一种无需训练的推理范式，包含主动视觉重定位机制（进行细粒度验证）和自适应上下文精炼策略（总结和去噪推理历史）。

Result: 实验结果显示：1. 模型成功自我纠正的比例不到10%；2. 模型主要表现出盲目的文本错误传播；3. 提出的主动视觉上下文精炼方法能显著抑制幻觉传播，增强推理鲁棒性。

Conclusion: 大型多模态模型在视频推理中存在严重的文本惯性问题，导致推理链脆弱。提出的LogicGraph扰动协议能有效评估模型自反思能力，而主动视觉上下文精炼方法能有效缓解幻觉传播，提升推理鲁棒性，且无需额外训练。

Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.

</details>


### [67] [Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction](https://arxiv.org/abs/2601.04090)
*Jiaxin Huang,Yuanbo Yang,Bangbang Yang,Lin Ma,Yuewen Ma,Yiyi Liao*

Main category: cs.CV

TL;DR: Gen3R是一种结合重建模型和视频扩散模型先验的场景级3D生成方法，通过生成解耦但对齐的几何和外观潜在表示，同时产生RGB视频和对应的3D几何信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景级3D生成中面临挑战，需要将强大的重建模型先验与视频扩散模型结合起来，实现更鲁棒和高质量的3D场景生成。

Method: 重新利用VGGT重建模型生成几何潜在表示，通过训练适配器使其与预训练视频扩散模型的外观潜在表示对齐，联合生成解耦但对齐的几何和外观潜在表示。

Result: 在单图像和多图像条件下的3D场景生成任务中达到最先进水平，能够同时生成RGB视频和对应的3D几何信息（相机姿态、深度图、全局点云），并通过生成先验增强重建鲁棒性。

Conclusion: Gen3R展示了紧密耦合重建模型和生成模型的相互益处，为场景级3D生成提供了有效框架，证明了结合两种模型先验的优势。

Abstract: We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.

</details>


### [68] [GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning](https://arxiv.org/abs/2601.04118)
*Wenshuai Li,Xiantai Xiang,Zixiao Wen,Guangyao Zhou,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuxin Hu*

Main category: cs.CV

TL;DR: GeoReason框架通过构建逻辑驱动数据集和两阶段训练策略，解决遥感视觉语言模型中的逻辑幻觉问题，提升空间推理的认知可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前遥感视觉语言模型存在逻辑幻觉问题，即正确答案来自错误推理链或依赖位置捷径而非空间逻辑，这削弱了复杂空间任务中的认知可靠性。需要从感知为中心转向高级演绎推理。

Method: 1. 构建GeoReason-Bench数据集：包含4000个从几何基元和专家知识合成的推理轨迹；2. 两阶段训练策略：监督知识初始化（学习推理语法和领域知识）和一致性感知强化学习（通过逻辑一致性奖励惩罚逻辑漂移）。

Result: 实验结果表明，该框架显著提升了遥感视觉语言模型的认知可靠性和可解释性，相比其他先进方法取得了最先进的性能。

Conclusion: GeoReason框架通过同步内部思维与最终决策，有效解决了遥感视觉语言模型中的逻辑幻觉问题，为复杂空间决策任务提供了更可靠的认知基础。

Abstract: The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.

</details>


### [69] [Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images](https://arxiv.org/abs/2601.04127)
*Leandro Stival,Ricardo da Silva Torres,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出基于像素级二维表示的多模态自监督方法PIMC，利用递归图编码卫星图像时间序列，结合遥感影像进行对比学习，在多个地球观测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 卫星持续产生大量地球观测数据，特别是卫星图像时间序列(SITS)。现有深度学习模型主要处理完整图像或时间序列，难以有效提取像素级特征。需要更有效的方法来编码SITS中的视觉属性变化。

Method: 提出像素级多模态对比学习(PIMC)：1) 将基于像素的植被指数时间序列(NDVI、EVI、SAVI)转换为递归图作为二维表示；2) 结合遥感影像(RSI)进行多模态自监督对比学习；3) 生成有效的编码器用于下游任务。

Result: 在PASTIS数据集上进行像素级预测和分类，在EuroSAT数据集上进行土地覆盖分类。实验表明：二维表示显著提升SITS特征提取能力，对比学习改善像素时间序列和RSI表示质量，多模态方法在各项任务中优于现有SOTA方法。

Conclusion: 提出的多模态方法为处理SITS和RSI建立了鲁棒的自监督框架，二维表示和对比学习的结合有效提升了地球观测任务性能，代码已开源。

Abstract: Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on

</details>


### [70] [Klear: Unified Multi-Task Audio-Video Joint Generation](https://arxiv.org/abs/2601.04151)
*Jun Wang,Chunyu Qiang,Yuxin Guo,Yiran Wang,Xijuan Zeng,Chen Zhang,Pengfei Wan*

Main category: cs.CV

TL;DR: Klear是一个音频-视频联合生成系统，通过改进模型架构、训练策略和数据管理，解决了现有方法中的音频-视觉不同步、唇语对齐差和单模态退化等问题，在多个任务上大幅超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 当前音频-视频联合生成方法存在三个主要问题：音频-视觉不同步、唇语-语音对齐差、单模态退化。这些问题源于音频-视觉对应建模弱、泛化能力有限、高质量密集标注数据稀缺。需要从模型架构、训练策略和数据管理三个维度进行系统性改进。

Method: 1. 架构：采用单塔设计，统一DiT块和Omni-Full Attention机制，实现紧密的音频-视觉对齐和强可扩展性。
2. 训练：采用渐进式多任务策略，包括随机模态掩码、跨任务联合优化和多阶段课程学习，增强鲁棒表示，强化A-V对齐的世界知识，防止单模态崩溃。
3. 数据：构建首个大规模带密集标注的音频-视频数据集，引入自动化数据构建流程，标注和过滤数百万个多样化、高质量、严格对齐的音频-视频-标注三元组。

Result: Klear能够扩展到大规模数据集，在联合和单模态设置下都能生成高保真、语义和时间对齐、遵循指令的内容，并在分布外场景中表现出强大的泛化能力。在所有任务上大幅超越先前方法，性能可与Veo 3相媲美。

Conclusion: Klear通过系统性的三轴改进（架构、训练、数据），为下一代音频-视频合成提供了统一、可扩展的路径，解决了现有方法的根本问题，实现了高质量的音频-视觉对齐生成。

Abstract: Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.

</details>


### [71] [Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning](https://arxiv.org/abs/2601.04153)
*Yifan Wang,Yanyu Li,Sergey Tulyakov,Yun Fu,Anil Kag*

Main category: cs.CV

TL;DR: 提出Diffusion-DRF方法，使用冻结的视觉语言模型作为免训练评判器，通过可微分奖励流微调视频扩散模型，无需额外奖励模型或偏好数据集


<details>
  <summary>Details</summary>
Motivation: 当前基于DPO的方法依赖不可微分的偏好信号（人工标注或学习型奖励模型），导致训练标签密集、易产生偏见、易被欺骗，常引发奖励攻击和不稳定训练

Method: 使用冻结的现成视觉语言模型作为免训练评判器，通过可微分奖励流直接反向传播VLM反馈到扩散去噪链中，将logit级响应转换为token感知梯度；提出自动化、方面结构化的提示流程获取可靠的多维VLM反馈，使用梯度检查点实现高效更新

Result: Diffusion-DRF提高了视频质量和语义对齐，同时缓解了奖励攻击和崩溃问题，无需额外奖励模型或偏好数据集；方法具有模型无关性，可泛化到其他基于扩散的生成任务

Conclusion: Diffusion-DRF提供了一种可微分、免训练的奖励流方法，有效解决了当前DPO方法的局限性，为视频扩散模型微调提供了更稳定、高效的解决方案

Abstract: Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.

</details>


### [72] [ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography](https://arxiv.org/abs/2601.04159)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: ToTMNet：一种轻量级rPPG架构，使用FFT加速的Toeplitz时间混合层替代注意力机制，在保持高性能的同时大幅减少参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有深度rPPG模型虽然提高了鲁棒性，但计算成本和参数数量增加，注意力机制的时间建模带来与时间长度平方相关的计算复杂度。需要一种更高效的时间建模方法。

Method: 提出ToTMNet架构，用FFT加速的Toeplitz时间混合层替代注意力机制。Toeplitz算子使用线性参数提供全序列时间感受野，通过循环嵌入和FFT卷积实现近线性时间计算。结合局部深度时间卷积分支和门控全局Toeplitz混合。

Result: 在UBFC-rPPG数据集上达到1.055 bpm MAE和0.996 Pearson相关系数；在合成到真实场景(SCAMPS到UBFC-rPPG)达到1.582 bpm MAE和0.994相关系数。模型仅63k参数。

Conclusion: Toeplitz结构的时间混合是rPPG中注意力机制的高效实用替代方案，门控机制对有效利用全局Toeplitz混合（尤其在域转移下）很重要。主要限制是仅使用两个数据集。

Abstract: Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.

</details>


### [73] [ImLoc: Revisiting Visual Localization with Image-based Representation](https://arxiv.org/abs/2601.04185)
*Xudong Jiang,Fangjinhua Wang,Silvano Galliani,Christoph Vogel,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出一种结合2D图像与深度图的视觉定位方法，通过密集匹配实现高精度，在存储和计算效率上取得平衡，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法存在局限性：2D图像方法易于构建和维护但几何推理能力有限；3D结构方法精度高但需要集中重建且难以更新。需要一种既能保持易用性又能实现高精度的方法。

Method: 为每张图像添加估计的深度图来捕捉几何结构，利用密集匹配器，结合紧凑压缩和GPU加速的LO-RANSAC实现，在存储和计算效率之间提供灵活权衡。

Result: 在多个标准基准测试中达到新的最先进精度，在可比地图大小下优于现有内存高效方法，同时保持高效存储和计算。

Conclusion: 通过2D图像增强深度图的表示方法不仅易于构建和维护，还能在挑战性条件下实现最高精度，为视觉定位提供了高效实用的解决方案。

Abstract: Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Lightweight Transformer Architectures for Edge Devices in Real-Time Applications](https://arxiv.org/abs/2601.03290)
*Hema Hariharan Samson*

Main category: cs.LG

TL;DR: 本文全面调查了面向边缘部署的轻量级Transformer架构，分析了模型压缩、量化、剪枝和知识蒸馏等技术，提供了性能基准测试和部署指南。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在资源受限的边缘设备上部署面临挑战，需要轻量化解决方案以支持实时AI应用。

Method: 系统性地回顾了MobileBERT、TinyBERT、DistilBERT、EfficientFormer、EdgeFormer、MobileViT等轻量级变体，分析了模型压缩、量化、剪枝、知识蒸馏技术，以及在不同硬件平台和部署框架上的优化策略。

Result: 现代轻量级Transformer能达到原模型75-96%的准确率，模型大小减少4-10倍，推理延迟降低3-9倍，可在2-5W功耗的设备上部署。15-40M参数模型能达到60-75%的硬件利用率。

Conclusion: 稀疏注意力机制、混合精度量化和硬件感知的神经架构搜索是最有效的优化策略。提出了6步部署流程，可实现8-12倍模型压缩且准确率损失小于2%。

Abstract: The deployment of transformer-based models on resource-constrained edge devices represents a critical challenge in enabling real-time artificial intelligence applications. This comprehensive survey examines lightweight transformer architectures specifically designed for edge deployment, analyzing recent advances in model compression, quantization, pruning, and knowledge distillation techniques. We systematically review prominent lightweight variants including MobileBERT, TinyBERT, DistilBERT, EfficientFormer, EdgeFormer, and MobileViT, providing detailed performance benchmarks on standard datasets such as GLUE, SQuAD, ImageNet-1K, and COCO. Our analysis encompasses current industry adoption patterns across major hardware platforms (NVIDIA Jetson, Qualcomm Snapdragon, Apple Neural Engine, ARM architectures), deployment frameworks (TensorFlow Lite, ONNX Runtime, PyTorch Mobile, CoreML), and optimization strategies. Experimental results demonstrate that modern lightweight transformers can achieve 75-96% of full-model accuracy while reducing model size by 4-10x and inference latency by 3-9x, enabling deployment on devices with as little as 2-5W power consumption. We identify sparse attention mechanisms, mixed-precision quantization (INT8/FP16), and hardware-aware neural architecture search as the most effective optimization strategies. Novel findings include memory-bandwidth bottleneck analysis revealing 15-40M parameter models achieve optimal hardware utilization (60-75% efficiency), quantization sweet spots for different model types, and comprehensive energy efficiency profiling across edge platforms. We establish real-time performance boundaries and provide a practical 6-step deployment pipeline achieving 8-12x size reduction with less than 2% accuracy degradation.

</details>


### [75] [Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts](https://arxiv.org/abs/2601.03315)
*Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: 研究团队尝试用6个LLM代理组成的流水线自动生成ML研究论文，4次尝试中3次失败，1次成功并被会议接受。研究识别了6种常见失败模式，并提出了4个设计原则。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统能否自主完成科学研究全过程，从问题提出到论文撰写，评估当前AI在科学研究工作流中的能力和局限性。

Method: 使用6个LLM代理组成的流水线，对应科学研究的不同阶段（如问题提出、方法设计、实验实施、结果分析、论文撰写等），进行端到端的自动化研究尝试。

Result: 4次尝试中3次失败（在实施或评估阶段），1次成功完成流水线并被Agents4Science 2025会议接受，通过了人类和多AI评审。识别出6种常见失败模式。

Conclusion: 提出了4个设计原则以构建更稳健的AI科学家系统，讨论了自主科学发现的启示，并开源了所有提示、工件和输出。

Abstract: We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1

</details>


### [76] [Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning](https://arxiv.org/abs/2601.03320)
*Yu Luo,Shuo Han,Yihan Hu,Dong Li,Jianye Hao*

Main category: cs.LG

TL;DR: R²VPO：一种基于策略比率方差约束的新RL框架，替代硬裁剪，提升LLM对齐的稳定性和数据效率


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的LLM微调方法使用策略比率硬裁剪，虽然稳定训练但存在两个问题：1）会截断高回报但高分歧动作的梯度，抑制复杂推理中的"顿悟时刻"；2）数据一旦过时就无法使用，导致样本效率低下

Method: 提出R²VPO（Ratio-Variance Regularized Policy Optimization），基于对策略比率方差（二阶中心矩）的显式约束，作为硬裁剪的原则性平滑松弛。采用原始-对偶框架，支持稳定在线学习，并通过动态重新加权过时样本实现原则性的离线数据重用

Result: 在DeepSeek-Distill-Qwen-1.5B和openPangu-Embedded系列（1B和7B）等SOTA LLM微调实验中，R²VPO在数学推理基准上表现优异：1）平均相对性能提升达17%；2）收敛所需rollout减少约50%

Conclusion: 策略比率方差控制是改进RL-based LLM对齐稳定性和数据效率的有前景方向，R²VPO框架在保持训练稳定性的同时，更好地保留了有价值轨迹的梯度信号

Abstract: On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative "eureka moments" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.

</details>


### [77] [Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting](https://arxiv.org/abs/2601.03321)
*Kun Zhao,Siyuan Dai,Pan Wang,Jifeng Song,Hui Ji,Chenghua Lin,Liang Zhan,Haoteng Tang*

Main category: cs.LG

TL;DR: 提出一个用于放射学报告生成的自洽框架，通过系统评估选择最佳视觉编码器和LLM骨干，采用"Reason-then-Summarize"架构和GRPO优化，显著减少幻觉并提升临床效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在放射学报告生成中存在架构异构性和事实幻觉问题，标准监督微调难以严格对齐语言输出与视觉证据，现有强化学习方法计算成本高或探索有限。

Method: 1) 系统评估选择最优视觉编码器和LLM骨干；2) 提出"Reason-then-Summarize"架构，包含详细发现的think块和结构化疾病标签的answer块；3) 使用Group Relative Policy Optimization (GRPO)优化；4) 采用多维复合奖励函数惩罚生成叙述与最终诊断间的逻辑不一致。

Result: 在MIMIC-CXR基准测试中达到最先进的临床效果指标，相比强监督基线显著减少幻觉。

Conclusion: 提出的自洽框架通过优化架构设计和强化学习策略，有效解决了放射学报告生成中的幻觉问题，提升了临床实用性。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel "Reason-then-Summarize" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.

</details>


### [78] [HEEGNet: Hyperbolic Embeddings for EEG](https://arxiv.org/abs/2601.03322)
*Shanglin Li,Shiwen Chu,Okan Koç,Yi Ding,Qibin Zhao,Motoaki Kawanabe,Ziheng Chen*

Main category: cs.LG

TL;DR: 提出HEEGNet，一种混合双曲网络架构，用于捕获EEG的层次结构并学习领域不变的双曲嵌入，通过结合欧几里得和双曲编码器以及粗到细的领域适应策略，在多个EEG数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: EEG脑机接口面临跨领域（如不同被试）分布偏移导致的泛化能力差问题。EEG数据具有层次结构特征，而双曲空间作为树结构的连续模拟，能更好地表示层次数据。研究发现EEG数据确实具有双曲特性，且双曲嵌入能改善泛化。

Method: 提出HEEGNet混合双曲网络架构，结合欧几里得和双曲编码器，采用新颖的粗到细领域适应策略。首先通过实验验证EEG数据的双曲特性，然后设计网络捕获EEG层次结构并学习领域不变的双曲嵌入。

Result: 在多个公开EEG数据集（包括视觉诱发电位、情绪识别和颅内EEG）上的广泛实验表明，HEEGNet实现了最先进的性能表现。

Conclusion: EEG数据具有双曲特性，利用双曲几何能更好地表示其层次结构。HEEGNet通过混合架构和领域适应策略，有效捕获EEG层次特征并学习领域不变表示，显著提升了EEG解码的泛化能力。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces facilitate direct communication with a computer, enabling promising applications in human-computer interactions. However, their utility is currently limited because EEG decoding often suffers from poor generalization due to distribution shifts across domains (e.g., subjects). Learning robust representations that capture underlying task-relevant information would mitigate these shifts and improve generalization. One promising approach is to exploit the underlying hierarchical structure in EEG, as recent studies suggest that hierarchical cognitive processes, such as visual processing, can be encoded in EEG. While many decoding methods still rely on Euclidean embeddings, recent work has begun exploring hyperbolic geometry for EEG. Hyperbolic spaces, regarded as the continuous analogue of tree structures, provide a natural geometry for representing hierarchical data. In this study, we first empirically demonstrate that EEG data exhibit hyperbolicity and show that hyperbolic embeddings improve generalization. Motivated by these findings, we propose HEEGNet, a hybrid hyperbolic network architecture to capture the hierarchical structure in EEG and learn domain-invariant hyperbolic embeddings. To this end, HEEGNet combines both Euclidean and hyperbolic encoders and employs a novel coarse-to-fine domain adaptation strategy. Extensive experiments on multiple public EEG datasets, covering visual evoked potentials, emotion recognition, and intracranial EEG, demonstrate that HEEGNet achieves state-of-the-art performance.

</details>


### [79] [Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme](https://arxiv.org/abs/2601.03327)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 提出首个用于法国野火严重程度预测的序数分类框架，比较不同损失函数对预测罕见高严重度火灾的影响，发现序数监督显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 野火在空间和严重程度上高度不平衡，预测极端事件特别困难。需要开发与法国运营决策直接对齐的野火严重程度预测框架，并研究损失函数设计对预测罕见高严重度火灾的影响

Method: 引入首个野火严重程度序数分类框架，比较标准交叉熵与多种序数感知目标函数，包括提出的基于截断离散指数广义帕累托分布的概率TDeGPD损失。在多种架构和真实运营数据上进行广泛基准测试

Result: 序数监督显著优于传统方法，加权Kappa损失（WKLoss）表现最佳，在最极端严重度类别上获得超过+0.1 IoU增益，同时保持有竞争力的校准质量。但由于数据集中罕见事件代表性极低，对这些事件的预测性能仍然有限

Conclusion: 研究强调了将严重程度排序、数据不平衡考虑和季节性风险整合到野火预测系统中的重要性。未来工作将专注于将季节动态和不确定性信息纳入训练，进一步提高极端事件预测的可靠性

Abstract: Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.

</details>


### [80] [Attention mechanisms in neural networks](https://arxiv.org/abs/2601.03329)
*Hasi Hays*

Main category: cs.LG

TL;DR: 这篇专著对注意力机制进行了全面严谨的数学处理，涵盖理论基础、计算特性和实际应用，重点分析了在NLP、CV和多模态学习中的应用，并讨论了当前限制。


<details>
  <summary>Details</summary>
Motivation: 注意力机制代表了神经网络架构的根本范式转变，但缺乏系统性的数学理论框架。需要全面理解其理论基础、计算特性和实际应用，以推动该领域发展。

Method: 采用数学严谨的方法分析注意力机制，包括理论建模、计算特性分析，并研究其在自回归变换器、双向编码器、序列到序列翻译、视觉变换器和跨模态注意力等具体架构中的应用。

Result: 揭示了注意力机制的训练特性、性能与模型规模和计算量之间的缩放规律，展示了注意力模式可视化结果，并在标准数据集上提供了性能基准。分析了学习到的注意力模式与语言和视觉结构的关系。

Conclusion: 注意力机制是深度学习的重要进展，但仍面临计算可扩展性、数据效率、系统泛化和可解释性等挑战，需要进一步研究解决这些限制。

Abstract: Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.

</details>


### [81] [LUT-KAN: Segment-wise LUT Quantization for Fast KAN Inference](https://arxiv.org/abs/2601.03332)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: LUT-KAN：一种基于查找表的KAN网络量化方法，通过将边缘函数转换为分段查找表，实现12倍CPU推理加速，同时保持分类精度


<details>
  <summary>Details</summary>
Motivation: KAN网络使用可学习单变量函数（通常用B样条实现）替代标量权重，虽然准确且可解释，但在CPU上推理成本高，因为每层需要大量样条计算。标准量化工具链难以应用，因为主要计算不是矩阵乘法而是重复的样条基函数评估。

Method: 提出LUT-KAN方法：将每个边缘函数转换为分段查找表（LUT），采用仿射int8/uint8量化和线性插值。提供明确的推理契约，包括边界约定和越界策略。提出"诚实基线"评估方法，在相同后端优化下比较B样条和LUT评估。

Result: 在DoS攻击检测案例中，编译后的模型保持分类质量（F1下降低于0.0002），CPU推理延迟降低12倍（NumPy）和10倍（Numba）。内存开销约为10倍（L=64时）。实验包括LUT分辨率16-128的受控扫描和两种量化方案。

Conclusion: LUT-KAN为KAN网络提供了一种有效的量化编译方法，显著提升CPU推理速度，同时保持模型精度，解决了KAN网络在CPU上推理效率低的问题。

Abstract: Kolmogorov--Arnold Networks (KAN) replace scalar weights by learnable univariate functions, often implemented with B-splines. This design can be accurate and interpretable, but it makes inference expensive on CPU because each layer requires many spline evaluations. Standard quantization toolchains are also hard to apply because the main computation is not a matrix multiply but repeated spline basis evaluation. This paper introduces LUT-KAN, a segment-wise lookup-table (LUT) compilation and quantization method for PyKAN-style KAN layers. LUT-KAN converts each edge function into a per-segment LUT with affine int8/uint8 quantization and linear interpolation. The method provides an explicit and reproducible inference contract, including boundary conventions and out-of-bounds (OOB) policies. We propose an ``honest baseline'' methodology for speed evaluation: B-spline evaluation and LUT evaluation are compared under the same backend optimization (NumPy vs NumPy and Numba vs Numba), which separates representation gains from vectorization and JIT effects. Experiments include controlled sweeps over LUT resolution L in 16, 32, 64, 128 and two quantization schemes (symmetric int8 and asymmetric uint8). We report accuracy, speed, and memory metrics with mean and standard deviation across multiple seeds. A two-by-two OOB robustness matrix evaluates behavior under different boundary modes and OOB policies. In a case study, we compile a trained KAN model for DoS attack detection (CICIDS2017 pipeline) into LUT artifacts. The compiled model preserves classification quality (F1 drop below 0.0002) while reducing steady-state CPU inference latency by 12x under NumPy and 10x under Numba backends (honest baseline). The memory overhead is approximately 10x at L=64. All code and artifacts are publicly available with fixed release tags for reproducibility.

</details>


### [82] [Physics-Informed Gaussian Process Regression for the Constitutive Modeling of Concrete: A Data-Driven Improvement to Phenomenological Models](https://arxiv.org/abs/2601.03367)
*Chenyang Li,Himanshu Sharma,Youcai Wu,Joseph Magallanes,K. T. Ramesh,Michael D. Shields*

Main category: cs.LG

TL;DR: 本文提出了一种物理信息化的高斯过程回归框架，用于替代KCC模型中的经验失效面，通过约束学习提高混凝土本构模型的泛化能力和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统KCC模型依赖经验标定的失效面，缺乏模型形式的灵活性和不确定性量化能力，需要开发更灵活且物理可靠的替代方法。

Method: 保留KCC模型的弹塑性模块结构，用约束高斯过程回归替代经验失效面，通过三轴压缩实验数据训练，并加入基于材料行为的导数约束来确保物理一致性。

Result: 无约束GPR在训练条件附近插值良好，但外推时违反物理约束；物理信息化的GPR即使在超出训练范围的高围压条件下也表现出更好的准确性和可靠性，同时降低预测方差。

Conclusion: 该方法提供了鲁棒、不确定性感知的代理模型，在不牺牲KCC模型可解释性和数值效率的前提下，改善了泛化能力并简化了标定过程，为改进混凝土本构模型提供了实用路径。

Abstract: Understanding and modeling the constitutive behavior of concrete is crucial for civil and defense applications, yet widely used phenomenological models such as Karagozian \& Case concrete (KCC) model depend on empirically calibrated failure surfaces that lack flexibility in model form and associated uncertainty quantification. This work develops a physics-informed framework that retains the modular elastoplastic structure of KCC model while replacing its empirical failure surface with a constrained Gaussian Process Regression (GPR) surrogate that can be learned directly from experimentally accessible observables. Triaxial compression data under varying confinement levels are used for training, and the surrogate is then evaluated at confinement levels not included in the training set to assess its generalization capability. Results show that an unconstrained GPR interpolates well near training conditions but deteriorates and violates essential physical constraints under extrapolation, even when augmented with simulated data. In contrast, a physics-informed GPR that incorporates derivative-based constraints aligned with known material behavior yields markedly better accuracy and reliability, including at higher confinement levels beyond the training range. Probabilistic enforcement of these constraints also reduces predictive variance, producing tighter confidence intervals in data-scarce regimes. Overall, the proposed approach delivers a robust, uncertainty-aware surrogate that improves generalization and streamlines calibration without sacrificing the interpretability and numerical efficiency of the KCC model, offering a practical path toward an improved constitutive models for concrete.

</details>


### [83] [Enhancing Small Dataset Classification Using Projected Quantum Kernels with Convolutional Neural Networks](https://arxiv.org/abs/2601.03375)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,A. Mahasinghe,Thushari Silva*

Main category: cs.LG

TL;DR: 该研究提出了一种使用投影量子核(PQK)增强CNN特征提取的方法，专门针对小数据集场景，在MNIST和CIFAR-10上显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在图像分类中表现出色，但通常依赖大规模标注数据集。在实际应用中，数据稀缺是一个常见挑战，需要开发能够在小数据集上有效工作的模型。

Method: 提出了一种创新方法，将投影量子核(PQK)集成到CNN的特征提取过程中。PQK基于量子计算原理，能够捕捉传统CNN可能遗漏的复杂模式和精细数据结构，从而增强CNN的表征能力。

Result: 在仅使用1000个训练样本的情况下，PQK增强的CNN在MNIST数据集上达到95%准确率，在CIFAR-10数据集上达到90%准确率。相比之下，传统CNN在相同条件下分别只达到60%和12%的准确率。

Conclusion: 研究表明量子计算在解决机器学习中的数据稀缺问题方面具有巨大潜力。投影量子核可以作为增强CNN分类能力的有效工具，特别是在数据受限的环境中，为量子辅助神经网络的研究开辟了新方向。

Abstract: Convolutional Neural Networks (CNNs) have shown promising results in efficiency and accuracy in image classification. However, their efficacy often relies on large, labeled datasets, posing challenges for applications with limited data availability. Our research addresses these challenges by introducing an innovative approach that leverages projected quantum kernels (PQK) to enhance feature extraction for CNNs, specifically tailored for small datasets. Projected quantum kernels, derived from quantum computing principles, offer a promising avenue for capturing complex patterns and intricate data structures that traditional CNNs might miss. By incorporating these kernels into the feature extraction process, we improved the representational ability of CNNs. Our experiments demonstrated that, with 1000 training samples, the PQK-enhanced CNN achieved 95% accuracy on the MNIST dataset and 90% on the CIFAR-10 dataset, significantly outperforming the classical CNN, which achieved only 60% and 12% accuracy on the respective datasets. This research reveals the potential of quantum computing in overcoming data scarcity issues in machine learning and paves the way for future exploration of quantum-assisted neural networks, suggesting that projected quantum kernels can serve as a powerful approach for enhancing CNN-based classification in data-constrained environments.

</details>


### [84] [Weather-Aware Transformer for Real-Time Route Optimization in Drone-as-a-Service Operations](https://arxiv.org/abs/2601.03376)
*Kamal Mohamed,Lillian Wassim,Ali Hamdi,Khaled Shaban*

Main category: cs.LG

TL;DR: 提出一个基于天气感知深度学习模型的无人机服务路线预测框架，相比传统路径规划算法显著提升计算速度，同时保持路线优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划算法（如A*和Dijkstra）虽然能提供最优解，但计算复杂度高，在动态环境中难以实时应用，特别是在需要考虑天气条件对无人机操作影响的场景下。

Method: 使用传统算法生成的合成数据集训练机器学习和深度学习模型，采用基于Transformer和注意力机制的架构，整合天气启发式信息（风模式、风向、温度等）来预测最优下一节点选择。

Result: 天气感知模型相比传统算法实现显著的计算加速，同时保持路线优化性能，其中Transformer架构在动态环境约束下表现出更好的适应性。

Conclusion: 该框架为大规模无人机服务运营提供了实时、天气响应的路线优化方案，显著提升了自主无人机系统的效率和安全性。

Abstract: This paper presents a novel framework to accelerate route prediction in Drone-as-a-Service operations through weather-aware deep learning models. While classical path-planning algorithms, such as A* and Dijkstra, provide optimal solutions, their computational complexity limits real-time applicability in dynamic environments. We address this limitation by training machine learning and deep learning models on synthetic datasets generated from classical algorithm simulations. Our approach incorporates transformer-based and attention-based architectures that utilize weather heuristics to predict optimal next-node selections while accounting for meteorological conditions affecting drone operations. The attention mechanisms dynamically weight environmental factors including wind patterns, wind bearing, and temperature to enhance routing decisions under adverse weather conditions. Experimental results demonstrate that our weather-aware models achieve significant computational speedup over traditional algorithms while maintaining route optimization performance, with transformer-based architectures showing superior adaptation to dynamic environmental constraints. The proposed framework enables real-time, weather-responsive route optimization for large-scale DaaS operations, representing a substantial advancement in the efficiency and safety of autonomous drone systems.

</details>


### [85] [SIGMA: Scalable Spectral Insights for LLM Collapse](https://arxiv.org/abs/2601.03385)
*Yi Gu,Lingyou Pang,Xiangkun Ye,Tianyu Wang,Jianyu Lin,Carey E. Priebe,Alexander Aue*

Main category: cs.LG

TL;DR: 提出SIGMA框架，通过分析嵌入Gram矩阵的谱来量化LLM递归训练中的模型崩溃现象


<details>
  <summary>Details</summary>
Motivation: 合成数据训练LLM导致"模型崩溃"问题，但缺乏在高维空间中量化预测崩溃的严格方法

Method: 引入SIGMA框架，通过推导Gram矩阵谱的确定性和随机边界来追踪表示空间的收缩

Result: SIGMA能有效捕捉向退化状态的转变，其随机公式可实现大规模基础模型的可扩展估计

Conclusion: SIGMA为模型崩溃机制提供理论洞见，并为递归训练管道的健康监控提供实用可扩展工具

Abstract: The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of "model collapse"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.

</details>


### [86] [Inferring Clinically Relevant Molecular Subtypes of Pancreatic Cancer from Routine Histopathology Using Deep Learning](https://arxiv.org/abs/2601.03410)
*Abdul Rehman Akbar,Alejandro Levya,Ashwini Esnakula,Elshad Hasanov,Anne Noonan,Upender Manne,Vaibhav Sahai,Lingbin Meng,Susan Tsai,Anil Parwani,Wei Chen,Ashish Manne,Muhammad Khalid Khan Niazi*

Main category: cs.LG

TL;DR: PanSubNet是一个可解释的深度学习框架，能够直接从标准H&E染色全切片图像预测胰腺导管腺癌(PDAC)的分子亚型，解决了传统分子分型方法成本高、耗时长、组织需求大的临床限制。


<details>
  <summary>Details</summary>
Motivation: PDAC的基底样和经典型分子分型具有重要的预后和预测价值，但传统基于RNA-seq的方法成本高、耗时长、组织需求大，限制了其在临床实践中的应用。需要开发一种快速、经济、基于常规病理切片的分子分型方法。

Method: PanSubNet采用双尺度架构，融合细胞级形态学和组织级结构特征，利用注意力机制进行多尺度表示学习和透明特征归因。使用1,055名患者的配对组织学和RNA-seq数据（PANCAN队列846例，TCGA队列209例），基于Moffitt 50基因特征和GATA6表达生成真实标签。

Result: 内部验证（五折交叉验证）平均AUC为88.5%，外部验证（TCGA队列）AUC为84.0%，显示出良好的泛化能力。模型预测与转录组程序、分化标志物和DNA损伤修复特征一致，预测不确定性反映了中间转录状态而非分类噪声。

Conclusion: PanSubNet能够从常规H&E染色切片实现快速、经济的分子分层，为PDAC提供了一种临床可部署、可解释的遗传分型工具，有望整合到数字病理工作流程中，推进胰腺癌的精准肿瘤学。

Abstract: Molecular subtyping of PDAC into basal-like and classical has established prognostic and predictive value. However, its use in clinical practice is limited by cost, turnaround time, and tissue requirements, thereby restricting its application in the management of PDAC. We introduce PanSubNet, an interpretable deep learning framework that predicts therapy-relevant molecular subtypes directly from standard H&E-stained WSIs. PanSubNet was developed using data from 1,055 patients across two multi-institutional cohorts (PANCAN, n=846; TCGA, n=209) with paired histology and RNA-seq data. Ground-truth labels were derived using the validated Moffitt 50-gene signature refined by GATA6 expression. The model employs dual-scale architecture that fuses cellular-level morphology with tissue-level architecture, leveraging attention mechanisms for multi-scale representation learning and transparent feature attribution. On internal validation within PANCAN using five-fold cross-validation, PanSubNet achieved mean AUC of 88.5% with balanced sensitivity and specificity. External validation on the independent TCGA cohort without fine-tuning demonstrated robust generalizability (AUC 84.0%). PanSubNet preserved and, in metastatic disease, strengthened prognostic stratification compared to RNA-seq based labels. Prediction uncertainty linked to intermediate transcriptional states, not classification noise. Model predictions are aligned with established transcriptomic programs, differentiation markers, and DNA damage repair signatures. By enabling rapid, cost-effective molecular stratification from routine H&E-stained slides, PanSubNet offers a clinically deployable and interpretable tool for genetic subtyping. We are gathering data from two institutions to validate and assess real-world performance, supporting integration into digital pathology workflows and advancing precision oncology for PDAC.

</details>


### [87] [Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning](https://arxiv.org/abs/2601.03413)
*Yigal Koifman,Eran Iceland,Erez Koifman,Ariel Barel,Alfred M. Bruckstein*

Main category: cs.LG

TL;DR: 提出基于图像的强化学习方法用于多智能体分散控制，将观测编码为结构化视觉输入，在有限距离和仅方位感知的智能体聚合任务中实现高收敛性和快速收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法依赖手工特征提取或原始向量表示，限制了策略在输入顺序和大小方面的可扩展性和效率。需要一种能有效处理空间特征并产生分散运动控制规则的方法。

Method: 提出基于图像的强化学习方法，将智能体观测编码为结构化视觉输入，通过神经网络处理提取空间特征，生成分散式运动控制规则。在有限距离和仅方位感知的多智能体聚合任务上进行评估。

Result: 该方法实现了高收敛性，收敛速度接近VariAntNet（神经网络框架），在某些场景中是唯一实用的替代方案。相比Bellaiche和Bruckstein的解析解（收敛慢但保证收敛）和VariAntNet（收敛快但成功率中等），在收敛性和速度之间取得了良好平衡。

Conclusion: 基于图像的强化学习方法为多智能体分散控制提供了有效解决方案，能够处理结构化视觉输入并提取空间特征，在保持群体凝聚力的聚合任务中表现出色，为相关任务提供了实用替代方案。

Abstract: This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm's performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.

</details>


### [88] [Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks](https://arxiv.org/abs/2601.03420)
*Zhakshylyk Nurlanov,Frank R. Schmidt,Florian Bernard*

Main category: cs.LG

TL;DR: RAILS框架通过随机迭代局部搜索，无需梯度或先验知识，仅使用模型logits实现高效对抗攻击，达到接近100%的成功率并提升跨模型可迁移性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估高估了模型鲁棒性，因为现有自动攻击方法依赖手工先验或需要白盒梯度访问。需要打破这些限制，开发更有效的对抗攻击评估方法。

Method: 提出RAILS框架：1）基于模型logits的token级迭代优化；2）新颖的自回归损失函数确保精确前缀匹配；3）基于历史的选择策略桥接代理目标与真实攻击成功率；4）支持跨分词器集成攻击。

Result: 在多个开源模型上达到接近100%的攻击成功率，对GPT、Gemini等闭源系统具有高黑盒攻击可迁移性，显著优于现有方法。

Conclusion: RAILS证明了无需梯度或先验知识也能实现高效对抗攻击，通过跨分词器集成攻击发现共享对抗模式，为LLM安全评估提供了更准确的基准。

Abstract: As Large Language Models (LLMs) are increasingly deployed in safety-critical domains, rigorously evaluating their robustness against adversarial jailbreaks is essential. However, current safety evaluations often overestimate robustness because existing automated attacks are limited by restrictive assumptions. They typically rely on handcrafted priors or require white-box access for gradient propagation. We challenge these constraints by demonstrating that token-level iterative optimization can succeed without gradients or priors. We introduce RAILS (RAndom Iterative Local Search), a framework that operates solely on model logits. RAILS matches the effectiveness of gradient-based methods through two key innovations: a novel auto-regressive loss that enforces exact prefix matching, and a history-based selection strategy that bridges the gap between the proxy optimization objective and the true attack success rate. Crucially, by eliminating gradient dependency, RAILS enables cross-tokenizer ensemble attacks. This allows for the discovery of shared adversarial patterns that generalize across disjoint vocabularies, significantly enhancing transferability to closed-source systems. Empirically, RAILS achieves near 100% success rates on multiple open-source models and high black-box attack transferability to closed-source systems like GPT and Gemini.

</details>


### [89] [Spectral Archaeology: The Causal Topology of Model Evolution](https://arxiv.org/abs/2601.03424)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出基于注意力图谱的训练无关机制探针，通过代数连通度等指标揭示模型处理策略，发现特定课程转换导致的连通性崩溃现象（PTCC），并识别四种可预测的处理策略。


<details>
  <summary>Details</summary>
Motivation: 行为基准测试只能告诉我们模型做了什么，但不能揭示其内部工作机制。需要一种训练无关的机制分析方法来理解模型如何处理信息，特别是跨不同语言和模型变体的稳定特征。

Method: 使用注意力图谱作为机制探针：将每个注意力层视为令牌图，计算代数连通度（λ₂）、平滑度和谱熵。在12个模型和10种语言上应用这些指标，形成"谱指纹"。

Result: 发现四个主要结果：1）特定课程转换（如代码到聊天）导致英语中非规范结构的连通性崩溃（PTCC）；2）PTCC反映了形式路由强化与风格灵活性之间的权衡；3）识别出四种可预测的处理策略；4）PTCC定位到第2层的稀疏"补偿补丁"头，激活引导可部分恢复连通性。

Conclusion: 注意力图谱为模型审计和训练制度验证提供了实用工具，揭示了拓扑机制与标记化密度而非语言身份的系统关系，有助于理解模型内部工作机制和脆弱性。

Abstract: Behavioral benchmarks tell us \textit{what} a model does, but not \textit{how}. We introduce a training-free mechanistic probe using attention-graph spectra. Treating each layer as a token graph, we compute algebraic connectivity ($λ_2$), smoothness, and spectral entropy. Across 12 models and 10 languages, these measures yield stable ``spectral fingerprints'' that expose discontinuities missed by standard evaluation.
  We report four results. (1) Models undergoing specific curriculum transitions (e.g., code-to-chat) show an English-only, syntax-triggered connectivity failure on non-canonical constructions, reaching $Δλ_2 \approx -0.76$. We term this scar \textit{Passive-Triggered Connectivity Collapse} (PTCC). Analysis of the Phi lineage reveals that PTCC appears and resolves across developmental stages, implicating brittle curriculum shifts rather than synthetic data per se. (2) PTCC reflects a specialization trade-off: strengthened formal routing at the expense of stylistic flexibility. (3) We identify four recurrent processing strategies; simple frozen-threshold rules enable perfect forensic identification across lineages. (4) Mechanistically, PTCC localizes to a sparse Layer 2 ``compensatory patch'' of heads that fails under syntactic stress; activation steering can partially restore connectivity, recovering $\approx 38\%$ of lost information flow.
  Finally, dominant topological regimes track tokenization density more than language identity, suggesting ``healthy'' geometry varies systematically across scripts. Overall, attention-graph spectra provide a practical tool for auditing and training-regime verification.

</details>


### [90] [The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models](https://arxiv.org/abs/2601.03425)
*Yan Wang,Yitao Xu,Nanhan Shen,Jinyan Su,Jimin Huang,Zining Zhu*

Main category: cs.LG

TL;DR: 论文通过COMMITTEEAUDIT框架分析MoE模型的路由行为，发现存在跨领域的"常设委员会"专家小组主导路由，挑战了MoE模型通过稀疏路由实现领域专业化的传统假设。


<details>
  <summary>Details</summary>
Motivation: 质疑混合专家模型通过稀疏路由实现领域专业化的普遍假设，探索MoE模型实际的路由行为模式。

Method: 提出COMMITTEEAUDIT后验分析框架，在专家组层面而非单个专家层面分析路由行为，在三个代表性模型和MMLU基准上进行实验。

Result: 发现跨领域、层级和路由预算的"常设委员会"现象——一个紧凑的专家联盟始终占据路由质量的主要部分；常设委员会锚定推理结构和语法，外围专家处理领域特定知识。

Conclusion: MoE模型的领域专业化远不如普遍认为的那样普遍，存在强烈的集中计算结构偏置；当前的训练目标（如强制均匀专家利用的负载均衡损失）可能违背模型的自然优化路径，限制了训练效率和性能。

Abstract: Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.

</details>


### [91] [VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding](https://arxiv.org/abs/2601.03434)
*Zibo Liu,Muyang Li,Zhe Jiang,Shigang Chen*

Main category: cs.LG

TL;DR: VNU-Bench：首个面向新闻领域多源跨视频理解的新基准，包含429个新闻组、1405个视频和2501个高质量问题，挑战当前多模态大语言模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型评估基准主要关注单源、视频内推理，而现实新闻消费本质上是多源的——同一事件由不同媒体以互补细节、不同叙事选择和有时相互矛盾的主张进行报道。鲁棒的新闻理解需要模型能够比较不同来源的视角、对齐跨来源的多模态证据并综合多源信息。

Method: 1. 设计了新型问题类型，从多个角度测试模型理解多源多模态新闻的能力；2. 开发了混合人机QA生成流程，解决了跨源新闻理解大规模数据集构建中的可扩展性和质量控制问题；3. 构建了包含429个新闻组、1405个视频和2501个高质量问题的数据集。

Result: 对闭源和开源多模态模型的全面评估表明，VNU-Bench对当前多模态大语言模型构成了实质性挑战，现有模型在多源跨视频理解任务上表现不佳。

Conclusion: VNU-Bench填补了新闻领域多源跨视频理解评估的空白，为开发更鲁棒的新闻理解模型提供了重要基准，揭示了当前模型在现实世界多源新闻消费场景中的局限性。

Abstract: News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.

</details>


### [92] [Soft Contextualized Encoder For User Defined Text Classification](https://arxiv.org/abs/2601.03450)
*Charu Maheshwari,Vyas Raina*

Main category: cs.LG

TL;DR: 提出一种软上下文编码器架构，用于用户自定义文本分类，通过上下文化候选标签和输入查询的软提示表示，在未见主题集上实现零样本分类。


<details>
  <summary>Details</summary>
Motivation: 用户自定义文本分类在现实应用中很常见（如企业分析、内容审核、领域特定信息检索），但现有方法难以处理用户指定的、未见过的类别，需要能够泛化到任意领域的新主题集。

Method: 提出软上下文编码器架构，将每个候选标签与标签集和输入查询的静态软提示表示进行上下文化。通过在多样化的多源数据集上训练，使模型能够有效泛化到从任意领域抽取的完全未见主题集的零样本分类。

Result: 在分布内测试数据和多个未见UDTC基准测试上评估，模型在所有数据集上都达到了最先进的性能，始终优于或匹配基线方法。

Conclusion: 提出的软上下文编码器架构能够有效处理用户自定义文本分类任务，在未见主题集上实现强大的零样本分类性能，为现实应用中的灵活分类需求提供了解决方案。

Abstract: User-Defined Text Classification (UDTC) considers the challenge of classifying input text to user-specified, previously unseen classes, a setting that arises frequently in real-world applications such as enterprise analytics, content moderation, and domain-specific information retrieval. We propose a soft-contextualized encoder architecture for UDTC which contextualizes each candidate label with the label set and a static soft prompt representation of the input query. Training on diverse, multi-source datasets enables the model to generalize effectively to zero-shot classification over entirely unseen topic sets drawn from arbitrary domains. We evaluate the proposed architecture both on held-out in-distribution test data and on multiple unseen UDTC benchmarks. Across datasets, the model achieves state-of-the-art performance, consistently outperforming or matching the baselines.

</details>


### [93] [An Expectation-Maximization Algorithm for Domain Adaptation in Gaussian Causal Models](https://arxiv.org/abs/2601.03459)
*Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 提出基于因果DAG的EM框架，在目标域缺失目标变量时，利用源域完整数据和DAG结构进行目标变量填补，通过一阶梯度EM更新实现高效参数估计。


<details>
  <summary>Details</summary>
Motivation: 解决在部署域中目标变量系统性缺失的问题，当源域有完整观测且已知高斯因果DAG时，需要将信息从源域转移到目标域以填补缺失的目标变量。

Method: 提出统一的基于EM的框架，结合源域和目标域数据通过DAG结构传递信息。引入一阶（梯度）EM更新，用单个投影梯度步替代昂贵的广义最小二乘M步。利用已知因果DAG冻结源域不变的机制，仅重新估计受域偏移直接影响的那些条件分布。

Result: 在标准局部强凹性和平滑性假设及BWY梯度稳定性条件下，证明一阶EM算子对真实目标参数具有局部收缩性，产生几何收敛性，并在协变量偏移和局部机制偏移下获得参数误差和目标填补误差的有限样本保证。在合成七节点SEM、64节点MAGIC-IRRI遗传网络和Sachs蛋白质信号数据上的实验表明，所提DAG感知一阶EM算法在目标填补准确性上优于基于源域的贝叶斯网络和Kiiveri式EM基线，在显著域偏移下增益最大。

Conclusion: 该方法通过因果DAG结构有效结合源域和目标域信息，在目标变量缺失的情况下实现准确填补，特别适用于存在显著域偏移的场景，且具有理论保证和实际有效性。

Abstract: We study the problem of imputing a designated target variable that is systematically missing in a shifted deployment domain, when a Gaussian causal DAG is available from a fully observed source domain. We propose a unified EM-based framework that combines source and target data through the DAG structure to transfer information from observed variables to the missing target. On the methodological side, we formulate a population EM operator in the DAG parameter space and introduce a first-order (gradient) EM update that replaces the costly generalized least-squares M-step with a single projected gradient step. Under standard local strong-concavity and smoothness assumptions and a BWY-style \cite{Balakrishnan2017EM} gradient-stability (bounded missing-information) condition, we show that this first-order EM operator is locally contractive around the true target parameters, yielding geometric convergence and finite-sample guarantees on parameter error and the induced target-imputation error in Gaussian SEMs under covariate shift and local mechanism shifts. Algorithmically, we exploit the known causal DAG to freeze source-invariant mechanisms and re-estimate only those conditional distributions directly affected by the shift, making the procedure scalable to higher-dimensional models. In experiments on a synthetic seven-node SEM, the 64-node MAGIC-IRRI genetic network, and the Sachs protein-signaling data, the proposed DAG-aware first-order EM algorithm improves target imputation accuracy over a fit-on-source Bayesian network and a Kiiveri-style EM baseline, with the largest gains under pronounced domain shift.

</details>


### [94] [Hybrid Approach for Driver Behavior Analysis with Machine Learning, Feature Optimization, and Explainable AI](https://arxiv.org/abs/2601.03477)
*Mehedi Hasan Shuvo,Md. Raihan Tapader,Nur Mohammad Tamjid,Sajjadul Islam,Ahnaf Atef Choudhury,Jia Uddin*

Main category: cs.LG

TL;DR: 本文提出一种混合方法分析驾驶员行为，使用Kaggle数据集，结合随机森林分类器（准确率95%）和LIME可解释AI技术，在保持性能的同时提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员行为分析方法使用机器学习和深度学习技术，但存在特征优化不足的问题，既影响高性能又损害可解释性。需要填补这一空白，在保持预测性能的同时提升模型可解释性。

Method: 采用混合方法：1）使用12,857行18列的Kaggle数据集；2）应用标签编码、随机过采样和标准缩放等预处理技术；3）测试13种机器学习算法；4）使用LIME可解释AI技术识别对准确性影响最大的前10个特征；5）基于这些特征重新训练算法。

Result: 随机森林分类器初始准确率达到95%，应用LIME技术识别关键特征后重新训练，准确率略微下降至94.2%，表明可以在不显著牺牲性能的情况下提升模型效率和可解释性。

Conclusion: 提出的混合模型在驾驶员行为分析中实现了预测能力和可解释性的平衡，为道路安全改进提供了投资回报，证明了在不牺牲性能的情况下提升模型可解释性的可行性。

Abstract: Progressive driver behavior analytics is crucial for improving road safety and mitigating the issues caused by aggressive or inattentive driving. Previous studies have employed machine learning and deep learning techniques, which often result in low feature optimization, thereby compromising both high performance and interpretability. To fill these voids, this paper proposes a hybrid approach to driver behavior analysis that uses a 12,857-row and 18-column data set taken from Kaggle. After applying preprocessing techniques such as label encoding, random oversampling, and standard scaling, 13 machine learning algorithms were tested. The Random Forest Classifier achieved an accuracy of 95%. After deploying the LIME technique in XAI, the top 10 features with the most significant positive and negative influence on accuracy were identified, and the same algorithms were retrained. The accuracy of the Random Forest Classifier decreased slightly to 94.2%, confirming that the efficiency of the model can be improved without sacrificing performance. This hybrid model can provide a return on investment in terms of the predictive power and explainability of the driver behavior process.

</details>


### [95] [From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs](https://arxiv.org/abs/2601.03484)
*Kaiyuan Deng,Hangyu Zheng,Minghai Qing,Kunxiong Zhu,Gen Li,Yang Xiao,Lan Emily Zhang,Linke Guo,Bo Hui,Yanzhi Wang,Geng Yuan,Gagan Agrawal,Wei Niu,Xiaolong Ma*

Main category: cs.LG

TL;DR: HAQA是一个利用大语言模型自动优化模型量化和部署的框架，通过硬件感知的超参数调优，在保持精度的同时提升推理速度2.3倍，降低用户使用门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大模型部署需求扩大到非专业用户，硬件资源限制和量化部署的复杂性成为主要挑战。现有量化技术虽然能缓解计算瓶颈，但调优和部署过程复杂，对大多数用户不友好。

Method: 提出硬件感知量化代理(HAQA)，利用LLM自动优化整个量化和部署流程，包括高效的超参数调优和硬件配置。框架能自适应地为不同硬件平台找到最优量化策略，即使这些设置看起来反直觉。

Result: 在Llama模型上实现高达2.3倍的推理加速，同时提高吞吐量和精度。相比未优化模型，HAQA在保持准确性的同时显著提升性能，并展示了跨硬件平台的强大适应性。

Conclusion: HAQA通过自动化量化和部署流程，既提高了部署质量又降低了使用门槛，为非专业用户提供了高效的模型部署解决方案，减少了大量手动调优工作。

Abstract: Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.

</details>


### [96] [VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation](https://arxiv.org/abs/2601.03525)
*Longwen Wang,Xuan'er Wu,Xiaohui Hu,Yirui Liu,Yuankai Fan,Kaidong Yu,Qizhen Weng,Wei Xi,Xuelong Li*

Main category: cs.LG

TL;DR: VeRPO是一个用于代码生成的强化学习框架，通过基于执行统计动态估计单元测试难度权重，从部分成功构建密集奖励，结合全局执行结果，仅依赖可验证执行反馈实现稳健密集奖励。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成RL中的奖励设计面临挑战：主流通过/失败结果奖励通过执行单元测试强制执行功能正确性，但稀疏性限制了性能提升；而外部奖励模型虽然能生成更丰富的连续奖励，但存在奖励错位和计算成本过高的问题。

Method: VeRPO的核心思想是从加权部分成功构建密集奖励：在训练期间基于执行统计动态估计每个单元测试的难度权重，从通过的单元测试权重总和推导出密集奖励。为巩固部分成功与端到端功能正确性之间的一致性，VeRPO进一步将密集信号与全局执行结果集成，建立仅依赖可验证执行反馈的稳健密集奖励范式。

Result: 在多样化的基准测试和设置中进行广泛实验，VeRPO始终优于结果驱动和基于奖励模型的基线方法，在pass@1指标上实现了高达+8.83%的提升，时间成本可忽略不计（<0.02%），且GPU内存开销为零。

Conclusion: VeRPO通过合成完全基于可验证执行反馈的稳健密集奖励，解决了代码生成中奖励设计的核心挑战，在性能提升、计算效率和内存开销方面都表现出显著优势。

Abstract: Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (< 0.02\%) and zero GPU memory overhead.

</details>


### [97] [Green's-Function Spherical Neural Operators for Biological Heterogeneity](https://arxiv.org/abs/2601.03561)
*Hao Tang,Hao Chen,Hao Li,Chao Li*

Main category: cs.LG

TL;DR: 提出GSNO（Green's-Function Spherical Neural Operator），通过可设计的格林函数框架融合三种算子解，在保持球面几何的同时建模现实世界异质性。


<details>
  <summary>Details</summary>
Motivation: 现有球面深度学习方法难以平衡强球面几何归纳偏置与建模现实世界异质性的需求。需要一种既能保留球面几何又能处理生物异质性的方法。

Method: 首先引入可设计的格林函数框架（DGF），然后提出GSNO融合三种算子解：1）等变解（来自等变格林函数）用于对称一致建模；2）不变解（来自不变格林函数）消除干扰异质性；3）各向异性解（来自各向异性格林函数）建模各向异性系统。

Result: 在球面MNIST、浅水方程、扩散MRI纤维预测、皮层分区和分子结构建模等任务上验证了GSNO的优越性。

Conclusion: GSNO能够在保留谱效率的同时，适应具有干扰变异性和各向异性的现实世界异质系统，在多个应用领域表现出色。

Abstract: Spherical deep learning has been widely applied to a broad range of real-world problems. Existing approaches often face challenges in balancing strong spherical geometric inductive biases with the need to model real-world heterogeneity. To solve this while retaining spherical geometry, we first introduce a designable Green's function framework (DGF) to provide new spherical operator solution strategy: Design systematic Green's functions under rotational group. Based on DGF, to model biological heterogeneity, we propose Green's-Function Spherical Neural Operator (GSNO) fusing 3 operator solutions: (1) Equivariant Solution derived from Equivariant Green's Function for symmetry-consistent modeling; (2) Invariant Solution derived from Invariant Green's Function to eliminate nuisance heterogeneity, e.g., consistent background field; (3) Anisotropic Solution derived from Anisotropic Green's Function to model anisotropic systems, especially fibers with preferred direction. Therefore, the resulting model, GSNO can adapt to real-world heterogeneous systems with nuisance variability and anisotropy while retaining spectral efficiency. Evaluations on spherical MNIST, Shallow Water Equation, diffusion MRI fiber prediction, cortical parcellation and molecule structure modeling demonstrate the superiority of GSNO.

</details>


### [98] [A Proposed Paradigm for Imputing Missing Multi-Sensor Data in the Healthcare Domain](https://arxiv.org/abs/2601.03565)
*Vaibhav Gupta,Florian Grensing,Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该论文综述了针对糖尿病等慢性疾病管理中血糖事件预测的多传感器数据缺失值填补方法，提出根据特征特性和缺失时长定制填补策略的系统范式。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如糖尿病的管理面临重大挑战，特别是低血糖等并发症需要及时检测和干预。虽然可穿戴传感器的连续健康监测为血糖事件的早期预测提供了有前景的解决方案，但多传感器数据的有效使用受到信号噪声和频繁缺失值的阻碍。

Method: 1. 分析现有数据集的局限性，强调与低血糖预测相关的关键特征的时间特性；2. 对最先进研究中使用的填补技术进行全面分析；3. 评估机器学习和深度学习在其他医疗保健应用中衍生的填补方法处理长时间序列数据缺失的潜力；4. 提出系统范式，根据特定特征的性质和缺失间隔的持续时间定制填补策略。

Result: 通过分析发现，需要针对个体特征的时间动态进行研究，并实施多种特征特定的填补技术，以有效处理数据中固有的异质时间模式。

Conclusion: 该综述强调研究个体特征时间动态的重要性，以及实施多种特征特定填补技术以有效处理数据中异质时间模式的必要性，为改善慢性疾病管理中多传感器数据的缺失值处理提供了系统方法。

Abstract: Chronic diseases such as diabetes pose significant management challenges, particularly due to the risk of complications like hypoglycemia, which require timely detection and intervention. Continuous health monitoring through wearable sensors offers a promising solution for early prediction of glycemic events. However, effective use of multisensor data is hindered by issues such as signal noise and frequent missing values. This study examines the limitations of existing datasets and emphasizes the temporal characteristics of key features relevant to hypoglycemia prediction. A comprehensive analysis of imputation techniques is conducted, focusing on those employed in state-of-the-art studies. Furthermore, imputation methods derived from machine learning and deep learning applications in other healthcare contexts are evaluated for their potential to address longer gaps in time-series data. Based on this analysis, a systematic paradigm is proposed, wherein imputation strategies are tailored to the nature of specific features and the duration of missing intervals. The review concludes by emphasizing the importance of investigating the temporal dynamics of individual features and the implementation of multiple, feature-specific imputation techniques to effectively address heterogeneous temporal patterns inherent in the data.

</details>


### [99] [Local Intrinsic Dimensionality of Ground Motion Data for Early Detection of Complex Catastrophic Slope Failure](https://arxiv.org/abs/2601.03569)
*Yuansan Liu,Antoinette Tordesillas,James Bailey*

Main category: cs.LG

TL;DR: 提出stLID方法，结合空间和时间信息改进LID异常检测，用于滑坡灾害早期预警


<details>
  <summary>Details</summary>
Motivation: 现有滑坡监测方法通常只分析表面位移数据，未能充分捕捉空间相关性和时间动态性，导致对复杂滑坡和连续失效的检测能力不足

Method: 基于sLID方法进行三方面改进：1) 运动学增强-引入速度信息；2) 空间融合-贝叶斯估计聚合邻域sLID值；3) 时间建模-提出tLID学习长期动态。最终整合为统一的stLID框架

Result: stLID在失效检测精度和预警提前时间方面持续优于现有方法

Conclusion: stLID通过同时考虑空间和时间维度，能够更有效地检测复杂滑坡和连续失效，为地质灾害缓解提供更可靠的早期预警

Abstract: Local Intrinsic Dimensionality (LID) has shown strong potential for identifying anomalies and outliers in high-dimensional data across a wide range of real-world applications, including landslide failure detection in granular media. Early and accurate identification of failure zones in landslide-prone areas is crucial for effective geohazard mitigation. While existing approaches typically rely on surface displacement data analyzed through statistical or machine learning techniques, they often fall short in capturing both the spatial correlations and temporal dynamics that are inherent in such data. To address this gap, we focus on ground-monitored landslides and introduce a novel approach that jointly incorporates spatial and temporal information, enabling the detection of complex landslides and including multiple successive failures occurring in distinct areas of the same slope. To be specific, our method builds upon an existing LID-based technique, known as sLID. We extend its capabilities in three key ways. (1) Kinematic enhancement: we incorporate velocity into the sLID computation to better capture short-term temporal dependencies and deformation rate relationships. (2) Spatial fusion: we apply Bayesian estimation to aggregate sLID values across spatial neighborhoods, effectively embedding spatial correlations into the LID scores. (3) Temporal modeling: we introduce a temporal variant, tLID, that learns long-term dynamics from time series data, providing a robust temporal representation of displacement behavior. Finally, we integrate both components into a unified framework, referred to as spatiotemporal LID (stLID), to identify samples that are anomalous in either or both dimensions. Extensive experiments show that stLID consistently outperforms existing methods in failure detection precision and lead-time.

</details>


### [100] [Variational Inference, Entropy, and Orthogonality: A Unified Theory of Mixture-of-Experts](https://arxiv.org/abs/2601.03577)
*Ye Su,Yong Liu*

Main category: cs.LG

TL;DR: 本文为MoE模型建立了首个统一理论框架，从贝叶斯和信息论角度严格推导了Top-k路由和负载均衡机制，证明了路由的NP-hard本质，并提出了正交性正则化作为最优工程松弛方案。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型的核心机制（Top-k路由和辅助负载均衡）缺乏统一的理论基础，仅作为启发式方法存在。需要建立理论框架来理解这些实践背后的原理，并为MoE设计提供理论支持。

Method: 1. 从贝叶斯视角构建统一理论框架，将路由机制形式化为最优稀疏后验近似和先验正则化；2. 从信息论视角将路由机制解释为最小化路由模糊性和最大化信道容量；3. 将路由问题定义为NP-hard稀疏子集选择问题；4. 证明"相干性障碍"的存在；5. 验证几何正交性在专家特征空间中的有效性。

Result: 1. 严格证明了当专家表示具有高互相关性时，贪婪路由策略理论上无法恢复最优专家子集；2. 形式化验证了在专家特征空间中施加几何正交性足以缩小NP-hard全局最优解与多项式时间贪婪近似之间的差距；3. 比较分析确认正交性正则化是大规模模型的最优工程松弛方案。

Conclusion: 本文为MoE模型提供了首个统一理论框架，不仅从贝叶斯和信息论角度解释了现有实践，还揭示了路由问题的本质困难，并提出了正交性正则化作为有效的解决方案，为深入理解和创新设计MoE模型提供了理论支持和技术保证。

Abstract: Mixture-of-Experts models enable large language models to scale efficiently, as they only activate a subset of experts for each input. Their core mechanisms, Top-k routing and auxiliary load balancing, remain heuristic, however, lacking a cohesive theoretical underpinning to support them. To this end, we build the first unified theoretical framework that rigorously derives these practices as optimal sparse posterior approximation and prior regularization from a Bayesian perspective, while simultaneously framing them as mechanisms to minimize routing ambiguity and maximize channel capacity from an information-theoretic perspective. We also pinpoint the inherent combinatorial hardness of routing, defining it as the NP-hard sparse subset selection problem. We rigorously prove the existence of a "Coherence Barrier"; when expert representations exhibit high mutual coherence, greedy routing strategies theoretically fail to recover the optimal expert subset. Importantly, we formally verify that imposing geometric orthogonality in the expert feature space is sufficient to narrow the divide between the NP-hard global optimum and polynomial-time greedy approximation. Our comparative analyses confirm orthogonality regularization as the optimal engineering relaxation for large-scale models. Our work offers essential theoretical support and technical assurance for a deeper understanding and novel designs of MoE.

</details>


### [101] [Local Gradient Regulation Stabilizes Federated Learning under Client Heterogeneity](https://arxiv.org/abs/2601.03584)
*Ping Luo,Jiahuan Wang,Ziqing Wen,Tao Sun,Dongsheng Li*

Main category: cs.LG

TL;DR: 论文提出通过调节本地梯度动态来稳定异构联邦学习，开发了ECGR方法平衡对齐和未对齐的梯度分量，在医疗影像数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在现实部署中面临统计异构性的挑战，客户端异构性通过扭曲本地梯度动态导致系统漂移，阻碍全局收敛，需要找到稳定异构联邦学习系统的方法。

Method: 提出客户端视角的通用框架，通过探索-收敛梯度重聚合（ECGR）方法调节本地梯度贡献，平衡对齐和未对齐的梯度分量，保留信息性更新同时抑制不稳定效应，无需额外通信开销。

Result: 理论分析和广泛实验表明，调节本地梯度动态能一致地稳定联邦学习，在LC25000医疗影像数据集等异构数据分布下优于现有方法。

Conclusion: 本地梯度动态是稳定异构联邦学习的关键调节杠杆，ECGR方法通过智能梯度重聚合有效解决异构性带来的稳定性问题，为联邦学习系统设计提供了新视角。

Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, yet its stability is fundamentally challenged by statistical heterogeneity in realistic deployments. Here, we show that client heterogeneity destabilizes FL primarily by distorting local gradient dynamics during client-side optimization, causing systematic drift that accumulates across communication rounds and impedes global convergence. This observation highlights local gradients as a key regulatory lever for stabilizing heterogeneous FL systems. Building on this insight, we develop a general client-side perspective that regulates local gradient contributions without incurring additional communication overhead. Inspired by swarm intelligence, we instantiate this perspective through Exploratory--Convergent Gradient Re-aggregation (ECGR), which balances well-aligned and misaligned gradient components to preserve informative updates while suppressing destabilizing effects. Theoretical analysis and extensive experiments, including evaluations on the LC25000 medical imaging dataset, demonstrate that regulating local gradient dynamics consistently stabilizes federated learning across state-of-the-art methods under heterogeneous data distributions.

</details>


### [102] [ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification](https://arxiv.org/abs/2601.03600)
*Xiao Lin,Philip Li,Zhichen Zeng,Tingwei Li,Tianxin Wei,Xuying Ning,Gaotang Li,Yuzhong Chen,Hanghang Tong*

Main category: cs.LG

TL;DR: ALERT是一个基于特征放大的零样本越狱检测框架，通过逐层、逐模块、逐token放大良性提示与越狱提示的内部特征差异，无需训练数据中的越狱模板即可有效检测新型攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种安全对齐策略，大语言模型仍易受越狱攻击。现有检测方法主要依赖训练数据中的越狱模板，但现实场景中新攻击不断涌现，需要零样本检测能力。

Method: 提出三层放大框架：1）识别安全相关层；2）发现编码零样本判别信号的特定模块；3）定位信息丰富的安全token。在此基础上构建ALERT检测器，使用两个独立互补的分类器对放大后的特征进行分类。

Result: 在三个安全基准测试中，ALERT在所有数据集和攻击策略上均稳定排名前二，平均准确率和F1分数比次优基线至少高出10%，有时高达40%。

Conclusion: ALERT通过放大内部特征差异实现了高效的零样本越狱检测，能够应对现实世界中不断演变的新型攻击，显著优于现有方法。

Abstract: Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status relying on jailbreak templates present in the training data. However, few studies address the more realistic and challenging zero-shot jailbreak detection setting, where no jailbreak templates are available during training. This setting better reflects real-world scenarios where new attacks continually emerge and evolve. To address this challenge, we propose a layer-wise, module-wise, and token-wise amplification framework that progressively magnifies internal feature discrepancies between benign and jailbreak prompts. We uncover safety-relevant layers, identify specific modules that inherently encode zero-shot discriminative signals, and localize informative safety tokens. Building upon these insights, we introduce ALERT (Amplification-based Jailbreak Detector), an efficient and effective zero-shot jailbreak detector that introduces two independent yet complementary classifiers on amplified representations. Extensive experiments on three safety benchmarks demonstrate that ALERT achieves consistently strong zero-shot detection performance. Specifically, (i) across all datasets and attack strategies, ALERT reliably ranks among the top two methods, and (ii) it outperforms the second-best baseline by at least 10% in average Accuracy and F1-score, and sometimes by up to 40%.

</details>


### [103] [A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data](https://arxiv.org/abs/2601.03603)
*Kaidong Feng,Zhu Sun,Roy Ka-Wei Lee,Xun Jiang,Yin-Leng Theng,Yi Ding*

Main category: cs.LG

TL;DR: 该研究首次对传统机器学习、深度学习和大语言模型在心理健康预测方面进行了全面基准测试，发现深度学习模型（特别是Transformer）表现最佳，而大语言模型在上下文推理方面有优势但时间建模较弱。


<details>
  <summary>Details</summary>
Motivation: 智能手机传感提供了一种不引人注目且可扩展的方式来追踪与心理健康相关的日常行为，但大多数先前研究侧重于检测现有状况，而预测心理健康能够通过适时适应性干预提供主动支持。

Method: 使用College Experience Sensing（CES）数据集，系统评估传统机器学习、深度学习和大型语言模型方法，比较不同时间窗口、特征粒度、个性化策略和类别不平衡处理技术。

Result: 深度学习模型（特别是Transformer，Macro-F1 = 0.58）达到最佳整体性能；大语言模型在上下文推理方面表现优势但时间建模较弱；个性化显著改善严重心理健康状态的预测。

Conclusion: 通过揭示不同建模方法如何随时间解释手机传感行为数据，这项工作为下一代自适应、以人为本的心理健康技术奠定了基础，能够推动研究和现实世界福祉。

Abstract: Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.

</details>


### [104] [Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries](https://arxiv.org/abs/2601.03606)
*Sumedh Pendurkar,Guni Sharon*

Main category: cs.LG

TL;DR: 将Levin树搜索算法适配到思维树框架中，利用语言模型概率作为启发式指导搜索，在有限计算预算下减少思维评估次数，提升问题解决性能。


<details>
  <summary>Details</summary>
Motivation: 现有思维树框架中的搜索算法往往忽视语言模型推理的高计算成本，特别是在计算资源受限的场景下。需要设计能在有限计算预算下提升语言模型问题解决性能的方法。

Method: 将Levin树搜索算法适配到思维树框架，利用语言模型分配的概率作为启发式指导树探索。理论分析显示该方法对剪枝树能保证状态扩展次数的上界，并分析了温度参数对边界的影响。

Result: 在固定语言模型查询预算下，LTS在三个领域（Blocksworld、PrOntoQA、Array Sorting）和四种不同语言模型上，相比基线搜索算法获得相当或更高的准确率。

Conclusion: LTS在思维树框架中表现出高效性，特别适合延迟敏感和资源受限的应用场景，实现了成本效益和时间效率的平衡。

Abstract: Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the ''Tree-of-Thoughts'' (ToT), generated by LMs, thereby improving performance on problem-solving tasks. However, the affiliated search algorithms often overlook the significant computational costs associated with LM inference, particularly in scenarios with constrained computational budgets. Consequently, we address the problem of improving LM performance on problem-solving tasks under limited computational budgets. We demonstrate how the probabilities assigned to thoughts by LMs can serve as a heuristic to guide search within the ToT framework, thereby reducing the number of thought evaluations. Building on this insight, we adapt a heuristic search algorithm, Levin Tree Search (LTS), to the ToT framework, which leverages LMs as policies to guide the tree exploration efficiently. We extend the theoretical results of LTS by showing that, for ToT (a pruned tree), LTS guarantees a bound on the number of states expanded, and consequently, on the number of thoughts generated. Additionally, we analyze the sensitivity of this bound to the temperature values commonly used in the final softmax layer of the LM. Empirical evaluation under a fixed LM query budget demonstrates that LTS consistently achieves comparable or higher accuracy than baseline search algorithms within the ToT framework, across three domains (Blocksworld, PrOntoQA, Array Sorting) and four distinct LMs. These findings highlight the efficacy of LTS on ToT, particularly in enabling cost-effective and time-efficient problem-solving, making it well-suited for latency-critical and resource-constrained applications.

</details>


### [105] [Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias](https://arxiv.org/abs/2601.03612)
*Joonwon Seo*

Main category: cs.LG

TL;DR: 提出基于结构归纳偏置的复调音乐生成新方法，解决"缺失中间层"问题，通过智能嵌入架构减少48.30%参数，验证损失降低9.47%


<details>
  <summary>Details</summary>
Motivation: 解决AI音乐生成中的"缺失中间层"问题，即现有方法在复调音乐生成中缺乏结构归纳偏置，导致生成质量受限

Method: 使用贝多芬钢琴奏鸣曲作为案例研究，通过归一化互信息验证音高和手部属性独立性，提出智能嵌入架构，结合信息论、Rademacher复杂度和范畴论进行数学证明

Result: 音高手部属性独立性验证(NMI=0.167)，参数减少48.30%，验证损失降低9.47%，泛化边界收紧28.09%，专家听音研究(N=53)确认质量提升

Conclusion: 该双理论应用框架填补了AI音乐生成的空白，为基于数学基础的深度学习提供了可验证的见解，通过结构归纳偏置显著提升复调音乐生成质量

Abstract: This monograph introduces a novel approach to polyphonic music generation by addressing the "Missing Middle" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.

</details>


### [106] [Learning Shortest Paths When Data is Scarce](https://arxiv.org/abs/2601.03629)
*Dmytro Matsypura,Yu Pan,Hanzhao Wang*

Main category: cs.LG

TL;DR: 提出一种利用数字孪生模拟器和有限真实数据校准网络路径成本的方法，通过拉普拉斯正则化最小二乘法估计边成本偏差，并提供路径最优性验证和主动学习算法。


<details>
  <summary>Details</summary>
Motivation: 数字孪生等模拟器在大型网络路由决策中应用广泛，但模拟器输出存在系统性偏差，而真实测量成本高且稀缺。需要解决模拟器与真实世界之间的偏差问题，在数据稀缺情况下获得校准的边成本估计。

Method: 将模拟器到现实的偏差建模为未知的边特定偏差，该偏差在相似性图上平滑变化。使用拉普拉斯正则化最小二乘法估计偏差，建立有限样本误差界，将估计误差转化为路径级次优性保证。提出可计算的数据驱动证书验证候选路径的近最优性。针对冷启动设置，开发偏差感知主动学习算法，自适应选择要测量的边。

Result: 该方法能够在数据稀缺情况下获得校准的边成本估计。建立了有限样本误差界，提供了路径级次优性保证。提出的数据驱动证书能够验证候选路径的近最优性。主动学习算法有效减少了所需真实测量数量。在多个道路网络和交通图上的数值实验证明了方法的有效性。

Conclusion: 通过结合丰富的合成样本、有限的真实观测和边相似性结构，提出的拉普拉斯正则化方法能够有效校准模拟器偏差，在数据稀缺情况下提供可靠的路径成本估计和最优性验证，为数字孪生辅助的路由决策提供了理论保证和实用工具。

Abstract: Digital twins and other simulators are increasingly used to support routing decisions in large-scale networks. However, simulator outputs often exhibit systematic bias, while ground-truth measurements are costly and scarce. We study a stochastic shortest-path problem in which a planner has access to abundant synthetic samples, limited real-world observations, and an edge-similarity structure capturing expected behavioral similarity across links. We model the simulator-to-reality discrepancy as an unknown, edge-specific bias that varies smoothly over the similarity graph, and estimate it using Laplacian-regularized least squares. This approach yields calibrated edge cost estimates even in data-scarce regimes. We establish finite-sample error bounds, translate estimation error into path-level suboptimality guarantees, and propose a computable, data-driven certificate that verifies near-optimality of a candidate route. For cold-start settings without initial real data, we develop a bias-aware active learning algorithm that leverages the simulator and adaptively selects edges to measure until a prescribed accuracy is met. Numerical experiments on multiple road networks and traffic graphs further demonstrate the effectiveness of our methods.

</details>


### [107] [Kantorovich-Type Stochastic Neural Network Operators for the Mean-Square Approximation of Certain Second-Order Stochastic Processes](https://arxiv.org/abs/2601.03634)
*Sachin Saini,Uaday Singh*

Main category: cs.LG

TL;DR: 提出一种新型Kantorovich型随机神经网络算子，通过随机神经元而非系数引入随机性，用于逼近随机过程。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络算子主要用于逼近确定性输入输出函数，对随机动力学的扩展研究相对不足。需要构建能够继承底层过程概率结构的随机神经网络算子来建模和逼近随机信号。

Method: 构建Kantorovich型随机神经网络算子，通过随机积分器驱动的随机神经元引入随机性，而非在系数层面加入随机性。该框架使算子能够继承底层过程的概率结构。

Result: 建立了K-SNNOs对目标随机过程的均方收敛性，推导了用连续性模表示逼近速率的定量误差估计。数值模拟验证了理论结果，展示了样本路径的准确重构和均方误差的快速衰减。

Conclusion: 提出的基于随机神经元的算子框架能够有效建模和逼近随机过程，数值实验证明了其鲁棒性和有效性，为随机动力学的神经网络逼近提供了新方法。

Abstract: Artificial neural network operators (ANNOs) have been widely used for approximating deterministic input-output functions; however, their extension to random dynamics remains comparatively unexplored. In this paper, we construct a new class of \textbf{Kantorovich-type Stochastic Neural Network Operators (K-SNNOs)} in which randomness is incorporated not at the coefficient level, but through \textbf{stochastic neurons} driven by stochastic integrators. This framework enables the operator to inherit the probabilistic structure of the underlying process, making it suitable for modeling and approximating stochastic signals. We establish mean-square convergence of K-SNNOs to the target stochastic process and derive quantitative error estimates expressing the rate of approximation in terms of the modulus of continuity. Numerical simulations further validate the theoretical results by demonstrating accurate reconstruction of sample paths and rapid decay of the mean square error (MSE). Graphical results, including sample-wise approximations and empirical MSE behaviour, illustrate the robustness and effectiveness of the proposed stochastic-neuron-based operator.

</details>


### [108] [ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning](https://arxiv.org/abs/2601.03646)
*Zhengyi Kwan,Zhang Wei,Aik Beng Ng,Zhengkui Wang,Simon See*

Main category: cs.LG

TL;DR: ReLA：基于结构化表示学习与聚合的强化学习调度器，通过多尺度架构学习作业操作和机器的多样化表示，在多种规模实例上实现最佳完工时间


<details>
  <summary>Details</summary>
Motivation: 现实制造系统中的作业调度需要将有序作业操作分配给机器，现有解决方案在运行时间或调度质量方面存在不足，尤其是在问题规模增大时表现受限

Method: ReLA采用结构化表示学习和聚合方法，使用两个具有自注意力和卷积的实体内学习模块和一个具有交叉注意力的实体间学习模块，在多尺度架构中学习调度实体（作业操作和机器）的多样化表示，并将输出聚合以支持强化学习决策

Result: 在小、中、大规模作业实例实验中，ReLA在大多数测试设置中实现了最佳完工时间。在非大型实例上，将SOTA基线的优化差距降低了13.0%；在大型实例上，将优化差距降低了78.6%，平均优化差距分别降至7.3%和2.1%

Conclusion: ReLA学习的表示和聚合为强化学习调度提供了强大的决策支持，能够实现快速作业完成和实时决策，适用于实际应用

Abstract: Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA's learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.

</details>


### [109] [Quantum Classical Ridgelet Neural Network For Time Series Model](https://arxiv.org/abs/2601.03654)
*Bahadur Yadav,Sanjay Kumar Mohanty*

Main category: cs.LG

TL;DR: 提出将脊波变换与单量子比特量子计算结合的时间序列预测方法，在金融数据上表现优于现有模型


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析方法在处理复杂金融数据时存在特征提取能力不足的问题，需要结合量子计算和先进变换方法来提升预测性能

Method: 将脊波神经网络与单量子比特量子计算方法集成到量子处理流程中，利用脊波变换增强特征提取能力

Result: 在金融时间序列数据上的实验结果表明，该方法相比现有模型具有更优越的性能表现

Conclusion: 脊波变换与量子计算的结合为时间序列分析提供了有效的解决方案，在金融预测领域展现出良好应用前景

Abstract: In this study, we present a quantum computing method that incorporates ridglet transforms into the quantum processing pipelines for time series data. Here, the Ridgelet neural network is integrated with a single-qubit quantum computing method, which improves feature extraction and forecasting capabilities. Furthermore, experimental results using financial time series data demonstrate the superior performance of our model compared to existing models.

</details>


### [110] [In Search of Grandmother Cells: Tracing Interpretable Neurons in Tabular Representations](https://arxiv.org/abs/2601.03657)
*Ricardo Knauer,Erik Rodner*

Main category: cs.LG

TL;DR: 该研究提出了两种信息论度量来量化神经元对单个概念的显著性和选择性，并在表格基础模型TabPFN中发现了具有统计显著性的概念特异性神经元。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽然强大但决策过程不透明，神经科学和人工智能领域一直关注是否存在类似"祖母细胞"的神经元（即对单一概念具有排他性响应的可解释神经元）。

Method: 提出了两种信息论度量：神经元显著性和选择性度量，应用于TabPFN表格基础模型的表示，通过简单的神经元-概念对搜索找到最显著和最具选择性的配对。

Result: 首次发现此类模型中的某些神经元对高级概念表现出中等但统计显著的显著性和选择性。

Conclusion: 可解释神经元可以自然涌现，在某些情况下无需复杂可解释性技术即可识别，为理解基础模型的内部机制提供了新途径。

Abstract: Foundation models are powerful yet often opaque in their decision-making. A topic of continued interest in both neuroscience and artificial intelligence is whether some neurons behave like grandmother cells, i.e., neurons that are inherently interpretable because they exclusively respond to single concepts. In this work, we propose two information-theoretic measures that quantify the neuronal saliency and selectivity for single concepts. We apply these metrics to the representations of TabPFN, a tabular foundation model, and perform a simple search across neuron-concept pairs to find the most salient and selective pair. Our analysis provides the first evidence that some neurons in such models show moderate, statistically significant saliency and selectivity for high-level concepts. These findings suggest that interpretable neurons can emerge naturally and that they can, in some cases, be identified without resorting to more complex interpretability techniques.

</details>


### [111] [Group and Exclusive Sparse Regularization-based Continual Learning of CNNs](https://arxiv.org/abs/2601.03658)
*Basile Tousside,Janis Mohr,Jörg Frochte*

Main category: cs.LG

TL;DR: GESCL是一种基于正则化的持续学习方法，通过稳定性和可塑性正则化项防止灾难性遗忘，同时利用CNN的过参数化特性稀疏化网络，相比动态扩展网络或记忆数据的方法需要更少参数和计算。


<details>
  <summary>Details</summary>
Motivation: 解决固定容量卷积神经网络在持续学习中的灾难性遗忘问题，避免现有方法需要动态扩展网络或记忆过去任务数据带来的参数和计算开销。

Method: 提出GESCL方法，包含两个正则化项：稳定性正则化防止对过去任务重要的滤波器在学习新任务时偏离过大；可塑性正则化利用CNN的过参数化稀疏化网络，调整不重要滤波器使其对未来任务有用。

Result: 在流行的持续学习视觉基准测试中，GESCL在整体分类准确率和避免灾难性遗忘方面显著优于现有最先进方法。

Conclusion: GESCL通过结合稳定性和可塑性正则化，为固定容量CNN提供了一种有效的持续学习解决方案，在保持高性能的同时显著减少了参数和计算需求。

Abstract: We present a regularization-based approach for continual learning (CL) of fixed capacity convolutional neural networks (CNN) that does not suffer from the problem of catastrophic forgetting when learning multiple tasks sequentially. This method referred to as Group and Exclusive Sparsity based Continual Learning (GESCL) avoids forgetting of previous tasks by ensuring the stability of the CNN via a stability regularization term, which prevents filters detected as important for past tasks to deviate too much when learning a new task. On top of that, GESCL makes the network plastic via a plasticity regularization term that leverage the over-parameterization of CNNs to efficiently sparsify the network and tunes unimportant filters making them relevant for future tasks. Doing so, GESCL deals with significantly less parameters and computation compared to CL approaches that either dynamically expand the network or memorize past tasks' data. Experiments on popular CL vision benchmarks show that GESCL leads to significant improvements over state-of-the-art method in terms of overall CL performance, as measured by classification accuracy as well as in terms of avoiding catastrophic forgetting.

</details>


### [112] [AMIR-GRPO: Inducing Implicit Preference Signals into GRPO](https://arxiv.org/abs/2601.03661)
*Amir Hossein Yari,Fajri Koto*

Main category: cs.LG

TL;DR: AMIR-GRPO改进GRPO，通过引入基于组内奖励排名的隐式对比正则化，解决GRPO在推理任务中的长度偏差、低质量轨迹惩罚不足和偏好信息利用不充分等问题，在数学推理任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: GRPO在复杂推理任务中存在结构限制：序列级优势归一化引入系统性长度偏差，低质量轨迹惩罚被稀释，标量目标丢弃了组内奖励排名中的丰富成对偏好信息，导致昂贵的rollout监督信息未被充分利用。

Method: 提出AMIR-GRPO，通过引入基于组内奖励排名的隐式DPO风格对比正则化来增强GRPO。该机制无需额外标注，直接利用组内奖励排名构建对比约束，增强对低奖励轨迹的抑制，减轻响应级长度偏差，并将每个rollout组转化为更密集的监督约束集。

Result: 在多个数学推理基准测试中，AMIR-GRPO持续优于强GRPO基线，在正确和错误推理链之间产生更清晰的分离，并在标准GRPO解决的实例子集之外提供更广泛的覆盖增益。

Conclusion: AMIR-GRPO通过有效利用组内奖励排名中的成对偏好信息，解决了GRPO在推理任务中的结构限制，实现了更好的对齐效果和推理性能提升。

Abstract: Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.
  We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.

</details>


### [113] [Stochastic Voronoi Ensembles for Anomaly Detection](https://arxiv.org/abs/2601.03664)
*Yang Cao*

Main category: cs.LG

TL;DR: 提出SVEAD方法，通过随机Voronoi图集成检测局部异常，在变密度数据中优于现有方法，具有线性时间复杂度和常数空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在处理具有变化局部密度的数据集时存在局限：基于距离的方法会错过局部异常，而基于密度的方法需要仔细的参数选择且具有二次时间复杂性。

Method: 提出SVEAD方法，基于几何洞察构建随机Voronoi图集成，将数据空间分解为受限区域，通过归一化的单元相对距离加权局部尺度来评分数据点。

Result: 在45个数据集上的实验表明，SVEAD优于12种最先进的方法，实现了线性时间复杂度和常数空间复杂度。

Conclusion: 通过分解数据空间并独立检查每个区域，局部异常变得明显，SVEAD方法有效解决了变密度数据集中的异常检测问题。

Abstract: Anomaly detection aims to identify data instances that deviate significantly from majority of data, which has been widely used in fraud detection, network security, and industrial quality control. Existing methods struggle with datasets exhibiting varying local densities: distance-based methods miss local anomalies, while density-based approaches require careful parameter selection and incur quadratic time complexity. We observe that local anomalies, though indistinguishable under global analysis, become conspicuous when the data space is decomposed into restricted regions and each region is examined independently. Leveraging this geometric insight, we propose SVEAD (Stochastic Voronoi Ensembles Anomaly Detector), which constructs ensemble random Voronoi diagrams and scores points by normalized cell-relative distances weighted by local scale. The proposed method achieves linear time complexity and constant space complexity. Experiments on 45 datasets demonstrate that SVEAD outperforms 12 state-of-the-art approaches.

</details>


### [114] [Disentangling Aleatoric and Epistemic Uncertainty in Physics-Informed Neural Networks. Application to Insulation Material Degradation Prognostics](https://arxiv.org/abs/2601.03673)
*Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 提出了一种异方差贝叶斯物理信息神经网络（B-PINN）框架，用于变压器绝缘材料老化预测，能同时建模认知不确定性和偶然不确定性，提供完整的预测后验分布。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络（PINNs）在预测健康管理（PHM）应用中存在不确定性量化能力有限的问题，大多数方法要么是确定性的，要么只考虑认知不确定性，限制了其在风险感知决策中的适用性。

Method: 提出异方差贝叶斯物理信息神经网络框架，将贝叶斯神经网络与基于物理的残差约束和先验分布相结合，在物理信息学习架构中实现概率推断。该方法能同时建模认知和偶然不确定性。

Result: 在变压器绝缘老化应用中，与确定性PINNs、基于dropout的PINNs和其他B-PINN变体相比，所提出的B-PINN提供了更好的预测精度和更校准的不确定性估计。敏感性研究分析了边界条件、初始条件和残差采样策略的影响。

Conclusion: 贝叶斯物理信息学习在支持变压器资产管理中的不确定性感知预测和知情决策方面具有潜力，所提出的B-PINN框架能提供改进的预测准确性和更好的不确定性校准。

Abstract: Physics-Informed Neural Networks (PINNs) provide a framework for integrating physical laws with data. However, their application to Prognostics and Health Management (PHM) remains constrained by the limited uncertainty quantification (UQ) capabilities. Most existing PINN-based prognostics approaches are deterministic or account only for epistemic uncertainty, limiting their suitability for risk-aware decision-making. This work introduces a heteroscedastic Bayesian Physics-Informed Neural Network (B-PINN) framework that jointly models epistemic and aleatoric uncertainty, yielding full predictive posteriors for spatiotemporal insulation material ageing estimation. The approach integrates Bayesian Neural Networks (BNNs) with physics-based residual enforcement and prior distributions, enabling probabilistic inference within a physics-informed learning architecture. The framework is evaluated on transformer insulation ageing application, validated with a finite-element thermal model and field measurements from a solar power plant, and benchmarked against deterministic PINNs, dropout-based PINNs (d-PINNs), and alternative B-PINN variants. Results show that the proposed B-PINN provides improved predictive accuracy and better-calibrated uncertainty estimates than competing approaches. A systematic sensitivity study further analyzes the impact of boundary-condition, initial-condition, and residual sampling strategies on accuracy, calibration, and generalization. Overall, the findings highlight the potential of Bayesian physics-informed learning to support uncertainty-aware prognostics and informed decision-making in transformer asset management.

</details>


### [115] [Rethinking Recurrent Neural Networks for Time Series Forecasting: A Reinforced Recurrent Encoder with Prediction-Oriented Proximal Policy Optimization](https://arxiv.org/abs/2601.03683)
*Xin Lai,Shiming Deng,Lu Yu,Yumin Lai,Shenghao Qiao,Xinze Zhang*

Main category: cs.LG

TL;DR: 提出RRE-PPO4Pred方法，通过强化学习增强RNN的时间序列预测能力，在多个真实数据集上超越现有基线方法


<details>
  <summary>Details</summary>
Motivation: 传统RNN预测器采用编码器策略，将滑动历史窗口作为输入来预测未来值，但这种方法将所有时间步和隐藏状态同等对待，没有考虑它们对预测的不同贡献，导致性能次优

Method: 提出RRE-PPO4Pred方法：1) 强化循环编码器框架，将RNN内部适应建模为马尔可夫决策过程；2) 改进的预测导向近端策略优化算法，配备Transformer智能体进行时序推理；3) 协同进化优化范式，促进RNN预测器和策略智能体的学习

Result: 在五个真实世界数据集上的综合评估表明，该方法始终优于现有基线方法，并且达到了比最先进的Transformer模型更好的准确性

Conclusion: RRE-PPO4Pred为工程信息学提供了先进的时间序列预测器，通过强化学习显著提高了RNN模型的时间序列建模能力和预测准确性

Abstract: Time series forecasting plays a crucial role in contemporary engineering information systems for supporting decision-making across various industries, where Recurrent Neural Networks (RNNs) have been widely adopted due to their capability in modeling sequential data. Conventional RNN-based predictors adopt an encoder-only strategy with sliding historical windows as inputs to forecast future values. However, this approach treats all time steps and hidden states equally without considering their distinct contributions to forecasting, leading to suboptimal performance. To address this limitation, we propose a novel Reinforced Recurrent Encoder with Prediction-oriented Proximal Policy Optimization, RRE-PPO4Pred, which significantly improves time series modeling capacity and forecasting accuracy of the RNN models. The core innovations of this method are: (1) A novel Reinforced Recurrent Encoder (RRE) framework that enhances RNNs by formulating their internal adaptation as a Markov Decision Process, creating a unified decision environment capable of learning input feature selection, hidden skip connection, and output target selection; (2) An improved Prediction-oriented Proximal Policy Optimization algorithm, termed PPO4Pred, which is equipped with a Transformer-based agent for temporal reasoning and develops a dynamic transition sampling strategy to enhance sampling efficiency; (3) A co-evolutionary optimization paradigm to facilitate the learning of the RNN predictor and the policy agent, providing adaptive and interactive time series modeling. Comprehensive evaluations on five real-world datasets indicate that our method consistently outperforms existing baselines, and attains accuracy better than state-of-the-art Transformer models, thus providing an advanced time series predictor in engineering informatics.

</details>


### [116] [A Pre-trained Reaction Embedding Descriptor Capturing Bond Transformation Patterns](https://arxiv.org/abs/2601.03689)
*Weiqi Liu,Fenglei Cao,Yuan Qi,Li-Cheng Xu*

Main category: cs.LG

TL;DR: RXNEmb是一种基于RXNGraphormer预训练模型的新型反应级描述符，能够通过区分真实反应与虚构反应来学习化学键形成与断裂的内在模式，为反应指纹和分析提供强大且可解释的工具。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动反应预测模型的兴起，有效的反应描述符对于连接真实化学与数字表示至关重要。然而，通用的反应级描述符仍然稀缺。

Method: 开发了RXNEmb描述符，源自RXNGraphormer模型，该模型通过区分真实反应与具有错误键变化的虚构反应进行预训练，从而学习内在的键形成和断裂模式。

Result: 1) 对USPTO-50k数据集进行数据驱动的重新聚类，得到比基于规则分类更能直接反映键变化相似性的分类；2) 结合降维技术实现反应空间多样性的可视化；3) 注意力权重分析揭示模型关注化学关键位点，提供机理洞察。

Conclusion: RXNEmb作为强大的可解释工具，可用于反应指纹和分析，为反应分析和发现中更数据驱动的方法铺平道路。

Abstract: With the rise of data-driven reaction prediction models, effective reaction descriptors are crucial for bridging the gap between real-world chemistry and digital representations. However, general-purpose, reaction-wise descriptors remain scarce. This study introduces RXNEmb, a novel reaction-level descriptor derived from RXNGraphormer, a model pre-trained to distinguish real reactions from fictitious ones with erroneous bond changes, thereby learning intrinsic bond formation and cleavage patterns. We demonstrate its utility by data-driven re-clustering of the USPTO-50k dataset, yielding a classification that more directly reflects bond-change similarities than rule-based categories. Combined with dimensionality reduction, RXNEmb enables visualization of reaction space diversity. Furthermore, attention weight analysis reveals the model's focus on chemically critical sites, providing mechanistic insight. RXNEmb serves as a powerful, interpretable tool for reaction fingerprinting and analysis, paving the way for more data-centric approaches in reaction analysis and discovery.

</details>


### [117] [Inference Attacks Against Graph Generative Diffusion Models](https://arxiv.org/abs/2601.03701)
*Xiuling Wang,Xin Huang,Guibo Luo,Jianliang Xu*

Main category: cs.LG

TL;DR: 本文研究了图生成扩散模型的隐私风险，提出了三种黑盒推理攻击方法（图重构攻击、属性推理攻击、成员推理攻击），并在真实数据集上验证了攻击有效性，最后提出了两种防御机制。


<details>
  <summary>Details</summary>
Motivation: 图生成扩散模型在生成复杂图结构方面表现出色，但其隐私风险尚未得到充分研究。作者旨在探索这类模型的信息泄露问题，通过设计攻击方法来评估其隐私脆弱性。

Method: 提出了三种黑盒推理攻击：1）图重构攻击，从生成的图中重构与训练图结构相似的图；2）属性推理攻击，从生成的图中推断训练图的属性（如图密度分布）；3）两种成员推理攻击，判断给定图是否在训练集中。在三种不同类型的图生成扩散模型和六个真实世界图上进行了广泛实验。

Result: 实验表明这些攻击方法非常有效，显著优于基线方法。攻击能够成功重构图结构、推断图属性，并准确识别训练集成员。

Conclusion: 图生成扩散模型存在严重的隐私泄露风险。作者提出了两种防御机制，能够在防御强度和模型效用之间取得更好的平衡，为保护图生成模型的隐私提供了解决方案。

Abstract: Graph generative diffusion models have recently emerged as a powerful paradigm for generating complex graph structures, effectively capturing intricate dependencies and relationships within graph data. However, the privacy risks associated with these models remain largely unexplored. In this paper, we investigate information leakage in such models through three types of black-box inference attacks. First, we design a graph reconstruction attack, which can reconstruct graphs structurally similar to those training graphs from the generated graphs. Second, we propose a property inference attack to infer the properties of the training graphs, such as the average graph density and the distribution of densities, from the generated graphs. Third, we develop two membership inference attacks to determine whether a given graph is present in the training set. Extensive experiments on three different types of graph generative diffusion models and six real-world graphs demonstrate the effectiveness of these attacks, significantly outperforming the baseline approaches. Finally, we propose two defense mechanisms that mitigate these inference attacks and achieve a better trade-off between defense strength and target model utility than existing methods. Our code is available at https://zenodo.org/records/17946102.

</details>


### [118] [TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL](https://arxiv.org/abs/2601.03703)
*Lang Cao,Hui Ruan,Yongqian Li,Peng Chao,Wu Ning,Haonan Song,Renhong Chen,Yitong Li*

Main category: cs.LG

TL;DR: TreeAdv提出树结构优势重分配方法，通过构建基于不确定性的决策树来改进GRPO，在数学推理任务上以更少token达到更好效果。


<details>
  <summary>Details</summary>
Motivation: 标准GRPO将每个轨迹视为独立序列，为所有token分配单一序列级优势，导致样本效率低下和长度偏差（冗长推理链不增加逻辑深度）。

Method: TreeAdv构建基于熵驱动采样的树结构（森林），在高不确定性决策点分支，共享低不确定性token，然后通过重分配完整轨迹优势来聚合token级优势。

Result: 在10个数学推理基准测试中，TreeAdv始终优于GRPO和GSPO，在相同监督、数据和解码预算下使用显著更少的生成token。

Conclusion: TreeAdv通过显式利用组滚动的树结构进行探索和优势分配，解决了标准GRPO的样本效率问题和长度偏差，为基于组的RL提供了更有效的框架。

Abstract: Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.

</details>


### [119] [Investigating Knowledge Distillation Through Neural Networks for Protein Binding Affinity Prediction](https://arxiv.org/abs/2601.03704)
*Wajid Arshad Abbasi,Syed Ali Abbas,Maryum Bibi,Saiqa Andleeb,Muhammad Naveed Akhtar*

Main category: cs.LG

TL;DR: 提出基于知识蒸馏的回归框架，在训练时使用蛋白质结构数据，推理时仅需序列数据，以提升蛋白质-蛋白质结合亲和力预测的准确性


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质-蛋白质结合亲和力预测中预测准确性与数据可用性之间的权衡问题。实验解析的蛋白质结构数据缺乏限制了基于结构的机器学习模型的性能，而序列方法通常性能较差

Method: 采用知识蒸馏框架，使用结构信息的教师网络指导序列基础的学生网络训练。通过结合亲和力标签和中间特征表示进行联合监督，训练时使用结构数据，推理时仅需序列数据

Result: 在非冗余蛋白质-蛋白质结合亲和力基准数据集上，序列基线模型Pearson相关系数0.375，RMSE 2.712 kcal/mol；结构模型0.512和2.445；蒸馏学生模型0.481和2.488，显著提升序列方法性能

Conclusion: 知识蒸馏是向序列基础预测器传递结构知识的有效方法，随着更大数据集的可用，有望缩小序列与结构模型间的性能差距

Abstract: The trade-off between predictive accuracy and data availability makes it difficult to predict protein--protein binding affinity accurately. The lack of experimentally resolved protein structures limits the performance of structure-based machine learning models, which generally outperform sequence-based methods. In order to overcome this constraint, we suggest a regression framework based on knowledge distillation that uses protein structural data during training and only needs sequence data during inference. The suggested method uses binding affinity labels and intermediate feature representations to jointly supervise the training of a sequence-based student network under the guidance of a structure-informed teacher network. Leave-One-Complex-Out (LOCO) cross-validation was used to assess the framework on a non-redundant protein--protein binding affinity benchmark dataset. A maximum Pearson correlation coefficient (P_r) of 0.375 and an RMSE of 2.712 kcal/mol were obtained by sequence-only baseline models, whereas a P_r of 0.512 and an RMSE of 2.445 kcal/mol were obtained by structure-based models. With a P_r of 0.481 and an RMSE of 2.488 kcal/mol, the distillation-based student model greatly enhanced sequence-only performance. Improved agreement and decreased bias were further confirmed by thorough error analyses. With the potential to close the performance gap between sequence-based and structure-based models as larger datasets become available, these findings show that knowledge distillation is an efficient method for transferring structural knowledge to sequence-based predictors. The source code for running inference with the proposed distillation-based binding affinity predictor can be accessed at https://github.com/wajidarshad/ProteinAffinityKD.

</details>


### [120] [The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling](https://arxiv.org/abs/2601.03706)
*Gil Shabat*

Main category: cs.LG

TL;DR: 论文揭示了Pivoted Cholesky分解在核方法中的几何解释：枢轴选择等价于核度量下的最远点采样，Cholesky因子构造是隐式的Gram-Schmidt正交化。


<details>
  <summary>Details</summary>
Motivation: 虽然Pivoted Cholesky分解在数值线性代数中的代数性质已有充分研究，但其在核方法中的几何直觉往往不够清晰。本文旨在阐明该算法在再生核希尔伯特空间中的几何解释。

Method: 通过将Pivoted Cholesky分解与再生核希尔伯特空间中的几何操作联系起来：证明枢轴选择步骤在数学上等价于使用核度量的最远点采样，而Cholesky因子构造则是隐式的Gram-Schmidt正交化过程。

Result: 成功建立了Pivoted Cholesky分解的几何解释，提供了简洁的推导和极简的Python实现，弥合了理论与实践的差距。

Conclusion: Pivoted Cholesky分解在核方法中具有清晰的几何意义：枢轴选择对应于最远点采样，因子构造对应于正交化。这一理解有助于更好地应用该算法于大规模核矩阵的低秩近似。

Abstract: Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.

</details>


### [121] [R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification](https://arxiv.org/abs/2601.03715)
*Weijie Shi,Yanxi Chen,Zexi Li,Xuchen Pan,Yuchang Sun,Jiajie Xu,Xiaofang Zhou,Yaliang Li*

Main category: cs.LG

TL;DR: R³L：一种新的强化学习方法，通过反思-重试机制、关键信用分配和正信号放大，解决LLM推理和智能体任务中的探索与利用问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在LLM推理和智能体能力方面面临双重挑战：探索方面成功率低且从头开始重复执行成本高；利用方面存在粗粒度信用分配和训练不稳定问题，轨迹级奖励会惩罚有效前缀，失败主导的样本会淹没少数正信号。

Method: R³L包含三个核心组件：1) 反思-重试语言引导探索：利用语言反馈诊断错误，将失败尝试转化为成功，从失败点重启降低执行成本；2) 关键信用分配：仅更新存在对比信号的分歧后缀，排除共享前缀；3) 正信号放大：对成功轨迹进行加权，确保正信号指导优化过程。

Result: 在智能体和推理任务上的实验表明，R³L相比基线方法取得了5%到52%的相对改进，同时保持了训练稳定性。

Conclusion: R³L通过主动合成高质量轨迹、精确信用分配和正信号放大，有效解决了强化学习在LLM推理和智能体任务中的探索与利用问题，显著提升了性能并保持了训练稳定性。

Abstract: Reinforcement learning drives recent advances in LLM reasoning and agentic capabilities, yet current approaches struggle with both exploration and exploitation. Exploration suffers from low success rates on difficult tasks and high costs of repeated rollouts from scratch. Exploitation suffers from coarse credit assignment and training instability: Trajectory-level rewards penalize valid prefixes for later errors, and failure-dominated groups overwhelm the few positive signals, leaving optimization without constructive direction. To this end, we propose R$^3$L, Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification. To synthesize high-quality trajectories, R$^3$L shifts from stochastic sampling to active synthesis via reflect-then-retry, leveraging language feedback to diagnose errors, transform failed attempts into successful ones, and reduce rollout costs by restarting from identified failure points. With errors diagnosed and localized, Pivotal Credit Assignment updates only the diverging suffix where contrastive signals exist, excluding the shared prefix from gradient update. Since failures dominate on difficult tasks and reflect-then-retry produces off-policy data, risking training instability, Positive Amplification upweights successful trajectories to ensure positive signals guide the optimization process. Experiments on agentic and reasoning tasks demonstrate 5\% to 52\% relative improvements over baselines while maintaining training stability. Our code is released at https://github.com/shiweijiezero/R3L.

</details>


### [122] [ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization](https://arxiv.org/abs/2601.03723)
*Shijie Zhang,Kevin Zhang,Zheyuan Gu,Xiang Guo,Rujun Guo,Shaoyu Liu,Guanjun Jiang,Xiaozhao Wang*

Main category: cs.LG

TL;DR: 本文提出弹性信任区域（ETR）算法，通过动态调整优化约束来改进GRPO在RLVR中的局限性，在AIME和MATH基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR领域主导算法GRPO存在结构限制：对所有样本施加统一、静态的信任区域约束，这假设信号同质性，与结果驱动学习中优势幅度和方差显著波动的异质性现实不符。静态约束无法充分利用高质量信号，同时抑制噪声不足，常导致快速熵崩溃。

Method: 提出弹性信任区域（ETR）机制，通过双级弹性构建信号感知的优化景观：微观层面基于优势幅度缩放裁剪边界以加速高置信路径学习；宏观层面利用组方差隐式分配更大更新预算给处于最优学习区域的任务。

Result: 在AIME和MATH基准测试上的广泛实验表明，ETR持续优于GRPO，实现更高准确率，同时有效缓解策略熵退化以确保持续探索。

Conclusion: ETR通过动态对齐优化约束与信号质量，解决了GRPO的静态约束问题，在RLVR中实现了更优的性能和探索能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \textbf{E}lastic \textbf{T}rust \textbf{R}egions (\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.

</details>


### [123] [EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning](https://arxiv.org/abs/2601.03725)
*Jing-Cheng Pang,Liu Sun,Chang Zhou,Xian Tang,Haichuan Ma,Kun Jiang,Jianlong Wang,Kai Zhang,Sijie Wu,Haoran Cai,Chenwei Wu,Xubin Li,Xin Chen*

Main category: cs.LG

TL;DR: EDCO是一个用于领域特定大语言模型微调的动态课程学习框架，通过推理熵估计和动态课程编排，在通信、医疗和法律领域优于传统静态课程策略，同时减少83.5%的计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型微调方法大多采用静态课程学习，即在训练前预先排序训练样本，这种方法缺乏对模型在微调过程中动态需求的适应性。需要一种能够根据模型学习状态动态调整课程的方法。

Method: EDCO框架基于推理熵和动态课程编排两个关键概念，包含三个核心组件：1) 使用前缀标记近似全序列熵的高效熵估计器；2) 基于熵的课程生成器，选择推理熵最高的数据点；3) 在选定课程上优化模型的LLM训练器。

Result: 在通信、医疗和法律领域的实验中，EDCO在监督学习和强化学习设置下，对Qwen3-4B和Llama3.2-3B模型的微调效果优于传统课程策略。提出的高效熵估计方法在保持高精度的同时，减少了83.5%的计算时间。

Conclusion: EDCO通过动态调整课程学习策略，有效提升了领域特定大语言模型的微调效率和效果，为自适应课程学习提供了新的解决方案。

Abstract: Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.

</details>


### [124] [Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables](https://arxiv.org/abs/2601.03753)
*Paulius Rauba,Viktor Cikojevic,Fran Bartolic,Sam Levang,Ty Dickinson,Chase Dwelle*

Main category: cs.LG

TL;DR: GEM-2是一个概率Transformer模型，直接学习全球大气动力学和用户决策相关的变量，相比传统NWP模型和复杂ML方法，在轻量级架构下实现了更好的预测性能和决策价值。


<details>
  <summary>Details</summary>
Motivation: 天气预报用户面临一个困境：许多决策相关的目标（如极值、累积量、阈值超限）是大气状态变量的函数，而非状态变量本身。用户需要通过后处理来估计这些目标，这可能导致次优结果和结构偏差。核心问题是决策依赖于这些函数分布的预测，而模型并未直接学习这些分布。

Method: 提出GEM-2概率Transformer，联合学习全球大气动力学和用户直接使用的变量套件。使用CRPS目标训练轻量级模型（约2.75亿参数），相比最先进方法实现20-100倍训练加速，无需昂贵的多步扩散过程或定制多阶段微调策略。

Result: GEM-2直接超越业务数值天气预报模型，与依赖复杂扩散过程或定制微调策略的ML模型竞争。在决策理论评估下展示最先进的经济价值指标，在S2S和季节时间尺度上稳定收敛到气候学，对许多常见架构和训练设计选择表现出惊人的不敏感性。

Conclusion: 通过直接训练模型学习决策相关的函数变量，可以避免后处理带来的偏差，在轻量级架构下实现更好的天气预报性能和决策价值，为天气预报的端到端学习提供了有前景的方向。

Abstract: Weather forecasts sit upstream of high-stakes decisions in domains such as grid operations, aviation, agriculture, and emergency response. Yet forecast users often face a difficult trade-off. Many decision-relevant targets are functionals of the atmospheric state variables, such as extrema, accumulations, and threshold exceedances, rather than state variables themselves. As a result, users must estimate these targets via post-processing, which can be suboptimal and can introduce structural bias. The core issue is that decisions depend on distributions over these functionals that the model is not trained to learn directly.
  In this work, we introduce GEM-2, a probabilistic transformer that jointly learns global atmospheric dynamics alongside a suite of variables that users directly act upon. Using this training recipe, we show that a lightweight (~275M params) and computationally efficient (~20-100x training speedup relative to state-of-the-art) transformer trained on the CRPS objective can directly outperform operational numerical weather prediction (NWP) models and be competitive with ML models that rely on expensive multi-step diffusion processes or require bespoke multi-stage fine-tuning strategies. We further demonstrate state-of-the-art economic value metrics under decision-theoretic evaluation, stable convergence to climatology at S2S and seasonal timescales, and a surprising insensitivity to many commonly assumed architectural and training design choices.

</details>


### [125] [Learning Shrinks the Hard Tail: Training-Dependent Inference Scaling in a Solvable Linear Model](https://arxiv.org/abs/2601.03764)
*Noam Levi*

Main category: cs.LG

TL;DR: 论文提出潜在实例难度(LID)模型，分析具有内在异质难度的目标在最后一层微调中的神经缩放规律，发现pass@k失败率呈幂律衰减，但指数β_eff随训练样本增加而增长，最终饱和于难度分布尾部决定的固有极限β。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络缩放规律时，现有模型通常假设目标具有同质难度，但现实世界中目标往往具有内在的、实例异质的难度。本文旨在分析这种异质难度如何影响神经缩放规律，特别是对推理性能的影响。

Method: 提出潜在实例难度(LID)模型，其中每个输入的目标方差由潜在"精度"参数控制，该参数服从重尾分布。在可解析的最后一层微调框架下，分析泛化损失和推理性能的缩放规律，特别是pass@k失败率的幂律衰减行为。

Result: 泛化损失遵循标准缩放规律，但pass@k失败率呈现幂律衰减k^{-β_eff}，其中β_eff随训练样本数N增加而增长，最终饱和于难度分布尾部决定的固有极限β。这表明学习过程缩小了误差分布的"硬尾"部分。

Conclusion: LID模型揭示了学习与推理之间的耦合关系：模型泛化误差的改善会陡化pass@k曲线，直到不可约的目标方差占主导。模型提供了可测试的闭式预测，包括计算分配规则（训练应在饱和前，推理尝试应在饱和后），并在模拟和真实数据中得到验证。

Abstract: We analyze neural scaling laws in a solvable model of last-layer fine-tuning where targets have intrinsic, instance-heterogeneous difficulty. In our Latent Instance Difficulty (LID) model, each input's target variance is governed by a latent ``precision'' drawn from a heavy-tailed distribution. While generalization loss recovers standard scaling laws, our main contribution connects this to inference. The pass@$k$ failure rate exhibits a power-law decay, $k^{-β_\text{eff}}$, but the observed exponent $β_\text{eff}$ is training-dependent. It grows with sample size $N$ before saturating at an intrinsic limit $β$ set by the difficulty distribution's tail. This coupling reveals that learning shrinks the ``hard tail'' of the error distribution: improvements in the model's generalization error steepen the pass@$k$ curve until irreducible target variance dominates. The LID model yields testable, closed-form predictions for this behavior, including a compute-allocation rule that favors training before saturation and inference attempts after. We validate these predictions in simulations and in two real-data proxies: CIFAR-10H (human-label variance) and a maths teacher-student distillation task.

</details>


### [126] [Improving Compactness and Reducing Ambiguity of CFIRE Rule-Based Explanations](https://arxiv.org/abs/2601.03776)
*Sebastian Müller,Tobias Schneider,Ruben Kemna,Vanessa Toborek*

Main category: cs.LG

TL;DR: CFIRE算法从局部解释构建紧凑的代理规则模型，但存在规则歧义问题。本文提出后剪枝策略去除低贡献或冲突覆盖的规则，减少模型大小和歧义，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据训练的模型在敏感领域广泛应用，需要解释方法满足透明度需求。CFIRE算法虽然有效，但可能将不同类别的规则分配给同一样本，导致解释歧义。

Method: 提出后剪枝策略，移除低贡献或冲突覆盖的规则，从而减少模型大小和歧义，同时保持模型保真度。

Result: 在多个数据集上的实验证实，该方法能显著减小模型规模、减少歧义，同时对预测性能影响最小。

Conclusion: 通过后剪枝策略改进CFIRE算法，能够构建更紧凑、歧义更少的代理规则模型，在保持预测性能的同时提高解释的清晰度。

Abstract: Models trained on tabular data are widely used in sensitive domains, increasing the demand for explanation methods to meet transparency needs. CFIRE is a recent algorithm in this domain that constructs compact surrogate rule models from local explanations. While effective, CFIRE may assign rules associated with different classes to the same sample, introducing ambiguity. We investigate this ambiguity and propose a post-hoc pruning strategy that removes rules with low contribution or conflicting coverage, yielding smaller and less ambiguous models while preserving fidelity. Experiments across multiple datasets confirm these improvements with minimal impact on predictive performance.

</details>


### [127] [Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs](https://arxiv.org/abs/2601.03793)
*Sethupathy Parameswaran,Suresh Sundaram,Yuan Fang*

Main category: cs.LG

TL;DR: 提出ZPT框架，通过双模态条件生成器实现零样本节点分类，无需标注数据


<details>
  <summary>Details</summary>
Motivation: 文本属性图(TAGs)中的零样本节点分类面临缺乏标注数据的挑战，现有方法难以有效处理

Method: 1) 预训练图-语言模型捕获图结构和文本信息；2) 训练条件生成器学习节点在图和文本模态的联合分布；3) 仅基于类别名称生成合成样本；4) 使用合成嵌入进行连续提示调优

Result: 在多个基准数据集上优于现有最先进基线，消融研究验证了双模态生成器的贡献

Conclusion: 提出的ZPT框架通过双模态条件生成和提示调优，有效解决了零样本节点分类问题

Abstract: Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.

</details>


### [128] [Quantum vs. Classical Machine Learning: A Benchmark Study for Financial Prediction](https://arxiv.org/abs/2601.03802)
*Rehan Ahmad,Muhammad Kashif,Nouhaila Innan,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出可复现的量子机器学习基准测试框架，系统比较QML模型与架构匹配的经典模型在三个金融任务上的表现，识别当前QML模型何时能超越经典方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在金融领域的应用潜力尚未得到系统评估，需要公平的基准测试来确定当前QML模型相对于经典方法的实际优势场景。

Method: 建立可复现的基准测试框架，标准化数据分割、特征和评估指标，在三个金融任务上比较QML与架构匹配的经典模型：方向性收益预测、实时交易模拟、已实现波动率预测。

Result: 量子方法在数据结构和电路设计良好对齐时表现更优：在方向性分类中，混合量子神经网络在AAPL和KCHOL股票上分别提升3.8/3.4和4.9/3.6个AUC/准确率点；在实时交易中，QLSTM在4个S&P 500机制中的2个获得更高风险调整收益；在波动率预测中，角度编码QSVR在KCHOL上获得最低QLIKE。

Conclusion: 基准测试框架清晰识别了当前QML架构提供实质性改进的场景（数据结构和电路设计对齐时），以及经典方法仍然占主导的领域，为QML在金融领域的实际应用提供了指导。

Abstract: In this paper, we present a reproducible benchmarking framework that systematically compares QML models with architecture-matched classical counterparts across three financial tasks: (i) directional return prediction on U.S. and Turkish equities, (ii) live-trading simulation with Quantum LSTMs versus classical LSTMs on the S\&P 500, and (iii) realized volatility forecasting using Quantum Support Vector Regression. By standardizing data splits, features, and evaluation metrics, our study provides a fair assessment of when current-generation QML models can match or exceed classical methods. Our results reveal that quantum approaches show performance gains when data structure and circuit design are well aligned. In directional classification, hybrid quantum neural networks surpass the parameter-matched ANN by \textbf{+3.8 AUC} and \textbf{+3.4 accuracy points} on \texttt{AAPL} stock and by \textbf{+4.9 AUC} and \textbf{+3.6 accuracy points} on Turkish stock \texttt{KCHOL}. In live trading, the QLSTM achieves higher risk-adjusted returns in \textbf{two of four} S\&P~500 regimes. For volatility forecasting, an angle-encoded QSVR attains the \textbf{lowest QLIKE} on \texttt{KCHOL} and remains within $\sim$0.02-0.04 QLIKE of the best classical kernels on \texttt{S\&P~500} and \texttt{AAPL}. Our benchmarking framework clearly identifies the scenarios where current QML architectures offer tangible improvements and where established classical methods continue to dominate.

</details>


### [129] [Detecting Semantic Backdoors in a Mystery Shopping Scenario](https://arxiv.org/abs/2601.03805)
*Arpad Berta,Gabor Danner,Istvan Hegedus,Mark Jelasity*

Main category: cs.LG

TL;DR: 提出一种检测语义后门的方法，假设已知干净训练数据和训练方法，通过创建参考模型池并校准模型距离阈值来识别干净模型，最可靠的方法是基于对抗训练请求。


<details>
  <summary>Details</summary>
Motivation: 语义后门检测是一个重要但关注较少的问题，相比基于触发模式的后门更难检测。研究动机来自消费者保护场景，监管机构通过神秘购物测试机器学习服务提供商是否植入了后门。

Method: 在已知干净训练数据和训练方法的前提下，创建包含干净和中毒模型的参考模型池，校准模型距离阈值。提出多种模型距离计算方法，并测试了提供者进行自适应攻击的防御场景。最可靠的方法是基于对抗训练请求，通过反转模型生成最大化与干净样本距离的输入样本来测量模型距离。

Result: 该方法通常能够完全分离干净和中毒模型，在检测语义后门方面优于现有最先进的后门检测器。

Conclusion: 提出的方法在已知训练数据和方法的假设下，能够有效检测语义后门，特别是在对抗训练设置下，通过模型反转生成特殊输入样本测量模型距离，展现出优越的检测性能。

Abstract: Detecting semantic backdoors in classification models--where some classes can be activated by certain natural, but out-of-distribution inputs--is an important problem that has received relatively little attention. Semantic backdoors are significantly harder to detect than backdoors that are based on trigger patterns due to the lack of such clearly identifiable patterns. We tackle this problem under the assumption that the clean training dataset and the training recipe of the model are both known. These assumptions are motivated by a consumer protection scenario, in which the responsible authority performs mystery shopping to test a machine learning service provider. In this scenario, the authority uses the provider's resources and tools to train a model on a given dataset and tests whether the provider included a backdoor. In our proposed approach, the authority creates a reference model pool by training a small number of clean and poisoned models using trusted infrastructure, and calibrates a model distance threshold to identify clean models. We propose and experimentally analyze a number of approaches to compute model distances and we also test a scenario where the provider performs an adaptive attack to avoid detection. The most reliable method is based on requesting adversarial training from the provider. The model distance is best measured using a set of input samples generated by inverting the models in such a way as to maximize the distance from clean samples. With these settings, our method can often completely separate clean and poisoned models, and it proves to be superior to state-of-the-art backdoor detectors as well.

</details>


### [130] [Logic Tensor Network-Enhanced Generative Adversarial Network](https://arxiv.org/abs/2601.03839)
*Nijesh Upreti,Vaishak Belle*

Main category: cs.LG

TL;DR: LTN-GAN结合逻辑张量网络与生成对抗网络，通过逻辑约束增强样本生成，在保持生成质量的同时提升逻辑一致性


<details>
  <summary>Details</summary>
Motivation: 传统GAN缺乏融入先验知识和逻辑一致性的机制，限制了在需要遵循规则的领域中的应用

Method: 将逻辑张量网络（LTNs）集成到GAN框架中，利用一阶逻辑约束指导生成过程，实现神经符号结合

Result: 在合成数据集（高斯、网格、环）和MNIST数据集上，LTN-GAN在遵循预定义逻辑约束方面显著优于传统GAN，同时保持生成样本的质量和多样性

Conclusion: 神经符号方法在知识密集型领域的生成建模中具有巨大潜力，逻辑约束能有效提升生成样本的逻辑一致性和多样性

Abstract: In this paper, we introduce Logic Tensor Network-Enhanced Generative Adversarial Network (LTN-GAN), a novel framework that enhances Generative Adversarial Networks (GANs) by incorporating Logic Tensor Networks (LTNs) to enforce domain-specific logical constraints during the sample generation process. Although GANs have shown remarkable success in generating realistic data, they often lack mechanisms to incorporate prior knowledge or enforce logical consistency, limiting their applicability in domains requiring rule adherence. LTNs provide a principled way to integrate first-order logic with neural networks, enabling models to reason over and satisfy logical constraints. By combining the strengths of GANs for realistic data synthesis with LTNs for logical reasoning, we gain valuable insights into how logical constraints influence the generative process while improving both the diversity and logical consistency of the generated samples. We evaluate LTN-GAN across multiple datasets, including synthetic datasets (gaussian, grid, rings) and the MNIST dataset, demonstrating that our model significantly outperforms traditional GANs in terms of adherence to predefined logical constraints while maintaining the quality and diversity of generated samples. This work highlights the potential of neuro-symbolic approaches to enhance generative modeling in knowledge-intensive domains.

</details>


### [131] [Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences](https://arxiv.org/abs/2601.03882)
*Shudong Liu,Hanwen Zhang,Xiuling Wang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FALCON框架通过特征感知的分层token序列生成和知识蒸馏，提升单次联邦学习在非IID图像数据上的性能


<details>
  <summary>Details</summary>
Motivation: 现有单次联邦学习方法在医学图像等真实场景中性能不足，处理非IID数据效率低下，需要更鲁棒高效的解决方案

Method: 1) 使用预训练视觉编码器将图像压缩为分层token序列；2) 多尺度自回归transformer生成器建模token分布；3) 客户端上传合成序列和本地分类器；4) 服务器端知识蒸馏减少对精确分布建模的依赖

Result: 在医学和自然图像数据集上验证有效，在多种非IID场景中平均准确率比最佳基线提升9.58%

Conclusion: FALCON框架显著提升了单次联邦学习在非IID图像数据上的性能，为医学图像分析等实际应用提供了有效解决方案

Abstract: One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.

</details>


### [132] [Spectral Manifold Regularization for Stable and Modular Routing in Deep MoE Architectures](https://arxiv.org/abs/2601.03889)
*Ibrahim Delibasoglu*

Main category: cs.LG

TL;DR: SR-MoE通过谱正则化约束路由流形几何结构，解决MoE架构中的专家坍缩问题，保持模块化结构完整性，实现稳定终身学习。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构虽然能高效扩展神经网络规模，但存在专家坍缩问题——路由收敛到少数主导专家，这降低了模型容量并在适应过程中导致灾难性干扰。

Method: 提出谱正则化混合专家方法，通过双重正则化施加几何约束：谱范数约束限制路由函数的Lipschitz连续性，稳定秩惩罚保持专家选择中的高维特征多样性。

Result: 传统线性门控随深度增加而失效（准确率下降达4.72%），而SR-MoE保持结构完整性（平均干扰-0.32%）。谱约束促进正向知识迁移，实现局部专家更新而无全局性能衰减。

Conclusion: SR-MoE为构建高容量、模块化网络提供通用解决方案，能够实现稳定的终身学习，通过几何约束路由流形来强制执行结构模块化。

Abstract: Mixture of Experts (MoE) architectures enable efficient scaling of neural networks but suffer from expert collapse, where routing converges to a few dominant experts. This reduces model capacity and causes catastrophic interference during adaptation. We propose the Spectrally-Regularized Mixture of Experts (SR-MoE), which imposes geometric constraints on the routing manifold to enforce structural modularity. Our method uses dual regularization: spectral norm constraints bound routing function Lipschitz continuity, while stable rank penalties preserve high-dimensional feature diversity in expert selection. We evaluate SR-MoE across architectural scales and dataset complexities using modular one-shot adaptation tasks. Results show that traditional linear gating fails with increasing depth (accuracy drops up to 4.72% due to expert entanglement), while SR-MoE maintains structural integrity (mean interference -0.32%). Our spectral constraints facilitate positive knowledge transfer, enabling localized expert updates without global performance decay. SR-MoE provides a general solution for building high-capacity, modular networks capable of stable lifelong learning.

</details>


### [133] [Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training](https://arxiv.org/abs/2601.03895)
*Chi Liu,Xin Chen*

Main category: cs.LG

TL;DR: 提出ABC-GRPO算法，通过自适应边界裁剪改进GRPO，在数学推理任务上表现更优，保持更高熵值避免早熟收敛。


<details>
  <summary>Details</summary>
Motivation: 分析发现GRPO的裁剪机制在某些场景下不够理想，通过适当修改可以显著提升其灵活性和泛化能力。

Method: 提出ABC-GRPO（自适应边界裁剪GRPO），这是对原始GRPO框架的非对称自适应改进，采用自适应边界裁剪机制。

Result: ABC-GRPO在Qwen3大语言模型的数学推理任务上表现优于标准GRPO，训练过程中保持更高的熵值，保护了模型的探索能力并缓解了早熟收敛。

Conclusion: ABC-GRPO是对GRPO的有效改进，通过自适应边界裁剪机制提升了性能，代码已开源便于复现。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.

</details>


### [134] [A Gap Between Decision Trees and Neural Networks](https://arxiv.org/abs/2601.03919)
*Akash Kumar*

Main category: cs.LG

TL;DR: 研究决策树与浅层神经网络之间的几何简单性与准确性权衡：决策树产生轴对齐的决策区域，而浅层ReLU网络通过阈值化预测。使用Radon全变差(RTV)半范数分析几何复杂度，发现决策树指示函数及其连续替代具有无限RTV，但构造了有限RTV的平滑屏障分数来精确恢复决策区域。


<details>
  <summary>Details</summary>
Motivation: 探索几何简单性（作为可解释性的一种形式）与浅层神经网络准确逼近轴对齐决策树能力之间的潜在冲突。决策树产生基于规则的轴对齐决策区域（盒子的有限并集），而浅层ReLU网络通常作为分数模型训练，通过阈值化获得预测。

Method: 使用Radon全变差(RTV)半范数分析无限宽度、有界范数、单隐藏层ReLU网络的几何复杂度。首先分析决策树指示函数及其连续替代（分段线性斜坡平滑、sigmoidal平滑、高斯卷积）的RTV性质。然后构造具有有限RTV的平滑屏障分数S_A，其固定阈值τ=1能精确恢复决策盒。在边界附近的温和管质量条件下，证明L_1(P)校准界限和RTV上界。

Result: 发现硬树指示函数1_A具有无限RTV；两种自然的分段连续替代（分段线性斜坡平滑和sigmoidal平滑）在d>1维时也具有无限RTV；高斯卷积产生有限RTV但具有指数依赖d。构造的平滑屏障分数S_A具有有限RTV且能精确恢复决策区域，并给出了校准界限和RTV上界。实验展示了准确性与复杂度之间的权衡。

Conclusion: 决策树的几何简单性（轴对齐决策区域）与浅层神经网络的准确逼近之间存在根本性冲突。通过构造具有有限RTV的平滑屏障分数，可以在保持几何简单性的同时实现准确分类，但需要在准确性和复杂度之间进行权衡。阈值选择影响训练在权衡曲线上的位置。

Abstract: We study when geometric simplicity of decision boundaries, used here as a notion of interpretability, can conflict with accurate approximation of axis-aligned decision trees by shallow neural networks. Decision trees induce rule-based, axis-aligned decision regions (finite unions of boxes), whereas shallow ReLU networks are typically trained as score models whose predictions are obtained by thresholding. We analyze the infinite-width, bounded-norm, single-hidden-layer ReLU class through the Radon total variation ($\mathrm{R}\mathrm{TV}$) seminorm, which controls the geometric complexity of level sets.
  We first show that the hard tree indicator $1_A$ has infinite $\mathrm{R}\mathrm{TV}$. Moreover, two natural split-wise continuous surrogates--piecewise-linear ramp smoothing and sigmoidal (logistic) smoothing--also have infinite $\mathrm{R}\mathrm{TV}$ in dimensions $d>1$, while Gaussian convolution yields finite $\mathrm{R}\mathrm{TV}$ but with an explicit exponential dependence on $d$.
  We then separate two goals that are often conflated: classification after thresholding (recovering the decision set) versus score learning (learning a calibrated score close to $1_A$). For classification, we construct a smooth barrier score $S_A$ with finite $\mathrm{R}\mathrm{TV}$ whose fixed threshold $τ=1$ exactly recovers the box. Under a mild tube-mass condition near $\partial A$, we prove an $L_1(P)$ calibration bound that decays polynomially in a sharpness parameter, along with an explicit $\mathrm{R}\mathrm{TV}$ upper bound in terms of face measures. Experiments on synthetic unions of rectangles illustrate the resulting accuracy--complexity tradeoff and how threshold selection shifts where training lands along it.

</details>


### [135] [FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning](https://arxiv.org/abs/2601.03938)
*Yujie Feng,Hao Wang,Jian Li,Xu Chu,Zhaolu Kang,Yiran Liu,Yasha Wang,Philip S. Yu,Xiao-Ming Wu*

Main category: cs.LG

TL;DR: FOREVER：基于遗忘曲线的大语言模型持续学习框架，通过模型时间对齐重放调度，有效缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法大多依赖固定的步数启发式重放策略，但这些策略与模型实际学习进度不一致，因为相同的训练步数可能导致不同程度的参数变化。受人类遗忘曲线启发，作者提出将重放调度与模型中心时间概念对齐。

Method: FOREVER框架包含三个核心组件：1）基于优化器更新幅度定义"模型时间"；2）遗忘曲线启发的重放调度器决定何时重放；3）强度感知正则化机制自适应控制如何重放。

Result: 在三个持续学习基准测试和0.6B到13B参数规模的模型上进行了广泛实验，FOREVER能够持续缓解灾难性遗忘问题。

Conclusion: 通过将重放调度与模型内部演化对齐，FOREVER提供了一种更符合大语言模型学习动态的持续学习方法，相比传统步数启发式方法更加有效。

Abstract: Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.

</details>


### [136] [Stage-specific cancer survival prediction enriched by explainable machine learning](https://arxiv.org/abs/2601.03977)
*Parisa Poorhasani,Bogdan Iancu*

Main category: cs.LG

TL;DR: 该研究开发了可解释的机器学习模型，用于预测结直肠癌、胃癌和肝癌的特定分期生存率，揭示了传统混合分期模型可能高估性能并忽视分期特异性差异的问题。


<details>
  <summary>Details</summary>
Motivation: 传统癌症生存预测模型通常将所有分期数据混合训练，这可能高估模型性能并忽视不同癌症分期之间的重要差异。同时，现有机器学习生存分析研究缺乏可解释性和透明度。

Method: 使用SEER数据集，为结直肠癌、胃癌和肝癌分别开发特定分期的可解释机器学习模型，并应用SHAP和LIME等可解释性技术来揭示特征与癌症分期之间的相互作用。

Result: 研究发现某些人口统计学和临床变量在不同癌症分期和类型中对生存率的影响存在差异，这些传统黑盒模型中隐藏的相互作用通过可解释性技术得以揭示。

Conclusion: 特定分期模型提供了对每个癌症阶段最重要因素的新见解，不仅提高了模型透明度，还具有临床相关性，支持个性化治疗规划。

Abstract: Despite the fact that cancer survivability rates vary greatly between stages, traditional survival prediction models have frequently been trained and assessed using examples from all combined phases of the disease. This method may result in an overestimation of performance and ignore the stage-specific variations. Using the SEER dataset, we created and verified explainable machine learning (ML) models to predict stage-specific cancer survivability in colorectal, stomach, and liver cancers. ML-based cancer survival analysis has been a long-standing topic in the literature; however, studies involving the explainability and transparency of ML survivability models are limited. Our use of explainability techniques, including SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), enabled us to illustrate significant feature-cancer stage interactions that would have remained hidden in traditional black-box models. We identified how certain demographic and clinical variables influenced survival differently across cancer stages and types. These insights provide not only transparency but also clinical relevance, supporting personalized treatment planning. By focusing on stage-specific models, this study provides new insights into the most important factors at each stage of cancer, offering transparency and potential clinical relevance to support personalized treatment planning.

</details>


### [137] [MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis](https://arxiv.org/abs/2601.04121)
*Gabriel Ansah,Eden Ruffell,Delmiro Fernandez-Reyes,Petru Manescu*

Main category: cs.LG

TL;DR: 提出联邦学习框架用于白细胞形态分析，在保护数据隐私的同时实现跨机构协作训练，提升模型在资源有限医疗环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 中低收入国家需要自动化血液形态分析支持血液学诊断，但面临染色差异、成像差异和罕见形态等数据集偏移问题。由于隐私法规和数据共享限制，构建集中式数据集往往不可行。

Method: 引入联邦学习框架，使多个临床机构能够在不交换训练数据的情况下进行协作训练。使用来自多个临床站点的血涂片，联邦模型学习鲁棒的、领域不变的表征，同时保持完整的数据隐私。

Result: 在卷积和基于Transformer的架构上进行评估，联邦训练实现了强大的跨站点性能，与集中式训练相比，对未见机构的泛化能力有所提高。

Conclusion: 联邦学习是开发公平、可扩展且可泛化的医学影像AI的实用且保护隐私的方法，特别适用于资源有限的医疗环境。

Abstract: Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning framework for white blood cell morphology analysis that enables collaborative training across institutions without exchanging training data. Using blood films from multiple clinical sites, our federated models learn robust, domain-invariant representations while preserving complete data privacy. Evaluations across convolutional and transformer-based architectures show that federated training achieves strong cross-site performance and improved generalization to unseen institutions compared to centralized training. These findings highlight federated learning as a practical and privacy-preserving approach for developing equitable, scalable, and generalizable medical imaging AI in resource-limited healthcare environments.

</details>


### [138] [Modeling Behavioral Patterns in News Recommendations Using Fuzzy Neural Networks](https://arxiv.org/abs/2601.04019)
*Kevin Innerebner,Stephan Bartl,Markus Reiter-Haas,Elisabeth Lex*

Main category: cs.LG

TL;DR: 提出使用模糊神经网络从行为数据中学习可读规则来预测文章点击的透明新闻推荐系统


<details>
  <summary>Details</summary>
Motivation: 当前新闻推荐系统多为黑盒模型，缺乏透明度，不利于编辑决策制定

Method: 使用模糊神经网络从行为数据中学习人类可读规则，通过可配置阈值控制规则复杂度

Result: 在MIND和EB-NeRD数据集上准确预测点击行为，性能与多个基线方法相当，同时学习到可读规则

Conclusion: 学习到的规则揭示了新闻消费模式，使编辑能够将内容策划目标与目标受众行为对齐

Abstract: News recommender systems are increasingly driven by black-box models, offering little transparency for editorial decision-making. In this work, we introduce a transparent recommender system that uses fuzzy neural networks to learn human-readable rules from behavioral data for predicting article clicks. By extracting the rules at configurable thresholds, we can control rule complexity and thus, the level of interpretability. We evaluate our approach on two publicly available news datasets (i.e., MIND and EB-NeRD) and show that we can accurately predict click behavior compared to several established baselines, while learning human-readable rules. Furthermore, we show that the learned rules reveal news consumption patterns, enabling editors to align content curation goals with target audience behavior.

</details>


### [139] [Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing](https://arxiv.org/abs/2601.04051)
*Viktor Martinek,Roland Herzog*

Main category: cs.LG

TL;DR: 该论文扩展了符号回归方法，引入多类别变量和参数共享的中间层级，减少参数数量同时增强问题理解


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法在处理类别变量时，参数要么完全共享（类别无关），要么完全不共享（类别特定）。作者希望引入中间层级的参数共享，以更好地描述具有多类别变量的现象，减少参数数量并揭示更多问题信息。

Method: 扩展符号回归框架，支持多个类别变量，并引入参数共享的中间层级：对于两个类别变量，参数可以跨一个类别共享但在另一个类别中变化。使用合成数据集测试数据需求减少和迁移学习能力，并在天体物理数据集上验证方法。

Result: 在合成实验中，新方法显著减少了数据需求并展示了良好的迁移学习能力。在天体物理数据集上，实现了与先前研究相似的拟合质量，但需要更少的参数，并提取了更多关于问题的信息。

Conclusion: 提出的多类别变量符号回归框架通过引入中间层级的参数共享，不仅减少了参数数量，还增强了模型的解释能力和科学发现潜力，为复杂现象的符号描述提供了更强大的工具。

Abstract: Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only "non-shared" (category-value-specific) parameters, and others also incorporate "shared" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.

</details>


### [140] [LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis](https://arxiv.org/abs/2601.04054)
*Yayati Jadhav,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出基于自回归扩散框架的机械连杆机构逆向设计方法，通过序列化图生成解决传统优化方法难以处理的高维非线性设计空间问题。


<details>
  <summary>Details</summary>
Motivation: 机械连杆机构设计面临节点位置连续变量、拓扑结构离散配置和非线性运动学约束的复杂耦合问题。传统优化和启发式方法难以处理这种高维非线性设计空间，因为微小扰动会显著改变轨迹，且组合设计空间呈指数增长。

Method: 采用自回归扩散框架，将连杆机构表示为序列构建的图（节点对应关节，边对应刚性连杆）。结合因果Transformer和去噪扩散概率模型（DDPM），两者都以目标轨迹编码为条件。因果Transformer自回归预测离散拓扑结构，DDPM细化每个节点的空间坐标和与已生成节点的边连接。

Result: 该方法能够成功合成包含多达20个节点的连杆系统，并可扩展到N节点架构。相比传统优化方法，该数据驱动方法实现了可扩展的逆向设计，能够处理任意节点数量的机构。

Conclusion: 该工作推进了自回归图生成方法和计算运动学合成，为复杂机械系统的可扩展逆向设计建立了新范式，能够自主纠正设计过程中的退化配置。

Abstract: Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.

</details>


### [141] [Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia](https://arxiv.org/abs/2601.04057)
*M. Yin,K. G. Ravindran,C. Hadjipanayi,A. Bannon,A. Rapeaux,C. Della Monica,T. S. Lande,Derk-Jan Dijk,T. G. Constandinou*

Main category: cs.LG

TL;DR: 提出基于对抗学习的深度迁移学习框架，利用大规模PSG数据增强UWB雷达睡眠分期性能，在老年人群中达到79.5%准确率


<details>
  <summary>Details</summary>
Motivation: UWB雷达作为非侵入式、低成本的居家睡眠监测方案具有前景，但雷达睡眠数据稀缺限制了模型的泛化能力，需要解决数据变异性和样本量有限的问题

Method: 开发端到端神经网络，结合大规模PSG数据集和雷达数据训练，采用对抗学习的域适应方法弥合PSG与雷达信号之间的知识鸿沟

Result: 在47名老年人（含18名阿尔茨海默病前期或轻度患者）的雷达数据集上，对清醒、REM、浅睡和深睡四分类达到79.5%准确率和0.65 Kappa值

Conclusion: 该方法有效解决了数据变异性和样本量有限的挑战，显著提升了自动睡眠分期的可靠性，特别适用于雷达数据有限的情况

Abstract: Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.

</details>


### [142] [Minimum distance classification for nonlinear dynamical systems](https://arxiv.org/abs/2601.04058)
*Dominique Martinez*

Main category: cs.LG

TL;DR: 提出Dynafit方法，通过核技巧学习轨迹与底层动力学之间的距离度量，用于非线性动力系统的分类任务


<details>
  <summary>Details</summary>
Motivation: 解决由不同非线性动力系统生成的轨迹数据的分类问题，每个类别对应一个不同的动力系统

Method: 基于核的方法，通过近似Koopman算子在特征空间中全局线性化动力学，利用核技巧计算距离度量，并可整合部分先验知识

Result: Dynafit可应用于各种非线性动力系统和传感器的分类任务，在逻辑映射的混沌检测、手写动力学识别和视觉动态纹理识别三个示例中展示了有效性

Conclusion: Dynafit是一种有效的核基方法，能够学习轨迹与底层动力学之间的距离度量，适用于非线性动力系统的分类问题

Abstract: We address the problem of classifying trajectory data generated by some nonlinear dynamics, where each class corresponds to a distinct dynamical system. We propose Dynafit, a kernel-based method for learning a distance metric between training trajectories and the underlying dynamics. New observations are assigned to the class with the most similar dynamics according to the learned metric. The learning algorithm approximates the Koopman operator which globally linearizes the dynamics in a (potentially infinite) feature space associated with a kernel function. The distance metric is computed in feature space independently of its dimensionality by using the kernel trick common in machine learning. We also show that the kernel function can be tailored to incorporate partial knowledge of the dynamics when available. Dynafit is applicable to various classification tasks involving nonlinear dynamical systems and sensors. We illustrate its effectiveness on three examples: chaos detection with the logistic map, recognition of handwritten dynamics and of visual dynamic textures.

</details>


### [143] [Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models](https://arxiv.org/abs/2601.04110)
*Magnus Bühler,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: CausalMixFT通过结构因果模型生成结构一致的合成样本，增强表格基础模型在数据稀缺情况下的微调鲁棒性和性能


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺情况下微调表格基础模型具有挑战性，因为即使在更稀缺的验证数据上提前停止也往往无法捕捉真实的泛化性能

Method: 提出CausalMixFT方法，使用在目标数据集上拟合的结构因果模型生成结构一致的合成样本，用因果信息增强的合成示例扩充有限的真实数据，保持特征依赖关系同时增加训练多样性

Result: 在TabArena的33个分类数据集和超过2300次微调运行中评估，CausalMixFT将中位数归一化ROC-AUC从0.10（标准微调）提升到0.12，优于CTGAN(-0.01)、TabEBM(-0.04)和TableAugment(-0.09)。同时将验证-测试性能相关性差距从0.67缩小到0.30

Conclusion: 将因果结构纳入数据增强为在低数据情况下微调表格基础模型提供了有效且有原则的途径，能够实现更可靠的基于验证的提前停止

Abstract: Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset. This approach augments limited real data with causally informed synthetic examples, preserving feature dependencies while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2300 fine-tuning runs, our CausalMixFT method consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators such as CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09). Moreover, it narrows the median validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable validation-based early stopping, a key step toward improving fine-tuning stability under data scarcity. These results demonstrate that incorporating causal structure into data augmentation provides an effective and principled route to fine-tuning tabular foundation models in low-data regimes.

</details>


### [144] [Clinical Data Goes MEDS? Let's OWL make sense of it](https://arxiv.org/abs/2601.04164)
*Alberto Marfoglia,Jong Ho Jhee,Adrien Coulet*

Main category: cs.LG

TL;DR: 提出MEDS-OWL本体和meds2rdf转换工具，将医疗事件数据标准(MEDS)与语义网生态系统连接，实现事件型临床数据的FAIR化表示和互操作。


<details>
  <summary>Details</summary>
Motivation: 医疗数据缺乏标准化和语义明确表示，导致机器学习工作流的互操作性和可重复性受限。虽然MEDS标准解决了部分问题，但缺乏与语义网生态系统的集成。

Method: 开发轻量级OWL本体MEDS-OWL，提供形式化概念和关系，将MEDS数据集表示为RDF图；实现Python转换库meds2rdf，将MEDS事件转换为符合本体的RDF图。

Result: MEDS-OWL第一版包含13个类、10个对象属性、20个数据属性和24个OWL公理。在颅内动脉瘤破裂患者护理路径的合成临床数据集上验证了该方法，使用SHACL约束验证了生成的图。

Conclusion: 通过连接MEDS与语义网，这项工作为事件型临床数据提供了可重用的语义层，为后续基于图的分析建立了坚实基础，支持FAIR对齐的数据集转换、溯源感知发布和互操作。

Abstract: The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.

</details>


### [145] [Agentic Rubrics as Contextual Verifiers for SWE Agents](https://arxiv.org/abs/2601.04171)
*Mohit Raghavendra,Anisha Gunjal,Bing Liu,Yunzhong He*

Main category: cs.LG

TL;DR: Agentic Rubrics：一种无需代码执行的软件工程代理验证方法，通过专家代理与代码库交互创建基于上下文的检查清单来评估补丁质量，在SWE-Bench上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程代理验证主要依赖代码执行，但环境设置开销大难以扩展；而可扩展的替代方法（如补丁分类器和启发式方法）缺乏代码库上下文且难以解释。需要一种既高效可扩展又基于代码库上下文的验证方法。

Method: 提出Agentic Rubrics方法：1）专家代理与代码库交互，创建基于上下文的检查清单（rubric checklist）；2）候选补丁根据该检查清单进行评分，无需执行测试。通过上下文收集确保标准具有代码库特异性且明确无歧义。

Result: 在SWE-Bench Verified的并行TTS评估中，Agentic Rubrics在Qwen3-Coder-30B-A3B上达到54.2%得分，在Qwen3-32B上达到40.6%得分，比最强基线至少提升3.5个百分点。分析显示rubric评分与真实测试一致，并能识别测试未捕获的问题。

Conclusion: Agentic Rubrics为软件工程代理提供了高效、可扩展且细粒度的验证信号，通过基于上下文的检查清单实现了无需代码执行的可靠评估，在保持可解释性的同时显著提升了验证效果。

Abstract: Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.

</details>


### [146] [Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation](https://arxiv.org/abs/2601.04176)
*Pietro de Oliveira Esteves*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINNs）从严重噪声下的非线性薛定谔方程（NLSE）中恢复物理参数，在仅500个稀疏随机采样点且含20%加性高斯噪声的情况下，非线性系数β的相对误差小于0.2%，优于传统有限差分方法。


<details>
  <summary>Details</summary>
Motivation: 传统有限差分方法在严重噪声条件下会因数值导数中的噪声放大而失效，特别是在实验数据稀缺且噪声大的时空动力学逆问题中，需要一种能够有效处理噪声的替代方法。

Method: 将物理信息神经网络（PINNs）与自动微分相结合，利用物理约束作为正则化来对抗高测量不确定性，从噪声数据中恢复NLSE的物理参数。

Result: 在β=0.5-2.0范围内，使用100-1000个训练点，都能保持低于1%的准确率；β=1.0时标准差小于0.15%；整个流程在NVIDIA Tesla T4上约80分钟完成。

Conclusion: 物理正则化能有效过滤高测量不确定性，PINNs可作为传统优化方法的可行替代方案，适用于数据稀缺且噪声大的时空动力学逆问题。

Abstract: We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.

</details>


### [147] [Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition](https://arxiv.org/abs/2601.04181)
*Nia Touko,Matthew O A Ellis,Cristiano Capone,Alessio Burrello,Elisa Donati,Luca Manneschi*

Main category: cs.LG

TL;DR: 提出轻量级测试时适应框架，使用TCN骨干网络，通过因果自适应批归一化、GMM对齐与经验回放、元学习三种策略，显著提升表面肌电信号长期解码的稳定性


<details>
  <summary>Details</summary>
Motivation: 表面肌电信号的长期解码受电极偏移、肌肉疲劳和姿势变化引起的信号漂移影响，现有模型在会话间性能急剧下降，且通常需要大数据集或高计算资源，不适用于能效穿戴设备

Method: 提出轻量级测试时适应框架，使用TCN骨干网络，包含三种部署就绪策略：1) 因果自适应批归一化用于实时统计对齐；2) GMM对齐与经验回放防止遗忘；3) 元学习用于快速少样本校准

Result: 在NinaPro DB6多会话数据集上评估，框架显著缩小会话间准确率差距且开销最小。经验回放更新在有限数据下提供优越稳定性，元学习在仅需当前基准数据一小部分的情况下，在一/二样本机制中实现竞争性性能

Conclusion: 为长期假肢使用的鲁棒"即插即用"肌电控制建立了路径，轻量级测试时适应框架能有效应对信号漂移问题，适用于能效穿戴设备

Abstract: Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, "plug-and-play" myoelectric control for long-term prosthetic use.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [148] [Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations](https://arxiv.org/abs/2601.03875)
*Yuyang Fu,Xiuzhen Guo,Ji Shi*

Main category: eess.IV

TL;DR: 提出SVL-DRL框架，通过分阶段强化学习解决医学图像分割中的噪声标注问题，将每个体素视为自主智能体，动态修正标注错误，在三个公开数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割依赖高质量标注数据，但实际中常存在噪声标注（由于器官形态复杂和标注者差异），严重影响分割模型效果。受标注者能基于先验知识修正错误的启发，需要自动化的噪声标注处理方法。

Method: 提出端到端分阶段体素级深度强化学习框架：1) 将噪声标注建模为体素依赖问题，采用分阶段强化学习保证收敛；2) 引入体素级异步优势演员-评论家模块，每个体素作为自主智能体动态优化状态表示；3) 设计新颖的动作空间和结合Dice值与空间连续性度量的复合奖励函数。

Result: 在三个公开医学图像数据集上实现最先进性能，在各种实验设置下Dice和IoU分数平均提升超过3%。

Conclusion: SVL-DRL框架能有效处理医学图像分割中的噪声标注问题，通过分阶段强化学习和体素级智能体设计，自动修正标注错误，显著提升分割精度和语义完整性。

Abstract: Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological structures of organs in medical images and variations among different annotators, which can substantially limit the efficacy of segmentation models. Motivated by the fact that medical imaging annotator can correct labeling errors during segmentation based on prior knowledge, we propose an end-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework for robust medical image segmentation under noisy annotations. This framework employs a dynamic iterative update strategy to automatically mitigate the impact of erroneous labels without requiring manual intervention. The key advancements of SVL-DRL over existing works include: i) formulating noisy annotations as a voxel-dependent problem and addressing it through a novel staged reinforcement learning framework which guarantees robust model convergence; ii) incorporating a voxel-level asynchronous advantage actor-critic (vA3C) module that conceptualizes each voxel as an autonomous agent, which allows each agent to dynamically refine its own state representation during training, thereby directly mitigating the influence of erroneous labels; iii) designing a novel action space for the agents, along with a composite reward function that strategically combines the Dice value and a spatial continuity metric to significantly boost segmentation accuracy while maintain semantic integrity. Experiments on three public medical image datasets demonstrates State-of-The-Art (SoTA) performance under various experimental settings, with an average improvement of over 3\% in both Dice and IoU scores.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [149] [Mastering the Game of Go with Self-play Experience Replay](https://arxiv.org/abs/2601.03306)
*Jingbin Liu,Xuechun Wang*

Main category: cs.AI

TL;DR: QZero是一种无需搜索的模型无关强化学习算法，通过自博弈和离策略经验回放学习纳什均衡策略，仅用7个GPU训练5个月就达到了AlphaGo水平


<details>
  <summary>Details</summary>
Motivation: 围棋长期以来作为人工智能的基准测试，需要复杂的战略推理和长期规划。以往方法如AlphaGo主要依赖基于模型的蒙特卡洛树搜索，本研究旨在探索模型无关强化学习在围棋中的效率

Method: QZero基于熵正则化Q学习，使用单一Q值网络统一策略评估和改进，通过自博弈和离策略经验回放进行训练，无需人类数据或搜索过程

Result: 仅用7个GPU训练5个月，QZero达到了与AlphaGo相当的性能水平，首次证明了模型无关强化学习在围棋中的效率

Conclusion: 该研究首次展示了模型无关强化学习在掌握围棋游戏中的效率，以及离策略强化学习在解决大规模复杂环境中的可行性

Abstract: The game of Go has long served as a benchmark for artificial intelligence, demanding sophisticated strategic reasoning and long-term planning. Previous approaches such as AlphaGo and its successors, have predominantly relied on model-based Monte-Carlo Tree Search (MCTS). In this work, we present QZero, a novel model-free reinforcement learning algorithm that forgoes search during training and learns a Nash equilibrium policy through self-play and off-policy experience replay. Built upon entropy-regularized Q-learning, QZero utilizes a single Q-value network to unify policy evaluation and improvement. Starting tabula rasa without human data and trained for 5 months with modest compute resources (7 GPUs), QZero achieved a performance level comparable to that of AlphaGo. This demonstrates, for the first time, the efficiency of using model-free reinforcement learning to master the game of Go, as well as the feasibility of off-policy reinforcement learning in solving large-scale and complex environments.

</details>


### [150] [Digital Red Queen: Adversarial Program Evolution in Core War with LLMs](https://arxiv.org/abs/2601.03335)
*Akarsh Kumar,Ryan Bahlous-Boldi,Prafull Sharma,Phillip Isola,Sebastian Risi,Yujin Tang,David Ha*

Main category: cs.AI

TL;DR: 提出Digital Red Queen算法，在Core War游戏中通过LLM驱动的自博弈演化程序，实现对抗性持续适应，观察到程序泛化能力增强和行为多样性减少的收敛现象。


<details>
  <summary>Details</summary>
Motivation: 现有LLM演化框架多为静态优化问题，忽视了真实世界演化过程中的开放对抗性动态（Red Queen效应）。研究旨在探索动态对抗目标下的演化过程。

Method: 提出Digital Red Queen算法：在Core War（图灵完备的虚拟环境）中，使用LLM演化汇编式程序（warriors），每轮生成新程序击败所有先前程序，形成持续对抗的演化序列。

Result: 经过多轮演化，程序相对于人类编写的测试程序表现出更强的泛化能力。有趣的是，独立运行中程序行为多样性减少，显示出向通用行为策略的收敛压力，类似于自然界的趋同演化。

Conclusion: 从静态目标转向动态Red Queen目标具有潜在价值。Core War可作为研究对抗性适应的可控沙盒，DRQ的简单有效性表明类似自博弈方法可用于实际对抗领域（如网络安全、抗药性研究）。

Abstract: Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optimization problems, overlooking the open-ended adversarial dynamics that characterize real-world evolutionary processes. Here, we study Digital Red Queen (DRQ), a simple self-play algorithm that embraces these so-called "Red Queen" dynamics via continual adaptation to a changing objective. DRQ uses an LLM to evolve assembly-like programs, called warriors, which compete against each other for control of a virtual machine in the game of Core War, a Turing-complete environment studied in artificial life and connected to cybersecurity. In each round of DRQ, the model evolves a new warrior to defeat all previous ones, producing a sequence of adapted warriors. Over many rounds, we observe that warriors become increasingly general (relative to a set of held-out human warriors). Interestingly, warriors also become less behaviorally diverse across independent runs, indicating a convergence pressure toward a general-purpose behavioral strategy, much like convergent evolution in nature. This result highlights a potential value of shifting from static objectives to dynamic Red Queen objectives. Our work positions Core War as a rich, controllable sandbox for studying adversarial adaptation in artificial systems and for evaluating LLM-based evolution methods. More broadly, the simplicity and effectiveness of DRQ suggest that similarly minimal self-play approaches could prove useful in other more practical multi-agent adversarial domains, like real-world cybersecurity or combating drug resistance.

</details>


### [151] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出多智能体工作流，将主任务描述优化与约束条件解耦，通过定量评分反馈迭代改进提示，显著提升LLM输出对形式约束的遵从性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常生成内容相关但不符合形式约束的输出，传统提示优化方法只关注主任务描述改写，忽略了作为响应验收标准的细粒度约束条件

Method: 提出多智能体工作流，将主任务描述优化与约束条件解耦，使用定量评分作为反馈，迭代重写和改进提示

Result: 该方法生成的修订提示在Llama 3.1 8B和Mixtral-8x 7B等模型上获得显著更高的遵从性评分

Conclusion: 通过解耦任务描述与约束条件优化，并使用定量反馈迭代改进，能有效提升LLM输出对形式约束的遵从性

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [152] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 论文提出一种基于内省探索的强化学习代理，通过隐马尔可夫模型从在线观察中推断"疼痛信念"，将其整合到主观奖励函数中研究自我意识对学习能力的影响，并比较正常与慢性疼痛感知模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能如何建模内部心理状态对于推进AI中的心智理论至关重要。证据表明自我认知与他人认知存在统一系统，本研究通过让强化学习代理在网格世界中推断自身内部状态来探索这种自我意识。

Method: 引入受生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，将该信号整合到主观奖励函数中。建立计算框架比较正常与慢性疼痛感知模型的性能差异。

Result: 内省代理总体上显著优于标准基线代理，能够复现复杂的人类行为模式。研究展示了自我意识建模如何提升AI代理的学习能力和行为复杂性。

Conclusion: 通过模拟生物疼痛作为学习信号的内省探索方法，能够有效增强AI代理的自我意识建模能力，为理解AI中的心智理论和自我认知机制提供了有价值的计算框架。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [153] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的框架，通过明确测量机制认证具身AI系统，需要结构化评估框架、量化评分机制和多目标权衡方法，以不确定性量化为例证，通过无人机系统检测案例展示可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性的具身AI认证方法，需要可验证的评估框架来确保AI系统的可信度，特别是在安全关键应用中。

Method: 提出基于成熟度的认证框架，包含结构化评估、量化评分机制和多目标权衡导航方法，以不确定性量化作为核心测量机制，通过无人机系统检测案例进行验证。

Result: 展示了该框架在无人机系统检测案例中的可行性，证明了通过明确测量机制认证具身AI系统的可行性。

Conclusion: 基于成熟度的框架为具身AI系统认证提供了系统化方法，通过量化测量机制和多目标权衡导航，能够有效评估AI系统的可信度。

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [154] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt：一个自动提示系统，将临床实践指南转化为LLM可用的结构化决策树，用于患者病例评估


<details>
  <summary>Details</summary>
Motivation: 临床实践指南（CPGs）为患者护理提供循证建议，但将其整合到人工智能中面临挑战。现有方法（如基于规则的系统）存在可解释性差、指南遵循不一致和领域适用性窄等限制。

Method: 开发CPGPrompt系统，将叙述性临床指南转化为结构化决策树，利用LLM动态导航决策树进行患者病例评估。在三个领域（头痛、腰痛、前列腺癌）生成合成病例，测试不同决策场景。

Result: 二元专科转诊分类在所有领域表现一致强劲（F1：0.85-1.00），召回率高（1.00±0.00）。多类路径分配表现较低，存在领域差异：头痛（F1：0.47）、腰痛（F1：0.72）、前列腺癌（F1：0.77）。性能差异反映了各指南的结构特点。

Conclusion: CPGPrompt能有效将临床指南转化为LLM可用的结构化决策支持系统，在二元决策任务上表现优异，但在复杂多类路径分配上仍有改进空间，性能受指南结构特征影响。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [155] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型在医疗AI中具有潜力，但无法替代N-of-1试验；两者互补，提出结合框架解决个性化医疗中的矛盾


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在医疗AI中展现出潜力，但能否提供真正个性化的治疗建议仍存疑问。研究发现个性化与外部有效性存在根本矛盾，且需要因果理解而非仅预测能力。N-of-1试验作为个性化医疗的黄金标准，能提供个体内因果证据并保护隐私。

Method: 提出混合框架：大型基础模型擅长从多模态数据中快速生成假设，而N-of-1试验擅长个体因果验证。框架结合两者优势：大型基础模型生成带不确定性估计的干预候选排序，触发后续N-of-1试验。

Result: 论证大型基础模型无法替代N-of-1试验，但两者互补。混合框架能解决个性化医疗中的矛盾：隐私-性能悖论、规模-特异性悖论、自动化-同理心悖论，以及泛化性悖论。

Conclusion: 明确预测与因果的边界，解决矛盾对于负责任地将AI整合到个性化医疗中至关重要。大型基础模型和N-of-1试验的互补结合能实现真正的个性化医疗，同时应对现有挑战。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [156] [Evolving Programmatic Skill Networks](https://arxiv.org/abs/2601.03509)
*Haochen Shi,Xingdi Yuan,Bang Liu*

Main category: cs.AI

TL;DR: PSN框架使用符号程序作为技能，通过LLM实现故障定位、渐进优化和重构机制，在开放环境中实现持续技能学习


<details>
  <summary>Details</summary>
Motivation: 解决开放环境中智能体持续获取、细化和重用可执行技能的问题，构建可扩展的技能库

Method: 提出程序化技能网络（PSN），将技能表示为可执行符号程序，通过LLM实现REFLECT故障定位、渐进优化和结构重构三个核心机制

Result: 在MineDojo和Crafter环境中展示了强大的技能重用、快速适应能力和对开放任务分布的泛化能力

Conclusion: PSN框架能有效支持持续技能获取，其学习动态与神经网络训练有结构相似性，为开放环境中的智能体学习提供了新途径

Abstract: We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\footnote{We plan to open-source the code.

</details>


### [157] [Variance Computation for Weighted Model Counting with Knowledge Compilation Approach](https://arxiv.org/abs/2601.03523)
*Kengo Nakamura,Masaaki Nishino,Norihito Yasuda*

Main category: cs.AI

TL;DR: 本文研究了加权模型计数(WMC)的方差计算问题，开发了在结构化d-DNNF上的多项式时间算法，并证明了该问题在结构化DNNF、d-DNNF和FBDD上的计算困难性。


<details>
  <summary>Details</summary>
Motivation: 在实际推理任务中，模型参数通常从数据中学习而来，存在不确定性。为了评估推理结果的不确定性程度，需要将推理结果视为随机变量并计算其方差，但该问题的可计算性尚不清楚。

Method: 1. 为结构化d-DNNF推导多项式时间算法来计算WMC方差；2. 证明该问题在结构化DNNF、d-DNNF和FBDD上的计算困难性；3. 将方法应用于贝叶斯网络推理的不确定性测量。

Result: 1. 成功开发了在结构化d-DNNF上计算WMC方差的多项式时间算法；2. 证明了该问题在结构化DNNF、d-DNNF和FBDD上是困难的，这很有趣，因为后两者允许多项式时间的WMC算法；3. 在真实世界贝叶斯网络上实证评估了边际概率的方差，并分析了参数方差对边际方差的影响。

Conclusion: 本文首次系统研究了WMC方差计算问题，揭示了该问题在不同知识编译表示下的计算复杂性，为评估概率推理中的不确定性提供了理论基础和实用算法。

Abstract: One of the most important queries in knowledge compilation is weighted model counting (WMC), which has been applied to probabilistic inference on various models, such as Bayesian networks. In practical situations on inference tasks, the model's parameters have uncertainty because they are often learned from data, and thus we want to compute the degree of uncertainty in the inference outcome. One possible approach is to regard the inference outcome as a random variable by introducing distributions for the parameters and evaluate the variance of the outcome. Unfortunately, the tractability of computing such a variance is hardly known. Motivated by this, we consider the problem of computing the variance of WMC and investigate this problem's tractability. First, we derive a polynomial time algorithm to evaluate the WMC variance when the input is given as a structured d-DNNF. Second, we prove the hardness of this problem for structured DNNFs, d-DNNFs, and FBDDs, which is intriguing because the latter two allow polynomial time WMC algorithms. Finally, we show an application that measures the uncertainty in the inference of Bayesian networks. We empirically show that our algorithm can evaluate the variance of the marginal probability on real-world Bayesian networks and analyze the impact of the variances of parameters on the variance of the marginal.

</details>


### [158] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S框架通过自学习循环整合安全规则推理，有效防御LLM的越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有方法通过训练模型在响应前进行安全规则推理来提升安全性，但难以确定何种形式的安全推理能有效防御越狱攻击，且难以显式设计或直接获取

Method: 提出STAR-S框架，将安全规则推理学习整合到自学习循环中：在安全规则引导下引发推理和反思，然后通过微调增强安全推理能力，重复此过程形成协同循环

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法

Conclusion: STAR-S通过自学习循环整合安全规则推理，为LLM安全部署提供了有效的防御框架

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [159] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 提出神经符号框架评估LLM推理过程，发现扩展token生成并非深度推理必要条件，混合长短CoT训练会导致过早饱和，蒸馏到小模型无法复制逻辑效能。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法存在局限，无法区分性能提升是来自真正推理还是单纯增加输出长度，需要更全面的过程中心评估框架。

Method: 提出神经符号框架进行非侵入式、全面的过程中心推理评估，识别四种行为原型并诊断失败模式，分析推理模式、训练策略和模型规模的影响。

Result: 发现扩展token生成不是深度推理的必要条件；混合长短CoT数据训练会导致过早饱和和崩溃；蒸馏到小模型能复制行为长度但无法复制逻辑效能。

Conclusion: 需要更精细的推理评估方法，训练策略需要谨慎设计，模型容量限制蒸馏效果，为LLM推理能力评估和优化提供新见解。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [160] [SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models](https://arxiv.org/abs/2601.03555)
*Yuxuan Jiang,Francis Ferraro*

Main category: cs.AI

TL;DR: SCRIBE是一个强化学习框架，通过基于技能原型的中间层奖励建模，解决了工具增强代理中多步推理的信用分配问题，显著提升了推理和工具使用性能。


<details>
  <summary>Details</summary>
Motivation: 训练可靠的工具增强代理面临的主要挑战是多步推理中的信用分配困难。现有的基于LLM的奖励模型由于缺乏细粒度、任务特定的评估标准，往往产生噪声和不一致的信号，无法区分高层规划与低层执行。

Method: SCRIBE引入了一个新颖的中间层抽象，将奖励建模基于精心策划的技能原型库。通过将每个子目标路由到相应的原型，将开放式LLM评估转化为受约束的验证问题，为奖励模型提供精确的结构化评估标准。

Result: SCRIBE在多个推理和工具使用基准测试中达到最先进性能：将Qwen3-4B模型的AIME25准确率从43.3%提升到63.3%，并显著提高了复杂多轮工具交互的成功率。训练动态分析显示不同抽象层次间的协同演化。

Conclusion: SCRIBE为构建更自主可靠的工具使用代理提供了可扩展且互补的途径，能够与低层工具优化相结合，通过中间层技能掌握促进有效高层规划行为的出现。

Abstract: Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.

</details>


### [161] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: SAE-Steering方法通过稀疏自编码器分解推理策略纠缠的隐藏状态，并识别策略特定特征作为控制向量，显著提升大推理模型的策略控制效果和准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型自主选择推理策略时经常产生低效甚至错误的推理路径，现有方法难以控制细粒度推理策略，因为策略概念在隐藏状态中纠缠在一起。

Method: 使用稀疏自编码器将策略纠缠的隐藏状态分解为解耦特征空间，提出SAE-Steering两阶段特征识别流程：首先通过策略特定关键词的logits放大召回特征，过滤99%以上特征，然后按控制效果对剩余特征排序。

Result: SAE-Steering在控制效果上比现有方法提升超过15%，通过控制推理策略可以将模型从错误路径重定向到正确路径，实现7%的绝对准确率提升。

Conclusion: SAE-Steering方法能有效识别和控制大推理模型的推理策略，提高推理的可靠性和灵活性，为解决策略控制问题提供了有效方案。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [162] [Interleaved Tool-Call Reasoning for Protein Function Understanding](https://arxiv.org/abs/2601.03604)
*Chuanliu Fan,Zicheng Ma,Huanran Meng,Aijia Zhang,Wenjie Du,Jun Zhang,Yi Qin Gao,Ziqiang Cao,Guohong Fu*

Main category: cs.AI

TL;DR: PFUA是一个工具增强的蛋白质推理代理，通过整合领域特定工具而非纯文本推理，在蛋白质功能预测任务上比纯文本推理模型平均提升103%性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现直接将文本推理范式（如思维链）迁移到蛋白质功能理解任务上是无效的，因为强化学习主要放大表面关键词模式而无法引入新的生物学知识，导致泛化能力有限。蛋白质功能预测是一个知识密集的科学任务，本质上依赖于外部生物学先验和计算工具而非纯内部推理。

Method: 提出PFUA（工具增强的蛋白质推理代理），统一问题分解、工具调用和基于证据的答案生成。PFUA不依赖长无约束的推理轨迹，而是整合领域特定工具来产生可验证的中间证据。

Result: 在四个基准测试上的实验表明，PFUA始终优于纯文本推理模型，平均性能提升103%。

Conclusion: 蛋白质功能预测需要整合领域工具而非纯文本推理，PFUA通过工具增强方法有效解决了这一挑战，为知识密集型科学任务提供了新的解决方案。

Abstract: Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.

</details>


### [163] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: 该论文提出了一种基于企业分布式系统标准、形式化方法和行业实践的AI智能体系统架构设计模式，重点关注智能体社区层级的协调框架，通过形式化验证确保企业级部署的可信治理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和智能体AI技术的快速发展，需要系统化的架构指导来构建复杂的生产级系统。当前缺乏结合实践指导和形式化验证的架构框架，特别是针对企业级多智能体生态系统的协调治理需求。

Method: 提出三层设计模式分类：LLM智能体（任务自动化）、智能体AI（自适应目标寻求者）、智能体社区（组织框架）。重点研究智能体社区，借鉴分布式系统协调原则，建立形式化框架，通过协作协议、角色分配和治理结构来协调AI智能体与人类参与者。

Result: 开发了一个形式化框架，能够表达组织、法律和伦理规则，通过问责机制确保智能体间通信、协商和意图建模的可操作和可验证治理。通过临床试验匹配案例研究验证了该框架的有效性。

Conclusion: 该框架为从业者提供了可操作的指导，同时保持了企业级部署所需的严格形式化严谨性，特别适用于动态多智能体生态系统中的可信治理需求。

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [164] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind：一种通过在推理步骤中动态注入安全提醒短语来增强大型推理模型安全性的解码时防御方法


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式推理步骤取得显著成功，但这些推理步骤可能放大不安全行为，而传统防御机制未能考虑LRMs独特的推理动态。研究发现推理步骤中出现安全提醒短语对确保LRM安全至关重要。

Method: 提出SafeRemind方法，在解码时动态向推理步骤注入安全提醒短语。利用熵触发器在决策锁定点进行干预，将潜在有害轨迹重定向到更安全的结果，无需参数更新。

Result: 在5个LRM和6个基准测试上的广泛评估表明，SafeRemind显著增强安全性，提升幅度高达45.5个百分点，同时保持核心推理效用。

Conclusion: SafeRemind通过动态注入安全提醒短语，有效解决了大型推理模型推理步骤中的安全漏洞问题，在保持模型推理能力的同时显著提升安全性。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [165] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR提出了一种新的推理范式，通过"答案-推理-答案"三明治结构，结合一致性强化学习和边界样本采样，在保持CoT推理精度的同时大幅降低延迟，解决了查询纠错中延迟与精度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 查询纠错在现代搜索管道中至关重要，需要在实时延迟约束下保持高精度。传统CoT推理虽然能提高精度，但延迟过高无法满足实时需求。提前输出答案虽然能降低延迟，但答案与后续推理无关，无法利用推理能力提升精度。

Method: 提出Sandwich Reasoning方法，采用"答案-推理-答案"三明治范式：先生成初始纠错，然后进行显式推理过程，最后生成最终精炼纠错。设计了基于一致性的强化学习策略，包括一致性奖励机制和基于边界的拒绝采样，确保初始答案与后推理洞察对齐。同时构建了高质量的查询纠错数据集。

Result: 实验结果表明，SandwichR在保持与标准CoT相当的最先进精度的同时，实现了40-70%的延迟降低，成功解决了在线搜索中的延迟-精度权衡问题。

Conclusion: SandwichR通过创新的三明治推理范式，结合一致性强化学习策略，在查询纠错任务中实现了延迟与精度的双赢，为实时搜索系统中的查询纠错提供了有效的解决方案。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [166] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 使用LLM生成特定问题启发式函数，结合GBFS搜索算法，将个性化用药规划的药物数量从7种扩展到至少28种，显著提升覆盖率和规划时间。


<details>
  <summary>Details</summary>
Motivation: 个性化用药规划需要为每位患者选择药物并确定给药方案。先前基于通用领域无关启发式的方法最多只能处理7种药物，这在临床实践中远远不够，需要扩展到更实用的规模。

Method: 通过编程方式指定领域（定义初始状态和状态转移过程），使用LLM生成针对特定问题的启发式函数，然后结合GBFS（贪心最佳优先搜索）算法进行规划。

Result: 方法显著提升了覆盖率和规划时间，能够处理至少28种药物的规划问题，使个性化用药规划更接近实际临床应用。

Conclusion: 自动生成的领域和问题特定启发式函数能够有效扩展个性化用药规划的规模，为临床实践应用迈出了重要一步。

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [167] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT：通过熵基分割和蒙特卡洛评估自动识别和优化低质量CoT监督数据，构建高质量数学推理数据集


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集存在"答案正确但推理错误"问题，即最终答案正确但中间步骤存在幻觉、冗余或逻辑错误，这影响了CoT推理的质量

Method: 1) 基于熵的机制在不确定节点分割推理轨迹；2) 蒙特卡洛rollout机制评估每个步骤的边际贡献；3) 准确过滤欺骗性推理样本，构建高质量数据集

Result: 在数学基准测试上，使用EntroCoT构建的子集进行微调始终优于全数据集监督的基线方法

Conclusion: EntroCoT能有效识别和优化低质量CoT监督数据，提升大语言模型的数学推理能力，为构建高质量推理数据集提供了统一框架

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [168] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: ROI-Reasoning：一种两阶段框架，通过元认知微调和理性感知强化学习，让LLM在严格令牌预算下进行推理时计算资源分配


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的推理能力，但无法自动判断不同任务所需的计算量。在严格的全局令牌约束下，需要让模型学会预测任务难度、估计投资回报率，并战略性地分配计算资源。

Method: 提出ROI-Reasoning两阶段框架：1) 元认知微调：训练模型在生成前预测推理成本和预期效用，做出明确的"解决或跳过"决策；2) 理性感知强化学习：在硬令牌预算下优化序列决策，学习长期分配策略。

Result: 在预算约束的数学推理基准测试中，ROI-Reasoning在严格计算预算下持续提高总体得分，同时显著减少遗憾值。

Conclusion: 将推理时间计算分配形式化为有序随机多选择背包问题，通过赋予LLM内在的预算感知理性，能够战略性地分配计算资源，在有限计算预算下实现更好的整体性能。

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [169] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算理性闭包（RC）的声明式方法，实现了从知识库自动构建最小排序模型并支持查询推理，相比现有命令式实现具有更好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 可废止推理关注从不完整信息中得出合理结论，KLM框架是其基础理论框架，理性闭包（RC）是该框架中最突出的算法之一。现有实现多为命令式方法，本文旨在提供一种声明式的ASP方法来实现RC。

Method: 使用答案集编程（ASP）为理性闭包（RC）提供声明式定义，能够从给定知识库自动构建最小排序模型，并支持对指定查询进行推理检查。通过形式化证明验证ASP编码的正确性。

Result: 实验评估表明，ASP实现遵循RC的理论基础，并与现有命令式实现（特别是InfOCF求解器）相比，提供了改进的计算效率。

Conclusion: ASP为理性闭包提供了一种有效的声明式实现方法，不仅保持理论正确性，而且在计算效率上优于现有命令式实现，为可废止推理的实际应用提供了新工具。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [170] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用ASP建模意大利刑法典，通过半自动学习从司法判例中推导法律规则，支持刑事审判推理和决策解释


<details>
  <summary>Details</summary>
Motivation: 为法律专家在刑事审判阶段提供推理支持和可能的法律结果，提高决策过程的透明度和可解释性

Method: 使用答案集编程(ASP)对意大利刑法典条款进行编码建模，包括"人身犯罪"和财产犯罪；利用稳定模型的"支持性"提供解释；集成归纳逻辑编程系统从案例中泛化法律规则

Result: 开发出能够处理法律矛盾、生成新案件决策并提供解释的工具；通过先前判决验证模型并不断优化；实现法律规则的半自动学习

Conclusion: ASP方法能有效建模刑法典并支持法律推理，自动解释功能增强了司法决策的可理解性，工具对法律实践有实际应用价值

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [171] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程（ASP）的方法，用于生成决策树模型的各种解释类型，包括充分解释、对比解释、多数解释和树特定解释，相比SAT方法更具灵活性并支持枚举所有可能解释。


<details>
  <summary>Details</summary>
Motivation: 决策树模型（如随机森林和梯度提升决策树）在机器学习中广泛使用，但其复杂结构使得模型难以解释，特别是在需要正式论证的安全关键应用中。现有的逻辑和溯因解释方法需要更灵活的解释生成能力。

Method: 使用答案集编程（ASP）来生成多种类型的解释：充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更具灵活性，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行了实证评估，证明了该方法的有效性，并与现有方法进行了比较，展示了其优势和局限性。

Conclusion: 基于ASP的方法为决策树模型解释提供了更灵活和全面的解决方案，能够生成多种类型的解释并支持用户偏好编码，为安全关键应用中的模型决策提供了更好的形式化论证支持。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [172] [xDNN(ASP): Explanation Generation System for Deep Neural Networks powered by Answer Set Programming](https://arxiv.org/abs/2601.03847)
*Ly Ly Trieu,Tran Cao Son*

Main category: cs.AI

TL;DR: xDNN(ASP)是一个基于答案集语义的深度神经网络全局解释系统，通过提取逻辑程序来理解模型内部结构，而不仅仅是输入输出关系。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法主要关注输入输出关系，忽略了神经网络内部结构在解释生成中的作用，需要一种能提供全局解释并考虑网络结构的方法。

Method: 提出xDNN(ASP)系统，给定神经网络模型和训练数据，提取基于答案集语义的逻辑程序，理想情况下该程序与训练模型一一对应。

Result: 在两个合成数据集上的实验表明，提取的逻辑程序不仅保持了高预测准确率，还提供了特征重要性和隐藏节点对预测影响等有价值信息。

Conclusion: xDNN(ASP)能有效生成全局解释，帮助理解模型内部机制，并为网络优化（如减少隐藏节点）提供指导。

Abstract: Explainable artificial intelligence (xAI) has gained significant attention in recent years. Among other things, explainablility for deep neural networks has been a topic of intensive research due to the meteoric rise in prominence of deep neural networks and their "black-box" nature. xAI approaches can be characterized along different dimensions such as their scope (global versus local explanations) or underlying methodologies (statistic-based versus rule-based strategies). Methods generating global explanations aim to provide reasoning process applicable to all possible output classes while local explanation methods focus only on a single, specific class. SHAP (SHapley Additive exPlanations), a well-known statistical technique, identifies important features of a network. Deep neural network rule extraction method constructs IF-THEN rules that link input conditions to a class. Another approach focuses on generating counterfactuals which help explain how small changes to an input can affect the model's predictions. However, these techniques primarily focus on the input-output relationship and thus neglect the structure of the network in explanation generation.   In this work, we propose xDNN(ASP), an explanation generation system for deep neural networks that provides global explanations. Given a neural network model and its training data, xDNN(ASP) extracts a logic program under answer set semantics that-in the ideal case-represents the trained model, i.e., answer sets of the extracted program correspond one-to-one to input-output pairs of the network. We demonstrate experimentally, using two synthetic datasets, that not only the extracted logic program maintains a high-level of accuracy in the prediction task, but it also provides valuable information for the understanding of the model such as the importance of features as well as the impact of hidden nodes on the prediction. The latter can be used as a guide for reducing the number of nodes used in hidden layers, i.e., providing a means for optimizing the network.

</details>


### [173] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: ASP技术在大规模配置问题中面临内存瓶颈，研究者通过增量求解和约束感知猜测方法显著降低内存需求


<details>
  <summary>Details</summary>
Motivation: 研究当前ASP技术在大规模配置问题（如包含3万多个组件的电子系统配置）中的可扩展性，特别是解决内存急剧增长的"基础化瓶颈"问题

Method: 采用增量求解方法，并开发了约束感知猜测技术，通过分析基础化过程来减少内存需求

Result: 增量求解在实践中有效，但仍有内存限制；约束感知猜测方法显著降低了内存需求

Conclusion: ASP技术在大规模配置问题中具有潜力，但需要继续优化内存管理方法以突破现有限制

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [174] [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905)
*Cheng Qian,Emre Can Acikgoz,Bingxuan Li,Xiusi Chen,Yuji Zhang,Bingxiang He,Qinyu Luo,Dilek Hakkani-Tür,Gokhan Tur,Yunzhu Li,Heng Ji,Heng Ji*

Main category: cs.AI

TL;DR: 当前基于视觉语言模型的智能体在利用生成式世界模型进行未来状态预测时存在严重问题：很少调用模拟、经常误用预测结果、甚至出现性能下降


<details>
  <summary>Details</summary>
Motivation: 智能体面临需要预测未来状态的任务，而不仅仅是短期推理。生成式世界模型作为外部模拟器理论上可以帮助智能体预见结果后再行动，但实际效果需要实证检验

Method: 通过多样化的智能体和视觉问答任务，实证研究当前智能体是否能有效利用世界模型作为工具来增强认知能力

Result: 发现智能体很少调用模拟（少于1%），经常误用预测结果（约15%），当模拟可用或被强制使用时，性能甚至下降（最多5%）。归因分析显示主要瓶颈在于智能体决定何时模拟、如何解释预测结果以及如何将预见整合到下游推理的能力

Conclusion: 需要开发能够促进与世界模型进行校准、战略性交互的机制，为未来智能体系统实现更可靠的预期性认知铺平道路

Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.

</details>


### [175] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1：通过过程级推理验证将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题，使用结构化RAG任务评估推理，构建三角一致性指标过滤噪声市场回报。


<details>
  <summary>Details</summary>
Motivation: 强化学习在数学和编程等可验证奖励领域表现出色，但在金融决策中面临挑战：市场具有随机性，奖励可验证但噪声大，导致标准RL退化为奖励黑客。需要将可验证奖励与随机环境连接的新方法。

Method: 提出Trade-R1训练框架，通过过程级推理验证连接可验证奖励与随机环境。核心创新是将长金融文档推理评估转化为结构化RAG任务，构建检索证据、推理链和决策之间的三角一致性指标作为噪声市场回报的有效性过滤器。探索两种奖励整合策略：固定效应语义奖励（FSR）和动态效应语义奖励（DSR）。

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客问题，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性。

Conclusion: Trade-R1通过过程级推理验证有效解决了金融决策中RL的奖励黑客问题，将可验证奖励与随机环境连接，为金融领域的RL应用提供了新范式。

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [176] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 提出DOT方法解决大模型推理中的过度思考问题，通过动态截断冗余token减少78%推理token使用，同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的大推理模型在简单查询上存在过度冗长问题，导致部署成本高昂。现有基于显式长度惩罚的方法存在优化冲突，且未深入探究过度思考的生成机制。

Method: 提出动态异常截断(DOT)：训练时选择性抑制冗余token，仅针对完全正确rollout组中的极端长度尾部；结合辅助KL正则化和预测动态采样确保稳定收敛

Result: 在多个模型规模上显著扩展了效率-性能帕累托前沿。在AIME-24上，相比初始策略减少78%推理token使用同时提高准确率，超越现有高效推理方法

Conclusion: DOT方法有效解决了大模型推理中的长度偏移现象，通过训练时干预减少冗余推理，在保持复杂问题长程推理能力的同时大幅提升推理效率

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [177] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：基于世界模型的移动GUI代理前瞻框架，通过文本草图世界模型预测动作后状态，提升长时程任务性能


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理多为反应式，仅基于当前屏幕做决策，限制了在长时程任务中的表现。构建世界模型能预测动作结果，支持更好的决策制定。

Method: 提出MobileDreamer框架：1) 文本草图世界模型，将数字图像转换为关键任务相关草图，采用顺序不变学习策略保留GUI元素空间信息；2) 滚动想象策略，利用世界模型预测能力优化动作选择过程。

Result: 在Android World上的实验表明，MobileDreamer达到最先进性能，任务成功率提升5.25%。世界模型评估验证了文本草图建模能准确预测关键GUI元素。

Conclusion: MobileDreamer通过高效的世界模型前瞻框架，显著提升了移动GUI代理在长时程任务中的性能，为实际部署提供了有效解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [178] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch是一个基于代理的框架，用于在ComfyUI平台上探索组件空间并生成功能性的工作流，通过验证引导的工作流构建来解决现有方法在复杂创意任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的AI生成内容已从单一模型发展为模块化工作流，但大量组件和严格图约束下保持长期结构一致性困难，导致通过率低和工作流质量有限。

Method: 提出ComfySearch框架，采用代理机制探索组件空间，通过验证引导的工作流构建方法生成功能性ComfyUI管道。

Result: 实验表明ComfySearch在复杂创意任务上显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决方案率和更强的泛化能力。

Conclusion: ComfySearch通过验证引导的工作流构建有效解决了ComfyUI平台中组件空间探索和工作流生成的挑战，为复杂创意任务提供了更可靠的解决方案。

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [179] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 该研究提出了"智能体漂移"概念，即多智能体LLM系统在长期交互中行为、决策质量和协作一致性的渐进性退化，并开发了量化框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作问题解决中表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互中可能出现的性能退化现象，以确保生产系统的可靠部署。

Method: 提出了智能体漂移的理论框架，包括语义漂移、协调漂移和行为漂移三种表现形式。开发了智能体稳定性指数（ASI）作为复合度量框架，包含12个维度（如响应一致性、工具使用模式、推理路径稳定性等）。通过模拟分析和理论建模研究漂移现象，并提出三种缓解策略：情景记忆巩固、漂移感知路由协议和自适应行为锚定。

Result: 研究表明，未受控制的智能体漂移会导致任务完成准确率显著降低，并增加人工干预需求。理论分析表明，提出的缓解策略能够显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 该工作建立了监测、量化和缓解生产性智能体AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义，为多智能体系统的长期稳定性提供了理论框架和实践指导。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [180] [CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature](https://arxiv.org/abs/2601.03319)
*Eldad Matmon,Amit Bracha,Noam Rotstein,Ron Kimmel*

Main category: cs.GR

TL;DR: 提出一个基于3D高斯泼溅的逼真可控3D人脸漫画化框架，通过曲率加权表面夸张和交替训练策略实现高质量实时渲染


<details>
  <summary>Details</summary>
Motivation: 现有基于内在高斯曲率的表面夸张方法结合纹理后会产生过度平滑的渲染效果，需要更逼真的3D漫画化解决方案

Method: 1) 从多视角序列提取FLAME网格，求解曲率加权泊松方程获得夸张形式；2) 通过局部仿射变换合成伪地面真实漫画图像；3) 设计交替训练方案，结合真实和合成监督；4) 引入原始和夸张表面之间的高效插值实现实时变形

Result: 在定量和定性评估中均优于先前工作，能够生成逼真、几何可控的漫画头像，支持局部编辑和连续强度控制

Conclusion: 提出的框架成功实现了高质量、实时可控的3D人脸漫画化，通过创新的训练策略和变形方法解决了现有技术的局限性

Abstract: A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.

</details>
