<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.21581)
*Alex Ning,Yen-Ling Kuo,Gabe Gomes*

Main category: cs.LG

TL;DR: 提出自适应长度潜在推理模型，通过强化学习优化推理长度，在保持准确性的同时将总推理长度降低52%


<details>
  <summary>Details</summary>
Motivation: 潜在推理相比链式思维推理具有压缩推理长度的潜力，通过直接传递信息丰富的潜在状态来突破人类语言标记作为推理媒介的限制

Method: 开发自适应长度潜在推理模型，引入后SFT强化学习方法，通过最小化推理长度同时保持准确性来优化潜在推理长度

Result: 在Llama 3.2 1B模型和GSM8K-Aug数据集上的实验显示，总推理长度下降52%且准确率没有损失

Conclusion: 潜在推理模型在计算使用和压缩能力方面提升了标准，未来计划扩展到更多模型和数据集，分析训练系数关系，实验架构变体，并继续知识蒸馏工作

Abstract: Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.

</details>


### [2] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过降维技术提取和可视化Transformer语言模型的潜在状态几何结构，发现注意力机制和MLP组件在中间层存在明显分离，并揭示了位置嵌入的高维螺旋结构等几何模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多自然语言任务中取得最先进成果，但其内部机制仍难以解释，需要开发系统性的可解释性分析方法来理解Transformer模型的内部工作方式。

Method: 使用主成分分析（PCA）和均匀流形近似（UMAP）等降维技术，在GPT-2和LLaMa模型的Transformer块中捕获层间激活状态，进行系统性的几何结构分析。

Result: 发现注意力机制和MLP组件输出在中间层存在明显分离，识别了初始序列位置潜在状态的高范数特征，可视化了位置嵌入的高维螺旋结构以及序列层面的几何模式。

Conclusion: 这项工作为Transformer内部机制的系统分析提供了支持，有助于推进可复现的可解释性研究，代码已开源供进一步研究使用。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>
