<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 184]
- [cs.GR](#cs.GR) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 提出了一个基于运动捕捉数据的交互式AI模型，通过部分模仿和创造性增强人类动作序列来生成人工舞伴，是首个利用单人运动数据和高层特征实现此功能的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然支持各种创意任务，但缺乏具身交互特性。舞蹈作为人类表达的基本形式，可以补充这种体验，探索创意性的人机交互。

Method: 结合两个扩散模型、运动修复和运动风格转换的思想，利用单人运动数据和高层特征生成时间连贯且对选定运动参考有响应的运动表示。

Result: 通过定量评估生成样本特征分布与测试集的收敛性来证明模型成功，生成的动作既多样化又真实，展现了与人类舞伴的各种偏离。

Conclusion: 该模型是实现与AI创意舞蹈共舞的第一步，生成的舞蹈动作既多样化又真实，为具身人机交互提供了新途径。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 提出基于机器学习的珊瑚白化分类系统，使用CNN模型在多样化的全球数据集上达到88%的准确率，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和海水温度异常等日益严重的威胁，需要高效的保护和监测方法。

Method: 使用包含不同环境条件下健康和漂白珊瑚样本的全球数据集，比较了ResNet、ViT和CNN三种最先进的模型，并进行全面的超参数调优。

Result: CNN模型实现了最高的88%准确率，超越了现有基准。

Conclusion: 研究为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 评估基于YOLOv8的深度学习管道，用于从肯尼亚和坦桑尼亚的视频样带中自动识别鱼类科级分类，为西印度洋珊瑚礁鱼类监测提供首个区域特定基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁监测受到水下视觉普查劳动需求的限制，需要自动化解决方案来提高监测效率。

Method: 使用YOLOv8深度学习模型构建自动化鱼类识别管道，在肯尼亚和坦桑尼亚收集的视频样带数据上测试24个鱼类科的识别性能。

Result: 最佳模型达到mAP@0.5为0.52，对丰富鱼类科识别准确率高，但对稀有或复杂分类群的检测效果较弱。

Conclusion: 深度学习有潜力作为传统监测方法的可扩展补充工具。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的场景补丁作为正样本，用于对比学习中的表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的InfoNCE损失方法虽然能减少人工标注需求，但数据选择和增强仍依赖人工假设或工程，可能不是最优的。特别是对比学习中的数据增强主要关注颜色抖动，旨在模拟真实世界的光照变化。

Method: 基于真实世界分布计算训练数据的互信息，选择在自然扰动（如颜色变化和运动）下具有高互信息的场景补丁作为正样本，用于对比损失学习。

Result: 在多个基准测试和最先进的表示学习框架上评估了所提出的互信息数据增强方法，证明了其有效性。

Conclusion: 基于互信息的数据增强方法是一个有前景的研究方向，能够使学习到的特征在开放环境中具有更好的泛化能力。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本文对三种联邦学习框架（NVIDIA FLARE、Flower和Owkin Substra）在医学影像应用中的性能进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI中具有重要价值，但需要评估不同框架在实际医疗环境中的适用性，为医疗机构选择合适的联邦学习解决方案提供指导。

Method: 使用PathMNIST数据集，对三种联邦学习框架进行系统性基准测试，评估模型性能、收敛效率、通信开销、可扩展性和开发者体验等多个维度。

Result: NVIDIA FLARE在生产可扩展性方面表现最佳，Flower在原型设计和学术研究方面具有灵活性，Owkin Substra在隐私保护和合规性方面表现突出。

Conclusion: 三种框架各有优势，适用于不同的使用场景，强调它们在医疗环境实际部署中的相关性和适用性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对水稻叶片图像进行去噪和对比度增强的对比研究，结合多种去噪方法与CLAHE技术，为农业图像处理提供有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 水稻叶片分析在疾病检测、营养评估和生长分析中至关重要，图像增强作为预处理步骤能提高后续任务的可靠性，但需要有效的去噪和对比度增强方法。

Method: 采用多种知名图像去噪方法结合CLAHE（对比度受限自适应直方图均衡化）技术，在水稻叶片图像数据集上进行实验，使用多种指标全面评估增强效果。

Result: 通过综合指标测试，验证了不同去噪方法结合CLAHE在水稻叶片图像处理中的有效性，为方法评估提供了坚实基础。

Conclusion: 该研究为数字图像处理方法的效果评估提供了可靠基础，其研究成果对农业研究和其他领域的未来应用具有重要参考价值。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [7] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 该研究系统分析了不同LiDAR扫描模式（重复式与非重复式）对路边感知性能的影响，发现非重复式LiDAR虽然感知范围有限，但成本效益高，在检测性能上与128线重复式LiDAR相当。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LiDAR在基础设施中的最优放置，但不同扫描模式对感知性能的深远影响尚未得到充分研究。重复式与非重复式扫描系统会产生不同的点云分布，这对物体检测和环境理解至关重要。

Method: 在CARLA仿真环境中创建了"InfraLiDARs' Benchmark"数据集，使用同时运行的基于基础设施的LiDAR，涵盖两种扫描范式。进行全面的统计分析，并评估不同扫描模式对各种领先3D物体检测算法性能的影响。

Result: 研究发现非重复式扫描LiDAR和128线重复式LiDAR在各种场景下表现出相当的检测性能。尽管非重复式LiDAR感知范围有限，但考虑到其较低价格，是一个成本效益高的选择。

Conclusion: 本研究为设置具有最优LiDAR扫描模式和兼容算法的路边感知系统提供了见解，并公开发布"InfraLiDARs' Benchmark"数据集以促进进一步研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [8] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5是新一代物理AI世界基础模型，基于流式架构统一了文本、图像、视频到世界的生成，结合Cosmos-Reason1提供更丰富的文本基础和精细的世界模拟控制。


<details>
  <summary>Details</summary>
Motivation: 开发更可靠的世界模拟工具，用于机器人学和自主系统的合成数据生成、策略评估和闭环模拟，推动具身智能的发展。

Method: 采用流式架构，结合物理AI视觉语言模型Cosmos-Reason1，在2亿个精选视频片段上训练，并使用基于强化学习的后训练进行优化。

Result: 相比Cosmos-Predict1在视频质量和指令对齐方面有显著提升，发布了20亿和140亿参数规模的模型。Cosmos-Transfer2.5虽然比前代小3.5倍，但提供了更高的保真度和鲁棒的长序列视频生成。

Conclusion: 这些进展使Cosmos-Predict2.5和Cosmos-Transfer2.5成为扩展具身智能的多功能工具，通过开源代码、预训练检查点和基准测试加速物理AI的研究和部署。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [9] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出MSDI框架，通过3D高斯表示和运动扩散先验生成4D人体运动，结合LLM语义信息和运动扩散分数蒸馏采样，实现零样本的物体感知人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的视频存在不真实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。

Method: 使用预生成的3D人体和物体，结合LLM的空间和语义信息，通过提出的运动扩散分数蒸馏采样(MSDS)进行空间感知运动优化，从预训练运动扩散模型蒸馏分数梯度来优化人体运动。

Result: 实验表明该框架能生成自然且物理合理的人体运动，尊重3D空间上下文，为真实4D生成提供可扩展解决方案。

Conclusion: 提出的零样本方法避免了重新训练，能泛化到分布外的物体感知人体运动，解决了现有方法的局限性。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [10] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 使用深度学习进行高山栖息地变化检测，比较了后分类变化检测和直接变化检测两种方法，发现Clay v1.0模型在复杂自然环境中表现最佳，准确率达51%，LiDAR数据集成显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统面临快速气候变化，需要频繁的栖息地监测，但人工测绘成本过高。研究旨在填补地理空间基础模型在复杂自然环境（具有模糊类别边界和高度不平衡类别）中应用的空白。

Method: 比较两种变化检测范式：后分类变化检测（使用Prithvi-EO-2.0、Clay v1.0和U-Net CNN）和直接变化检测（使用ChangeViT和U-Net基线）。使用高分辨率多模态数据（RGB、近红外、LiDAR、地形属性），覆盖15.3平方公里内的4,480个记录变化。

Result: Clay v1.0在多类栖息地变化检测中达到51%总体准确率，U-Net为41%；二元变化检测两者均达67%。直接变化检测在二元检测中IoU更优（0.53 vs 0.35），但多类检测准确率仅28%。跨时间评估显示GFM鲁棒性更好，Clay在2020年数据上保持33%准确率。集成LiDAR将语义分割准确率从30%提升至50%。

Conclusion: 虽然总体准确率低于更均质景观，但反映了复杂高山栖息地的实际性能。未来工作将集成基于对象的后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [11] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D是一个免费生成式AI框架，利用Google街景图像重建孟加拉国文化遗产的3D模型，解决了传统3D数字化方法成本高、技术要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源有限和技术专家稀缺的双重挑战，传统3D数字化方法需要昂贵硬件、专家操作和大量现场访问，这在发展中国家往往不可行。

Method: 采用两阶段流程：使用Gemini 2.5 Flash Image进行多模态视觉推理实现结构-纹理合成，通过Hexagen进行神经图像到3D生成实现几何恢复。

Result: 系统在几秒钟内生成照片级真实感、度量一致的重建结果，相比传统运动结构恢复流程显著提速，且无需专门硬件或专家监督。在Ahsan Manzil、Choto Sona清真寺和Paharpur等地标上的实验表明，Oitijjo-3D保持了视觉和结构保真度。

Conclusion: 通过将开放图像转化为数字遗产，这项工作将保护重新定义为资源有限国家社区驱动、AI辅助的文化延续行为。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [12] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一个无需训练的高效扩散视频生成加速框架，通过词典最小最大路径优化策略显著提升生成视频的全局一致性和质量，同时实现2.9倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有的缓存策略主要关注减少局部启发式误差，但忽略了全局误差的累积，导致加速视频与原始视频之间存在明显的内容退化。

Method: 将缓存调度建模为带误差权重边的有向图，引入词典最小最大路径优化策略来显式限制最坏情况路径误差。

Result: 在多个文本到视频基准测试中，LeMiCa在推理速度和生成质量上均实现双重提升，在Latte模型上实现2.9倍加速，在Open-Sora上达到0.05的LPIPS分数。

Conclusion: LeMiCa以最小的感知质量损失实现了显著的性能提升，为加速扩散视频生成提供了一个鲁棒且可泛化的范式。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [13] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo是一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位，通过建模不同类型的辩论关系来提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法受限于数据库覆盖范围和质量，而现有的大视觉语言模型在复杂地理场景中表现不佳。多智能体系统虽然通过协作提升性能，但缺乏处理冲突预测的有效机制。

Method: 使用异构图神经网络建模多样化的辩论关系，包括支持性协作、竞争性论证和知识转移。引入双级辩论机制，结合节点级精炼和边级论证建模，以及跨级拓扑精炼策略。

Result: 在多个基准测试中，GraphGeo显著优于最先进的方法，将智能体间的认知冲突转化为更高的地理定位精度。

Conclusion: GraphGeo通过结构化辩论框架有效提升了视觉地理定位的准确性，证明了异构图神经网络在多智能体协作中的价值。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [14] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD是一个三阶段即插即用框架，通过残差强化学习和分布感知数据收集来改进视觉语言动作模型，在多个任务上实现接近饱和的成功率。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖昂贵的人工演示，限制了视觉语言动作模型的可扩展性和泛化能力，需要更高效的改进方法。

Method: 三阶段框架：1)训练轻量级残差actor探测失败区域；2)混合rollout方案对齐部署分布并捕获恢复行为；3)通过标准SFT将精选轨迹蒸馏回通用模型。

Result: 在LIBERO上达到99%任务成功率，在SimplerEnv上提升超过50%，在真实世界Franka和YAM机械臂任务上实现100%成功率。

Conclusion: 残差探测和分布感知回放是收集部署对齐数据的关键，为自改进视觉语言动作模型提供了可扩展路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [15] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文分析了自动语音驱动3D手势生成领域的人类评估实践，发现缺乏标准化和存在实验设计缺陷。作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估。


<details>
  <summary>Details</summary>
Motivation: 当前手势生成领域缺乏标准化的评估方法，实验设计存在缺陷，导致无法准确比较不同方法的性能，也不清楚该领域的最新技术水平。

Method: 提出了详细的人类评估协议，在BEAT2数据集上进行大规模众包评估，从运动真实性和语音-手势对齐两个关键维度对6个最新手势生成模型进行排名。

Result: 评估结果显示：1）新模型并不总是优于早期方法；2）已发表的高运动真实性或语音-手势对齐声明在严格评估下可能不成立；3）需要对运动质量和多模态对齐进行分离评估才能准确基准测试。

Conclusion: 手势生成领域必须采用分离的运动质量和多模态对齐评估方法，以实现准确的基准测试和进步。作者将发布合成运动数据、渲染视频刺激、开源渲染脚本和16,000对偏好投票以推动标准化。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [16] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM-R1，一个融合微调SAM和DeepSeek-R1的多模态视觉语言交互系统，用于脊柱CT图像分割，解决了传统方法在低对比度和复杂边界下的分割难题。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割面临低对比度和复杂边界的挑战，现有先进模型如SAM在脊柱CT成像中受限于高标注要求和差的领域适应性。

Method: 引入解剖学引导的注意力机制提升脊柱分割性能，采用语义驱动的交互协议支持自然语言引导的细化，使用LoRA进行高效微调适配。

Result: 在脊柱解剖结构CT图像上验证，获得优越的分割性能，开发了基于PyQt5的交互软件，支持点、框和文本提示，实现94.3%的解析准确率和低于800ms的响应时间。

Conclusion: SpinalSAM-R1有效解决了脊柱CT图像分割的挑战，提供了高效的多模态交互解决方案，软件已开源发布。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [17] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出HMVLM框架，通过MoE LoRA策略和零专家机制解决3D人体运动与文本模态融合中的灾难性遗忘问题，采用身体部位特定标记化提升姿态表示质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D人体运动与文本模态融合中的灾难性遗忘问题，以及开发跨异构下游任务保持泛化能力的自回归兼容姿态表示。

Method: 基于MoE LoRA策略的统一框架，使用门控网络动态分配LoRA专家权重，引入零专家保护预训练参数，采用身体部位特定标记化进行姿态表示。

Result: 实验表明该方法有效缓解指令调优中的知识遗忘，在多样化人体运动下游任务中取得显著性能。

Conclusion: HMVLM框架成功解决了模态融合中的关键挑战，为多模态人体运动理解与生成提供了有效解决方案。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [18] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 提出了一种用于共聚焦激光内窥镜(CLE)视频序列的过滤方法，通过减少SSL训练中的数据集冗余来提高训练效率和收敛性，在两种肿瘤数据集上显著优于非SSL基线。


<details>
  <summary>Details</summary>
Motivation: CLE图像对非专业医生难以解读，机器学习可辅助诊断但面临数据不足导致的过拟合问题。SSL可利用大量未标记数据，但CLE视频帧间相关性高导致数据分布不均衡。

Method: 使用四种先进基线网络和基于视觉transformer的SSL师生网络，在CLE视频序列上应用过滤功能减少冗余，评估在两个肿瘤数据集上的下游任务表现。

Result: 在两种数据集上，过滤后的SSL预训练模型获得最高测试准确率(67.48%和73.52%)，显著优于非SSL基线，训练时间减少67%。

Conclusion: SSL是CLE预训练的有效方法，提出的CLE视频过滤器可提高自监督场景下的训练效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [19] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出一个完整的纹理控制编辑系统，能够生成具有局部特征（如污渍、撕裂、孔洞等）的逼真纹理，采用基于学习的无监督方法，无需手动标注。


<details>
  <summary>Details</summary>
Motivation: 自然界中材料表面普遍存在各种瑕疵和变化，将这些特征纳入纹理合成过程对于生成逼真纹理至关重要。

Method: 采用基于学习的方法，利用无标签样本，通过无监督异常检测识别外观改变特征，自动聚类为语义连贯的组，用于指导条件图像生成。

Result: 开发了一个从少量图像到多功能生成模型的完整流程，支持用户交互式创建和绘制任意尺寸纹理的特征。

Conclusion: 该系统实现了可控的纹理创作和编辑，提出的基于扩散的编辑和无限静态纹理生成算法具有通用性，可应用于其他场景。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [20] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种无需训练、模态无关的方法，通过部分估计概念滑块公式在推理过程中实现细粒度可控生成，支持图像、视频和音频等多种模态的概念控制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像、音频和视频生成方面已成为最先进技术，但实现细粒度可控生成（即在不干扰无关内容的情况下持续引导特定概念）仍然具有挑战性。现有概念滑块方法需要针对每个概念进行训练和架构特定的微调，限制了扩展到新模态的能力。

Method: 提出FreeSliders方法，完全无需训练且模态无关，通过在推理过程中部分估计概念滑块公式来实现。扩展了概念滑块基准以支持视频和音频，建立了首个多模态细粒度概念生成控制套件。提出了一个两阶段程序来自动检测饱和点并重新参数化遍历，实现感知均匀、语义有意义的编辑。

Result: 大量实验表明，该方法能够实现跨模态的即插即用、无需训练的概念控制，优于现有基线方法，并为原则性可控生成建立了新工具。

Conclusion: FreeSliders提供了一种简单而有效的训练自由方法，实现了跨模态的细粒度概念控制，解决了现有方法在可扩展性和模态适应性方面的限制。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [21] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新颖的分层文本到视频生成框架，通过组合场景理解和时间感知扩散模型实现高保真视频合成，在时间一致性、组合理解和视觉叙事控制方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持时间一致性、组合理解和视觉叙事精细控制方面存在困难，需要更先进的框架来解决这些问题。

Method: 提出三个关键创新：组合场景解析器将文本描述分解为带时间标注的分层场景图；时空注意力机制确保帧间连贯运动动态；渐进式视频细化模块通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准测试中达到最先进性能，LPIPS指标提升15.3%，FVD指标提升12.7%，用户偏好研究提升18.9%，特别擅长生成复杂多对象场景。

Conclusion: MOVAI框架在生成具有真实时间动态和精细语义控制的复杂多对象场景方面表现出色，为文本到视频生成提供了有效的解决方案。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [22] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种名为"时间链"的认知启发方法，通过生成模拟过程中的中间图像来改进和解释视觉语言模型中的物理模拟，无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中的上下文推理和人类心理模拟启发，旨在提高视觉语言模型的物理模拟能力并揭示其内部动态。

Method: 在推理时生成一系列中间图像来模拟物理过程，应用于2D图形模拟和3D自然视频，测试速度、加速度、流体动力学和动量守恒等物理属性。

Result: 时间链方法显著提升了最先进图像生成模型的性能，同时揭示了模型能够模拟随时间展开的物理属性（如速度、重力和碰撞），但也发现模型在某些情况下难以从输入图像推断特定物理参数。

Conclusion: 时间链方法不仅提高了物理模拟性能，还提供了对图像生成模型内部物理推理过程的深入理解，超越了传统评估方法的局限性。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [23] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出了首个集成生成AI和深度强化学习的端到端框架，用于实现自主、可重复的心脏超声扫描，解决了现有方法缺乏可重复性、依赖专有数据和使用简化模型的问题。


<details>
  <summary>Details</summary>
Motivation: 心脏超声检查存在操作者依赖性强、时间限制和人为错误等问题，且偏远地区缺乏专业医生。现有基于深度强化学习的方法缺乏可重复性，依赖专有数据，使用简化模型。

Method: 框架包含两个组件：(1) 结合GAN和VAE的条件生成模拟器，生成逼真的动作条件图像；(2) 深度强化学习模块，利用模拟器学习自主、准确的扫描策略。

Result: VAE-GAN在性能评估中优于现有GAN变体，DRL扫描系统在不同配置下均表现出有效性。发布了公开可用的真实心脏超声数据集以确保可重复性。

Conclusion: 该框架通过专家验证模型提供AI驱动指导，支持生成逼真超声图像，建立了可扩展到其他器官的可重复基础。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [24] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D是一种新颖的双流架构，利用RGB-D输入的视觉和几何数据优势，通过集成DINOv2视觉Transformer和PointNet++点云编码器，实现鲁棒且精确的6D物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中6D物体姿态估计的挑战，特别是现有方法在从合成数据泛化到真实场景时，对光照变化、无纹理物体和严重遮挡的脆弱性问题。

Method: 采用双流架构：使用自监督Vision Transformer（DINOv2）处理RGB模态，利用PointNet++编码器处理深度数据生成的3D点云，然后将互补特征流有效融合到多任务预测头。

Result: 在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其卓越的鲁棒性和准确性。

Conclusion: VLM6D通过整合视觉和几何数据的优势，成功解决了6D姿态估计中的关键挑战，特别是在复杂真实场景下的鲁棒性问题。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [25] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出一种结合ConvNeXt和Vision Transformer的混合架构，用于面部图像年龄估计，在多个基准数据集上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 利用CNN的局部特征提取能力和Transformer的全局注意力机制的优势互补，解决面部年龄估计这一复杂计算机视觉任务。

Method: 使用预训练模型，结合线性层和高级正则化技术优化架构，在ConvNeXt框架内采用自适应注意力机制来关注年龄相关面部特征。

Result: 在MORPH II、CACD和AFAD等基准数据集上，该混合架构在平均绝对误差(MAE)方面优于传统方法。

Conclusion: ConvNeXt-ViT混合架构展示了CNN和Transformer无缝集成的潜力，为年龄估计和相关视觉任务提供了稳健基础。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [26] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC是一个基于设施选址函数的高效视觉token压缩框架，通过选择紧凑且具有代表性的视觉token子集，显著减少长视频理解中的视觉token数量，同时保证接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LMMs在处理长视频序列时面临视觉token数量过多的可扩展性限制，需要高效的token压缩方法来解决这一挑战。

Method: 提出基于设施选址函数的视觉token压缩框架，结合惰性贪心算法快速选择紧凑且多样化的视觉token子集，在预定义token预算内实现高效压缩。

Result: 在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC框架持续超越现有压缩技术，在效果和速度方面都表现出色。

Conclusion: FLoC提供了一个无需训练、模型无关且查询无关的通用解决方案，能无缝集成到各种视频-LLMs和现有工作流程中，有效解决长视频理解的关键挑战。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [27] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 提出了一种通过自适应高斯模糊增强图像保护方法对抗噪声逆转技术鲁棒性的简单方法，在保持图像质量的同时提高保护效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于对抗噪声的图像保护方法容易被简单技术（如JPEG压缩）逆转，需要开发更鲁棒的保护方法。

Method: 应用自适应区域高斯模糊来调整噪声的频域特性，增强对抗逆转技术的鲁棒性。

Result: 实验表明该方法能显著提升现有保护方法在多种图像编辑场景下的最差情况保护性能，同时减少噪声引起的质量下降。

Conclusion: 该方法为图像保护提供了一种简单有效的增强方案，提高了对抗噪声逆转的鲁棒性。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [28] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 提出了CompAgent，首个用于视觉合规验证的代理框架，通过增强多模态大语言模型(MLLMs)的视觉工具和动态规划代理，显著提升了视觉合规验证的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉合规验证在媒体、娱乐和广告等领域至关重要，但现有方法依赖特定任务的深度学习模型，成本高且泛化能力有限。MLLMs虽然具有广泛知识，但难以处理细粒度视觉细节和结构化合规规则。

Method: CompAgent框架包含：1)视觉工具套件(目标检测器、人脸分析器、NSFW检测器等)；2)规划代理动态选择工具；3)验证代理整合图像、工具输出和政策上下文进行多模态推理。

Result: 在公开基准测试中，CompAgent优于专用分类器、直接MLLM提示和精心设计的路由基线，在UnsafeBench数据集上达到76%的F1分数，比现有最佳方法提升10%。

Conclusion: 代理规划和工具增强推理为可扩展、准确和适应性强的视觉合规验证提供了有效解决方案。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [29] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: 提出AI Fo框架，通过多智能体协作模拟人类取证调查，用于AI生成图像检测，在6000张图像测试中达到97.05%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法存在局限性：传统分类器缺乏可解释性且无法适应不断发展的生成模型，而视觉语言模型仅限于单次分析和像素级推理。

Method: 开发了基于智能体的图像取证框架，使用反向图像搜索、元数据提取、预训练分类器和VLM分析等取证工具，通过专门的LLM智能体协调收集、综合和推理跨源证据，并采用结构化多智能体辩论机制处理冲突证据。

Result: 在6000张图像的综合评估中，AI Fo达到97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 基于智能体的程序推理为AI生成图像检测提供了更鲁棒、可解释和适应性强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [30] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于能量的多提示学习方法（EMPL），通过从能量分布中采样生成多个提示嵌入，在保持参数效率的同时平衡域内和域外的开放词汇泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单提示范式，很少探索多提示学习的技术潜力。本文旨在为视觉语言多提示学习提供原则性回顾，并证明多提示增强在视觉语言迁移中的优越性。

Method: 将模态间隙现象扩展到可学习提示，提出EMPL方法从能量分布中生成多个提示嵌入，该分布由视觉语言预训练模型隐式定义。

Result: 综合实验验证了方法的有效性，EMPL在保持参数效率的同时，实现了域内和域外开放词汇泛化的平衡。

Conclusion: EMPL方法不仅参数高效，而且严格实现了域内和域外开放词汇泛化之间的平衡，为多提示学习提供了新的技术路径。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [31] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出一种高效的迁移学习方法，用于卫星互联网地面终端组件的细粒度天气条件检测，能够检测雪、潮湿等天气相关状况，性能优于主流深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气事件对低轨卫星互联网性能和可靠性有显著影响，需要地面终端组件具备细粒度天气条件检测能力，以协助故障诊断和缓解，但现有解决方案缺乏且泛化能力不足。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性的天气相关条件，包括雪、潮湿等恶劣和典型天气事件造成的情况。

Result: 该方法在检测雪、潮湿等天气条件方面表现出色，性能优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法，并显示出良好的泛化能力。

Conclusion: 所提出的迁移学习方法能够有效检测地面终端组件的天气相关条件，具有优越性能和泛化能力，为可靠的卫星互联网提供了实用的故障诊断和缓解解决方案。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [32] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 提出DM-QPMNet双编码器网络，通过多模态特征融合解决单次定量相位显微镜细胞分割问题，相比传统方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统阈值方法对噪声和细胞密度敏感，而深度学习简单通道拼接无法充分利用偏振强度图像和相位图的互补特性

Method: 使用双编码器分别处理偏振强度图像和相位图，通过多头注意力在中间深度融合模态特定特征，采用双源跳跃连接和每模态归一化

Result: 相比单一模态基线和简单拼接方法有显著改进

Conclusion: 模态特定编码与可学习融合能有效利用ssQPM同时捕获的互补照明和相位线索，实现稳健的细胞分割

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [33] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出基于VQ-VAE的电子显微镜数据压缩框架，支持16x到1024x压缩比，具有按需解码功能，并引入ROI驱动的工作流程进行选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 解决海量电子显微镜数据集在存储、传输和下游分析方面的挑战，提供高效的数据压缩方案。

Method: 使用向量量化变分自编码器(VQ-VAE)进行压缩，结合Transformer先验模型预测底部token以恢复纹理，采用特征线性调制(FiLM)和拼接技术，并实现ROI驱动的选择性重建。

Result: 实现了从16x到1024x的可扩展压缩比，支持仅顶部解码的极端压缩，同时保持纹理恢复能力。

Conclusion: 该压缩框架为处理海量EM数据集提供了有效的解决方案，支持灵活的按需解码和选择性重建，显著降低了存储和传输需求。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [34] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中计算最优传输映射的新算法，通过几何变分技术将欧几里得和球面几何的方法扩展到双曲设置。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输计算方法主要针对欧几里得空间和球面，但在涉及层次数据、网络和多亏格黎曼曲面的场景中，双曲空间中的最优传输问题自然出现。

Method: 使用几何变分技术，将欧几里得和球面几何的最优传输计算方法扩展到双曲空间设置。

Result: 在合成数据和多亏格曲面模型上的实验验证了所提方法的有效性。

Conclusion: 成功开发了双曲空间中的高效最优传输映射计算算法，填补了现有方法在该领域的空白。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [35] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 提出了L48数据集，这是一个真实世界的细粒度多标签数据集，用于评估单正多标签（SPML）方法，揭示了现有方法在真实场景下的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的SPML方法在合成数据集上表现良好，但这些数据集无法反映真实世界的复杂性和细粒度挑战，需要更真实的基准测试。

Method: 构建了L48数据集，这是一个基于鸟类声音记录的真实世界细粒度多标签数据集，提供了自然SPML设置和两个扩展设置。

Result: 在L48数据集上评估现有SPML方法时，发现与合成数据集相比存在显著的性能差异，并分析了方法的弱点。

Conclusion: 需要更真实和具有挑战性的基准测试来推动SPML方法的发展，L48数据集为此提供了重要工具。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [36] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 开发了一个3阶段自动化流水线来处理大规模甲虫图像数据，包括检测、裁剪和形态分割，以加速生物研究。


<details>
  <summary>Details</summary>
Motivation: 在昆虫学和生态学研究中，生物学家需要处理大量甲虫图像数据，传统手动处理效率低下，需要自动化解决方案。

Method: 使用基于transformer的开放词汇目标检测器和视觉语言模型进行迭代检测，然后手动标注670张甲虫图像并微调transformer分割模型进行精细分割。

Result: 构建了一个专门用于甲虫图像处理的集成深度学习流水线，能够相对准确地完成甲虫检测和分割任务。

Conclusion: 该流水线能显著提高大规模甲虫数据的处理效率，加速生物学研究进程。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [37] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出MambaNetLK，一种基于Mamba状态空间模型的无对应点3D配准方法，在结肠镜导航中显著降低配准误差，并创建了C3VD-Raycasting-10k临床数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜导航中生物组织重复纹理和局部同质几何导致的特征退化问题，以及术前解剖与术中观察之间的显著域偏移问题。

Method: 将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中，使用Lucas-Kanade算法进行迭代配准，有效捕获长程依赖关系。

Result: 在C3VD-Raycasting-10k数据集上，相比次优方法，中值旋转误差降低56.04%，RMSE平移误差降低26.19%，在ModelNet40上表现出强泛化能力和对初始姿态扰动的鲁棒性。

Conclusion: MambaNetLK为手术导航中的3D配准提供了稳健基础，结合全局表达性SSM特征提取器和大规模临床数据集，能够在结肠镜等微创手术中实现更准确可靠的导航系统。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [38] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 提出了Spot The Ball基准测试，用于评估视觉语言模型在视觉社交推理方面的能力，发现人类在定位被移除的球类方面比最先进的模型准确2-3倍。


<details>
  <summary>Details</summary>
Motivation: 人类擅长从微妙的行为线索（如注视、姿势和方向）推断场景的隐藏元素，这种能力对开发更类似人类的AI代理至关重要。

Method: 使用足球、篮球和排球图像创建评估集，要求定位被移除的球，评估四种最先进的视觉语言模型（Gemini、GPT、LLaMA、Qwen）和三种提示策略。

Result: 人类准确率（20-34%）比模型（≤17%）高2-3倍，模型依赖表面空间启发式方法，而人类利用社交线索如注视方向和身体姿势。

Conclusion: 揭示了视觉社交推理中持续存在的人机差距，强调需要明确编码结构化行为线索的架构来实现稳健、类似人类的推理。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [39] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出一种结合CLIP视觉变换器和轻量级变换器分类器的联邦学习框架，用于解决农业分类中的隐私保护和数据非IID问题，在减少通信开销的同时提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练需要大规模数据收集引发隐私担忧，而标准联邦学习在处理非IID数据和通信成本方面存在困难。

Method: 使用预训练的CLIP ViT进行特征提取，仅对轻量级变换器分类器进行联邦更新，并共享1%的CLIP提取特征来对齐类别表示。

Result: 在农业分类任务中达到86.6%的准确率，比基线联邦学习方法高出4倍以上。

Conclusion: 将视觉语言模型特征与联邦学习相结合，能够有效实现隐私保护且可扩展的农业智能应用。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [40] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView是一个轻量级适配方法，仅需100个训练样本即可让现有模型获得多视角生成能力，通过条件架构设计和语义对应对齐损失实现多视角一致性生成。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型虽然能在不同场景下生成身份一致的图像，但无法控制生成图像的视角，也无法生成一致的多视角人物图像。

Method: 设计了条件架构利用预训练扩散变换器的上下文学习能力，并引入语义对应对齐损失来保持预训练模型的原始生成能力。

Result: 在多视角一致性、文本对齐、身份相似性和视觉质量方面显著优于需要大量多视角数据训练的基线方法。

Conclusion: PersonalView仅用少量训练样本就能有效实现多视角生成，显著优于现有方法。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [41] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: LITHOS是最大的公开自动化岩石学实验框架，包含211,604个高分辨率RGB偏振光图像块和105,802个专家标注的矿物颗粒，涵盖25种矿物类别。提出了双编码器Transformer架构，整合两种偏振模式，在矿物分类中优于单偏振模型。


<details>
  <summary>Details</summary>
Motivation: 岩石学分析是劳动密集型任务，需要专家通过光学偏振显微镜进行详细视觉检查，限制了可扩展性，迫切需要自动化技术。

Method: 引入LITHOS数据集，包含高分辨率偏振光图像和专家标注的矿物颗粒。提出双编码器Transformer架构，整合两种偏振模式作为基线方法。

Result: 双编码器Transformer方法在矿物分类中持续优于单偏振模型，证明了偏振协同在矿物分类中的价值。

Conclusion: LITHOS基准测试已公开，包括数据集、代码和预训练模型，以促进自动化岩石学分析的可重复性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [42] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 该研究系统评估了11种轻量级视觉模型在7个数据集上的跨域泛化能力，提出了xScore指标来衡量模型鲁棒性，发现ImageNet性能不能可靠预测其他领域表现，并识别了促进泛化的关键架构组件。


<details>
  <summary>Details</summary>
Motivation: 轻量级视觉模型主要基于ImageNet进行基准测试，但缺乏对其跨数据集泛化能力的系统评估，需要量化模型在不同视觉领域的鲁棒性表现。

Method: 在7个多样化数据集上，以固定100轮训练计划评估11种轻量级视觉模型（250万参数），引入跨数据集评分(xScore)作为统一度量标准。

Result: ImageNet准确率不能可靠预测细粒度或医学数据集性能；xScore可作为移动模型性能的可扩展预测指标；各向同性卷积、高空间分辨率和通道注意力等架构组件能促进泛化，而Transformer块带来额外参数开销但增益有限。

Conclusion: 研究提供了超越ImageNet评估轻量级视觉模型的可复现框架，强调了移动友好架构的关键设计原则，为开发跨领域鲁棒泛化的未来模型提供指导。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [43] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合DeepONet和NTK的混合方法来解决复杂逆问题，包括Navier-Stokes方程控制的源定位和图像重建，克服了非线性、稀疏性和噪声数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决复杂逆问题中非线性、稀疏性和噪声数据带来的挑战，确保解既物理一致又准确。

Method: 将DeepONet与NTK结合，在损失函数中加入物理约束和任务特定正则化。

Result: 在多种合成和真实数据集上验证了方法的鲁棒性、可扩展性和精度。

Conclusion: 该方法在计算物理和成像科学中具有广泛的应用潜力。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [44] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC框架通过联邦学习整合缺失模态恢复，解决了多模态情感识别中模态缺失问题，在多种缺失模式下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中不可预测的模态缺失会显著降低现有多模态情感识别方法的性能，传统依赖完整多模态数据的恢复方法在极端数据分布下容易出现语义失真。

Method: 提出FedDISC框架，通过联邦聚合客户端训练的模态特定扩散模型，并广播给缺失对应模态的客户端；使用DISC-Diffusion模块确保恢复模态与可用模态在上下文、说话者身份和语义上的一致性；引入交替冻结聚合策略循环冻结恢复和分类器模块以促进协作优化。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上的广泛实验表明，FedDISC在不同缺失模态模式下实现了优越的情感分类性能，超越了现有方法。

Conclusion: FedDISC成功将联邦学习整合到缺失模态恢复中，克服了单客户端对模态完整性的依赖，并通过语义一致性机制确保了恢复质量。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [45] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个生成框架，能够直接从OpenStreetMap原始数据生成逼真的卫星图像，支持生成前后对比图像对，用于解决城市监测数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 准确和最新的地理空间数据对城市规划、基础设施监测和环境管理至关重要，但特定城市特征及其变化的策划数据集稀缺，自动化城市监测仍然困难。

Method: 使用OSM JSON的完整丰富性，包括矢量几何、语义标签、位置和时间，提供细粒度的场景生成控制，能够生成一致的前后图像对。

Result: 框架能够生成训练数据以解决稀缺性和类别不平衡问题，并为规划者提供通过编辑地图数据预览拟议干预措施的简单方法。

Conclusion: OSMGen为静态和变化状态生成配对的（JSON，图像）数据，为实现卫星图像自动驱动结构化OSM更新的闭环系统铺平了道路。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [46] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型重建动态的AI生成图像检测框架，通过分析不同噪声强度下的重建指标变化来区分真实和合成图像。


<details>
  <summary>Details</summary>
Motivation: 传统基于频率或像素级伪影的深度伪造检测方法对现代文本到图像系统（如Stable Diffusion和DALL-E）失效，这些系统能生成逼真且无伪影的结果。

Method: 利用多强度图像重建动态（称为扩散回弹），通过分析LPIPS、SSIM和PSNR等重建指标在不同噪声强度下的演变，提取可解释的基于流形的特征。

Result: 在包含4000张图像的平衡数据集上评估，该方法在交叉验证下达到0.993 AUROC，并对压缩和噪声等常见失真保持鲁棒性。

Conclusion: 尽管使用有限数据和单一扩散主干（Stable Diffusion v1.5），该方法展示了强大的泛化能力和可解释性，为可扩展、模型无关的合成媒体取证提供了基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [47] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 该论文提出了一种基于迁移学习的轻量级热红外云分割方法，用于FOREST-2立方卫星任务，通过使用MobileNet编码器的UNet架构，在有限硬件条件下实现高效准确的热红外云检测。


<details>
  <summary>Details</summary>
Motivation: 立方卫星在热红外地球观测中面临硬件限制和标注数据不足的挑战，传统云掩码技术不可行，需要开发适用于单热红外波段的高效云分割方法。

Method: 使用UNet架构配合轻量级MobileNet编码器，先在Landsat-7云覆盖评估数据集上预训练，然后用少量任务特定样本进行联合训练微调，最后转换为TensorRT引擎。

Result: 与仅使用FOREST-2数据的基线相比，宏F1分数从0.850提升到0.877，在NVIDIA Jetson Nano上实现全图像推理时间低于5秒。

Conclusion: 利用公开数据集和轻量级架构能够在轨道上实现准确高效的热红外云掩码，支持数据有限的地球观测任务中的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [48] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频时刻检索模型，通过多智能体系统和证据学习解决不同模型定位结果的冲突问题，并能检测无对应时刻的查询。


<details>
  <summary>Details</summary>
Motivation: 当前视频时刻检索方法未考虑不同模型定位结果的冲突，导致模型无法有效整合产生更好的结果。

Method: 使用强化学习模型扫描整个视频找到时刻边界并生成位置证据，提出多智能体系统框架利用证据学习解决智能体定位输出的冲突。

Result: 在基准数据集上的广泛实验显示，所提方法相比最先进方法具有有效性。

Conclusion: 建模多智能体系统的竞争和冲突是提高强化学习在时刻检索中性能的有效方式，并展示了证据学习在多智能体框架中的新作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [49] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射学辅助框架，通过摄像头直接从显示器捕获医学图像，绕过了传统CAD系统与医院IT基础设施集成的障碍。


<details>
  <summary>Details</summary>
Motivation: 传统计算机辅助诊断系统部署困难，需要与医院现有IT基础设施集成，这阻碍了其广泛临床应用。VisionCAD旨在通过视觉方法解决这一集成难题。

Method: 采用自动化流水线，通过摄像头系统检测、恢复和分析屏幕上的医学图像，将相机捕获的视觉数据转换为适合自动分析和报告生成的诊断质量图像。模块化架构可灵活利用最先进的诊断模型。

Result: 在多样化医学影像数据集上验证，诊断性能与传统CAD系统相当，分类任务中F1分数下降通常小于2%，自动报告的自然语言生成指标与原图像相比差异在1%以内。

Conclusion: VisionCAD仅需摄像头设备和标准计算资源，为AI辅助诊断提供了可访问的方法，可在不修改现有基础设施的情况下在各种临床环境中部署诊断能力。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [50] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench基准测试评估了20个最先进的多模态大语言模型在面部表情识别任务上的表现，发现虽然分类性能良好，但在推理和可解释性方面存在显著局限。为此开发了UniFER-7B模型，通过后训练策略提升面部表情推理能力，在多个数据集上超越了开源和闭源通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域取得了成功，但在面部表情识别任务上的性能尚未得到充分探索。现有方法通常将FER数据集转换为视觉问答格式，但MLLMs在FER任务中的推理和可解释性能力仍有待评估和改进。

Method: 1) 构建FERBench基准，评估20个SOTA MLLMs在4个常用FER数据集上的表现；2) 开发后训练策略，包括使用UniFER-CoT-230K进行冷启动初始化和UniFER-RLVR-360K进行带可验证奖励的强化学习；3) 构建统一的FER基础模型UniFER-7B。

Result: 评估结果显示MLLMs在FER分类任务上表现良好，但在推理和可解释性方面存在显著局限。UniFER-7B模型在多个数据集上超越了包括Gemini-2.5-Pro和Qwen2.5-VL-72B在内的开源和闭源通用MLLMs。

Conclusion: MLLMs在面部表情识别任务中具有潜力但存在推理能力不足的问题。通过专门设计的后训练策略可以显著提升MLLMs的面部表情推理能力，UniFER-7B模型为统一的FER基础模型提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [51] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练框架解决现有视觉语言模型在代码生成任务中的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在代码生成任务中依赖单任务训练，限制了通用视觉代码智能的发展，需要更统一的解决方案。

Method: 采用两阶段训练框架：1) 构建160万图像-代码对的监督微调语料库；2) 引入视觉强化学习策略，通过从粗到细的奖励机制计算局部和全局图像块的视觉相似度。

Result: 在各种多模态代码生成基准测试中，VinciCoder实现了最先进的性能，证明了从粗到细的ViRL策略的有效性。

Conclusion: VinciCoder通过统一的多模态代码生成方法和创新的视觉强化学习策略，显著提升了视觉代码智能的性能，为多模态代码生成提供了有效的解决方案。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [52] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了首个统一框架，通过将SOD、CoSOD和SIS三个异构显著性任务转化为VLM中的思维链推理过程来处理任务异质性。


<details>
  <summary>Details</summary>
Motivation: 解决操作异构的显著性任务（SOD、CoSOD、SIS）的统一处理问题，通过思维链推理桥接任务异质性。

Method: 采用两阶段训练范式：监督微调（SFT）和强化学习（RL）。提出置信度引导策略优化（CGPO）算法，利用奖励与模型置信度差异作为单样本优势信号。引入"输出到推理"策略构建高质量SFT数据。

Result: 模型在所有任务上匹配或超越专用SOTA方法和强闭源VLM，特别是在CoSOD任务上CoCA的S-measure达到0.899，比之前最佳方法提升8.0个百分点，且使用更少训练数据。

Conclusion: 提出的统一框架通过思维链推理和CGPO算法有效处理异构显著性任务，在减少训练数据的情况下实现优越性能。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [53] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出了LGCA框架，通过局部-全局交叉对齐方法解决CLIP模型在图像裁剪时产生的错误信息和偏见问题，显著提升了零样本图像分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像裁剪时，由于小尺度图像特征的相似性，随机裁剪会引入错误信息和偏见，影响模型性能。

Method: LGCA框架首先捕获图像的局部特征，然后重复选择最显著区域并扩展，通过结合原始图像和扩展图像的相似性评分来同时捕捉局部和全局特征。

Result: 实验表明该方法在多个数据集上显著提升了零样本性能，优于现有最先进基线方法。

Conclusion: LGCA框架有效解决了CLIP模型在图像裁剪时的偏见问题，在保持时间复杂度的同时显著提升了零样本图像分类性能。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [54] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出ITEM检测器，利用图像-文本不对齐作为判别线索来检测生成图像，通过预训练CLIP空间测量不对齐程度，结合全局和局部语义不对齐进行检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注视觉线索，容易过拟合特定图像模式且无法泛化到未见模型。发现生成图像与对应描述无法正确对齐，因此从多模态角度解决此问题。

Method: 在预训练CLIP空间中测量图像和描述的不对齐程度，然后微调MLP头部进行检测。提出分层不对齐方案，先关注整个图像，再关注描述中的每个语义对象。

Result: 在多个最新生成模型上表现出优异的泛化能力和鲁棒性，优于其他最先进方法。

Conclusion: 图像-文本不对齐是检测生成图像的有效线索，多模态方法能显著提升检测器的泛化性能。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [55] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出一种基于频率伪造线索(F^2C)的扩散生成图像检测方法，通过增强所有频段的频率特征来提升对不同扩散模型和扰动的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像质量很高，但可能被恶意使用。现有检测器难以捕捉不同模型和设置下的判别性线索，泛化性和鲁棒性有限。

Method: 提出频率选择性函数作为加权滤波器，抑制判别性较弱的频段，增强信息丰富的频段，基于对自然图像和扩散生成图像频率差异的全面分析。

Result: 在多个扩散生成图像数据集上的实验表明，该方法优于现有最先进的检测器，具有更好的泛化性和鲁棒性。

Conclusion: 通过增强频率伪造线索的方法能够有效检测来自未见扩散模型的图像，并对各种扰动具有强鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [56] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP是一个针对CLIP模型预训练阶段的文本模态对抗攻击框架，通过背景感知选择和背景驱动增强生成高质量的中毒文本，在分类和检索任务中达到高攻击成功率并能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖大规模网络数据进行自监督对比学习，但未经过滤的互联网数据使其面临数据中毒和后门攻击风险。现有研究主要关注图像模态攻击，而文本模态作为CLIP训练的核心部分尚未得到充分探索。

Method: ToxicTextCLIP框架迭代应用两个组件：1) 背景感知选择器，优先选择与目标类别背景内容对齐的文本；2) 背景驱动增强器，生成语义连贯且多样化的中毒样本。

Result: 在分类和检索任务中，ToxicTextCLIP实现了高达95.83%的中毒成功率和98.68%的后门Hit@1命中率，并能成功绕过RoCLIP、CleanCLIP和SafeCLIP等防御机制。

Conclusion: 该研究表明CLIP模型在文本模态上同样存在严重的安全漏洞，需要开发更全面的防御策略来应对多模态攻击威胁。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [57] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出弱监督深度学习框架，使用Grad-CAM从胸部X光片进行肺炎分类和定位，无需像素级标注，仅需图像级标签即可生成临床有意义的肺炎区域热图。


<details>
  <summary>Details</summary>
Motivation: 解决传统肺炎诊断方法需要昂贵像素级标注的问题，开发能够提供可解释性结果的AI模型，增强临床对AI辅助医疗影像的信任。

Method: 使用七种ImageNet预训练架构(ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3、ViT-B16)，在相同训练条件下采用焦点损失和患者级数据分割，利用Grad-CAM生成热图解释。

Result: 在Kermany CXR数据集上，ResNet-18和EfficientNet-B0达到最佳测试准确率98%，ROC-AUC=0.997，F1=0.987；MobileNet-V2在准确率和计算成本间提供最佳平衡。

Conclusion: 弱监督可解释模型能够有效定位肺炎相关肺区域，增强肺炎筛查的透明度和临床可信度，展示了可解释AI在放射学诊断中的应用潜力。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [58] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，通过单张图像联合建模外观和人体部位语义，在3D人体重建和分割任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在3D人体重建方面取得了高保真度，但在特定任务（如人体3D分割）中的实用性仍然受限。

Method: 在重建阶段整合人体几何先验，在分割阶段整合自监督语义先验；开发交互式标注程序生成高质量数据标签对；采用像素对齐聚合实现跨任务协同；多任务目标同时优化纹理建模保真度和语义一致性。

Result: 广泛实验表明，HumanCrafter在单张图像的3D人体部位分割和3D人体重建方面均超越了现有最先进方法。

Conclusion: 该框架通过联合建模外观和语义，有效解决了3D人体任务中的数据稀缺问题，实现了跨任务的协同优化。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [59] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出了一个基于引导式深度学习框架的前庭神经鞘瘤MRI分割数据集，通过迭代分割和质量优化实现高效自动分割，在内部验证集上DSC从0.9125提升至0.9670，效率提升约37.4%。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤的准确MRI分割对患者管理至关重要，但传统手动标注耗时且依赖专家。深度学习自动分割在多样数据集和复杂临床案例中仍面临鲁棒性挑战。

Method: 采用引导式深度学习框架进行迭代分割和质量优化，结合多中心数据并依赖专家共识确保标注可信度，实现人类在环模型训练。

Result: 在目标内部验证集上分割准确率显著提升(DSC从0.9125到0.9670)，在外部数据集上保持稳定性能，专家评估143个扫描识别出需要专家干预的细微案例。

Conclusion: 人类在环模型训练方法实现了高分割准确率，展示了在多样化临床环境中自动前庭神经鞘瘤分割的临床适应性和泛化潜力。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [60] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP是一种用于视觉语言模型的个性化联邦提示学习新范式，通过多组文本和视觉提示捕获细粒度语义，采用动态提示聚合策略实现参数高效的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中个性化与通用知识保留的平衡问题，通过多组提示捕获客户端多样化的局部特征，同时保持参数效率。

Method: 为每个客户端配备多组配对的文本和视觉提示，引入多样性损失使各组专注于不同语义方面，采用基于相似性概率采样的动态提示聚合策略。

Result: 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均优于现有方法，且在所有联邦提示学习方法中通信参数最低。

Conclusion: FedMGP通过多组提示和动态聚合策略有效平衡了共享语义学习和客户端特定特征保留，实现了参数高效的个性化联邦学习。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [61] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种前馈方法，可从单张图像合成可控且显式的4D场景，结合视频扩散模型的生成先验与4D数据集学习的几何和运动约束，直接预测可变形3D高斯场。


<details>
  <summary>Details</summary>
Motivation: 旨在从单张图像高效合成高质量的4D场景，克服传统基于优化的方法在动态场景合成中计算成本高的问题。

Method: 使用视频潜在变换器增强视频扩散模型，联合捕捉时空依赖性并预测时变3D高斯基元，通过外观保真度、几何精度和运动一致性的目标进行训练。

Result: 在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取方面匹配或超越基于优化的方法，同时显著更高效。

Conclusion: Diff4Splat提供了一种高效的单图像到4D场景合成方法，结合生成先验和几何运动约束，实现了快速且高质量的动态场景生成。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [62] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模可解释医学视觉问答数据集，包含17,597个问答对和4,394张胸部X光图像，带有放射科医生验证的边界框和临床推理解释。


<details>
  <summary>Details</summary>
Motivation: 推动可解释和临床基础的医学视觉问答研究，解决现有数据集在空间定位和可靠性方面的不足。

Method: 构建包含六种诊断类型问题的平衡数据集，使用MedGemma-4B-it模型进行基准测试，并包含放射科医生验证的边界框标注。

Result: 在基准测试中F1得分为0.624，相比基线提升11.8%，同时实现了病灶定位功能。

Conclusion: VinDr-CXR-VQA数据集促进了可重现和临床基础的Med-VQA研究，数据集和评估工具已公开可用。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [63] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++是一个用于全景图像多目标跟踪的反馈驱动框架，通过动态特征稳定、轨迹反馈关联和专家记忆系统解决全景视角带来的失真、大搜索空间和身份模糊等挑战，在JRDB和EmboTrack基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法在全景图像中表现不佳，因为360度视野带来分辨率稀释、视角依赖失真等独特挑战，需要专门设计的方法来处理全景失真、大搜索空间和身份模糊问题。

Method: 采用反馈驱动框架：DynamicSSM块稳定全景特征，FlexiTrack实例使用轨迹反馈进行灵活定位和短期关联，ExpertTrack记忆通过专家混合设计整合外观线索，Tracklet管理模块根据场景动态自适应切换跟踪模式。

Result: 在JRDB和EmboTrack基准测试中，OmniTrack++相比原始OmniTrack实现了显著改进：JRDB上HOTA提升+25.5%，QuadTrack上HOTA提升+43.07%，达到了最先进的性能水平。

Conclusion: OmniTrack++通过反馈驱动的渐进式感知优化方法，有效解决了全景多目标跟踪的独特挑战，为真实世界全景感知提供了一个平衡且可扩展的解决方案。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [64] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个新颖的多主体视频生成框架，通过文本提示和参考图像生成包含多个主体的视频，解决了现有模型在身份保持、语义整合和时间一致性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常只能基于文本或单张图像生成视频，限制了可控性和应用范围。需要开发能够处理多主体视频生成并保持主体身份一致性的方法。

Method: 设计了分层身份保持注意力机制来聚合跨主体和模态的特征；利用预训练视觉语言模型提供细粒度语义指导；采用在线强化学习阶段优化关键概念对齐。

Result: 大量实验表明，ID-Composer在身份保持、时间一致性和视频质量方面优于现有方法。

Conclusion: ID-Composer通过创新的注意力机制、语义理解和强化学习策略，成功解决了多主体视频生成中的关键挑战，为可控视频生成提供了有效解决方案。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [65] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出一种无需训练或偏置标注的测试时去偏方法，使用预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，从而消除混杂视觉区域的偏置信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法需要训练数据和显式组标签进行微调或调整嵌入，限制了实际应用。测试时方法虽然避免这一约束，但仍依赖数据集特定偏置的先验知识，在开放集设置中泛化性有限。

Method: 使用预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，在保留目标属性的同时消除混杂视觉区域的偏置信号。

Result: 在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和Attention IoU方面优于现有测试时去偏方法。

Conclusion: 分割引导的干预方法在视觉语言模型中实现了可扩展且无需标注的偏置缓解，证明了其有效性。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [66] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了T-VAD框架，基于大视觉语言模型实现文本引导的细粒度视频异常检测，通过异常热图解码器和区域感知异常编码器提升检测的粒度和交互性。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法输出有限（仅正常或异常），且多为半自动化需要人工评估，需要更细粒度和交互性的异常检测解决方案。

Method: T-VAD框架包含异常热图解码器（AHD）进行像素级视觉-文本特征对齐生成细粒度异常热图，以及区域感知异常编码器（RAE）将热图转换为可学习文本嵌入，指导LVLM准确识别和定位视频异常事件。

Result: 在UBnormal数据集上达到94.8% AUC和67.8%/76.7%的热图精度；在ShanghaiTech数据集上BLEU-4得分62.67/88.84，Yes/No准确率97.67%；在UBnormal数据集上BLEU-4得分50.32/78.10，Yes/No准确率89.73%。

Conclusion: T-VAD显著提升了视频异常检测的粒度和交互性，实现了最先进的性能表现。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [67] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了Real-IAD Variety，这是最大最全面的工业异常检测基准数据集，包含198,960张高分辨率图像，涵盖160个物体类别、28个行业、24种材料和22种颜色变化，旨在解决现有基准数据集类别多样性不足和规模有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测基准数据集存在类别多样性不足、规模有限的问题，导致模型性能饱和且在实际场景中泛化能力差，需要更全面的大规模基准来推动领域发展。

Method: 构建了包含198,960张高分辨率图像的Real-IAD Variety数据集，覆盖160个物体类别、28个行业、24种材料类型和22种颜色变化，确保数据多样性，并在多类无监督、多视角和零/少样本设置下进行严格评估。

Result: 实验表明，当类别从30扩展到160时，最先进的多类无监督异常检测方法性能显著下降，而视觉语言模型展现出对类别扩展的强鲁棒性，在不同类别数量下性能变化最小，显著提升了在多样化工业环境中的泛化能力。

Conclusion: Real-IAD Variety的规模和复杂性使其成为训练和评估下一代异常检测基础模型的重要资源，将加速超越领域特定限制的研究，推动可扩展通用异常检测系统的发展。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [68] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出了一种从单张图像中精确学习和合成多实例语义的方法，通过惩罚性注意力优化和盒子控制来解决相似语义分离和布局控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像学习多实例语义时训练数据有限的问题，特别是在实例具有相似语义或外观时的挑战。

Method: 使用惩罚性注意力优化在学习阶段分离相似语义，在合成阶段引入并优化注意力层中的盒子控制来减少语义泄漏并精确控制输出布局。

Result: 实验结果表明该方法实现了分离且高质量的语义学习和合成，在可编辑性和实例一致性之间取得了良好平衡，在处理语义或视觉相似实例或罕见对象时保持鲁棒性。

Conclusion: 该方法能够有效解决多实例语义学习和合成的挑战，特别是在处理相似语义实例时表现出色。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [69] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-NVS结合体素表示与神经高斯泼溅，通过紧凑的神经体素和变形场建模动态场景，大幅减少内存消耗并加速训练，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅在动态场景中因跨帧复制高斯而导致的严重内存开销问题。

Method: 使用紧凑的神经体素集合和学习的变形场来建模时间动态，避免为每个时间戳生成独立的高斯集合；引入视图细化阶段，通过针对性优化选择性改善挑战性视角。

Result: 实验表明该方法在显著减少内存和加速训练的同时，超越了最先进方法，实现实时渲染和优越的视觉保真度。

Conclusion: 4D-NVS通过神经体素和变形场的结合，有效解决了动态场景建模中的内存效率问题，实现了高效且高质量的渲染。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [70] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了FREE框架，通过频域信息增强模型在分布偏移下发现类别的能力，解决了域偏移广义类别发现(DS_GCD)问题


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在标准条件下表现良好，但在分布偏移时性能下降。本文探索更现实的场景：未标记数据不仅包含未知类别，还来自未知域

Method: 1) 基于频率的域分离策略，通过测量幅度差异划分样本到已知和未知域；2) 跨域和域内频率扰动策略；3) 扩展自监督对比目标和语义聚类损失；4) 聚类难度感知重采样技术

Result: 在多个基准数据集上的广泛实验表明，该方法有效缓解分布偏移影响，在发现已知和未知类别方面均取得优异性能

Conclusion: FREE框架通过频域信息有效解决了域偏移广义类别发现问题，在各种分布偏移场景下表现出色

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [71] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出一种上下文感知的零样本异常检测方法，通过融合时间特征和视觉嵌入，结合上下文记忆实现实时异常分类，在UCF-Crime和XD-Violence数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视频异常通常依赖于上下文信息和时间演化，但大多数异常检测器无法感知这种上下文依赖，限制了其在实际场景中的泛化能力。

Method: 采用记忆增强的流水线，通过交叉注意力关联时间信号与视觉嵌入，并使用上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime上达到90.4% AUC，在XD-Violence上达到83.67% AP，在零样本模型中创下新的SOTA，并实现实时推理。

Conclusion: 通过融合交叉注意力时间融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在现实世界监控应用中的适用性迈出了重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [72] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench是首个专注于上下文感知视频异常理解的基准测试，通过事件中心分层分类法定义了14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性，并在识别、时序定位、检测和预测等任务上统一评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型对真实世界视频异常的理解仅停留在表面，缺乏对复杂原理和微妙上下文的理解能力，无法区分如带安全装备与不带安全装备攀岩等细微差异。

Method: 提出了CueBench基准测试框架，建立事件中心分层分类法，并开发了Cue-R1方法，基于R1风格强化微调，使用可验证、任务对齐和层次细化的奖励进行统一生成式训练。

Result: 在CueBench上的广泛实验显示，现有视觉语言模型对真实世界异常理解仍远未令人满意，而Cue-R1方法平均超越最先进方法超过24%。

Conclusion: 真实世界视频异常理解仍具挑战性，CueBench为生成-判别和通用-专用视觉语言模型提供了严格的评估套件，Cue-R1展现了显著性能提升。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [73] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了一个名为triplet segmentation的新任务，通过仪器实例分割来空间定位手术动作三元组<仪器、动词、目标>，并开发了TargetFusionNet架构和CholecTriplet-Seg数据集来提升手术动作理解的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作三元组识别方法局限于帧级分类，无法可靠地将动作与特定仪器实例关联；之前的空间定位方法主要依赖类激活图，缺乏精确性和鲁棒性，无法满足详细仪器-组织交互分析的需求。

Method: 提出了triplet segmentation任务，创建了CholecTriplet-Seg数据集（包含30,000+标注帧），并设计了TargetFusionNet架构，该架构扩展了Mask2Former，通过目标感知融合机制将弱解剖先验与仪器实例查询融合。

Result: 在识别、检测和三元组分段的多个指标上，TargetFusionNet始终优于现有基线方法，表明强实例监督与弱目标先验的结合显著提高了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分段为空间定位手术动作三元组建立了统一框架，提出的基准和架构为更可解释的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [74] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究介绍了首个大规模多光谱激光雷达基准数据集FGI-EMIT，用于个体树木分割，并比较了传统无监督算法和深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统个体树木分割方法缺乏大规模基准数据集，特别是多光谱激光雷达数据，而多光谱反射率已被证明能提高分割精度。

Method: 使用FGI-EMIT数据集（1,561棵人工标注树木）全面评估了4种传统无监督算法和4种深度学习模型，无监督方法采用贝叶斯优化超参数，深度学习模型从头训练。

Result: 无监督方法中Treeiso表现最佳（F1分数52.7%），深度学习方法显著更优，ForestFormer3D达到73.3% F1分数，在底层树木分割上比Treeiso高出25.9个百分点。

Conclusion: 深度学习方法在个体树木分割中明显优于传统算法，但当前深度学习方法未能充分利用多光谱反射率信息，单通道反射率对底层树木分割有轻微改善。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [75] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个元数据引导的框架，通过学习MRI对比度表示，将体积图像与其DICOM采集参数对齐，解决MRI数据异质性和缺乏标准化对比度标签的问题。


<details>
  <summary>Details</summary>
Motivation: MRI存在显著的数据异质性，且缺乏跨扫描仪、协议和机构的标准化对比度标签，这严重限制了大规模自动化分析。统一的MRI对比度表示可以实现从自动序列识别到协调和质量控制的各种下游应用，而无需依赖手动注释。

Method: 引入MR-CLIP框架，通过将体积图像与DICOM采集参数对齐来学习MRI对比度表示。该框架利用常规可用的采集元数据作为监督信号。

Result: 生成的嵌入显示MRI序列的明显聚类，在数据稀缺情况下，在少样本序列分类任务中优于监督的3D基线方法。此外，MR-CLIP通过图像-元数据嵌入距离识别损坏或不一致的元数据，实现无监督数据质量控制。

Conclusion: 通过将常规可用的采集元数据转化为监督信号，MR-CLIP为跨不同临床数据集的标签高效MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [76] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种用于图像超分辨率网络的后训练量化方法，通过双区域量化策略和敏感度感知微调来解决激活值异常值问题，在保持性能的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法在图像超分辨率任务中性能不佳，主要原因是忽视了激活值中的异常值影响。研究发现这些异常值与图像颜色信息强相关，直接移除会导致性能显著下降。

Method: 1. 双区域量化策略：将激活值划分为异常值区域和密集区域，分别进行均匀量化以优化比特分配；2. 敏感度感知微调：针对不同网络层对量化的敏感度差异，让模型更关注高敏感层。

Result: 在多种超分辨率网络和数据集上的实验表明，该方法优于现有后训练量化方法，在大多数场景下达到与量化感知训练相当的性能，同时实现至少75倍的加速。

Conclusion: 该方法有效解决了图像超分辨率网络中后训练量化的挑战，在保持高质量输出的同时显著提升推理效率。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [77] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，通过LLM进行语义演化并使用CLIP嵌入量化新颖性，从单个输入提示生成多样化图像集合，显著提升了图像多样性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像，但输出多样性有限，限制了在探索性和构思任务中的应用。现有提示优化技术通常针对美学适应性或不适合创意视觉领域。

Method: WANDER直接在自然语言提示上操作，使用大型语言模型进行语义演化，利用CLIP嵌入量化新颖性，并应用发射器引导搜索到不同的提示空间区域。

Result: 使用FLUX-DEV生成和GPT-4o-mini变异的实证评估表明，WANDER在多样性指标上显著优于现有的进化提示优化基线方法。消融研究证实了发射器的有效性。

Conclusion: WANDER通过新颖性搜索和语义演化成功解决了文本到图像模型输出多样性不足的问题，为创意视觉任务提供了有效的解决方案。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [78] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 本文分析了低剂量CT图像增强中不同损失函数与图像质量指标之间的一致性，发现两者存在不一致性，并强调在开发新损失函数时需要考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT成像广泛用于减少辐射暴露，但常受噪声和伪影影响。虽然深度学习模型能增强LDCT图像，但常用的PSNR和SSIM指标在反映医学图像感知质量方面存在局限。

Method: 对基于深度学习的LDCT图像增强架构中的损失函数进行客观分析，评估不同损失函数与图像质量指标的相关性和一致性。

Result: 研究发现损失函数与质量指标之间存在不一致性，表明仅优化传统损失函数可能无法保证图像感知质量的提升。

Conclusion: 在开发用于图像质量增强的新损失函数时，必须考虑图像质量指标，以确保模型性能与临床诊断需求一致。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [79] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 深度学习模型在ADNI数据集上表现优异（AUC达0.96-0.97），但在拉丁美洲FLENI队列上性能显著下降（AUC降至0.80-0.82），揭示了显著的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习模型在阿尔茨海默病诊断中对代表性不足人群的泛化能力，特别是从北美ADNI数据集到拉丁美洲FLENI临床队列的跨域性能。

Method: 使用卷积和Transformer架构模型在ADNI数据集上训练，并在FLENI队列上评估泛化性能，通过消融研究和遮挡敏感度分析识别关键影响因素。

Result: 所有模型在ADNI上表现优异，但在FLENI上性能显著下降；Transformer并未显示出明显优势；图像归一化和采样选择是影响泛化的关键因素。

Conclusion: 需要基于人群的AI模型验证，未来工作应关注领域适应和队列多样化，以确保诊断模型在不同人群中的有效性。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [80] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 将地点识别重新定义为多类分类问题，通过为LiDAR扫描分配离散位置标签，直接训练编码器-解码器模型进行分类，在NuScenes数据集上取得与对比学习方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有地点识别方法大多依赖对比学习，本文探索将其作为多类分类问题的替代方法，以提高训练效率和稳定性。

Method: 为LiDAR扫描分配离散位置标签，使用编码器-解码器模型直接对每个扫描的位置进行分类。

Result: 在NuScenes数据集上评估，该方法取得了与基于对比学习方法竞争的性能。

Conclusion: 多类分类方法在地点识别任务中具有竞争力，同时在训练效率和稳定性方面具有优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [81] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 研究发现生成式AI模型在编码'美'概念时存在系统性偏见，86.5%生成图像为浅肤色人群，74%为年轻年龄段，非二元性别个体被描绘得更年轻和过度性化，负面特征提示会产生更多NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的传播，导致负面自我形象问题。随着AI生成内容增多，担忧这些审美标准被夸大，需要研究生成式AI如何编码'美'概念并消除'丑'。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型再到图像模型。开发结构化审美分类法，使用三个语言模型和两个文本到图像模型生成5984张图像，招募女性和非二元社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像为浅肤色人群，22%包含明确内容（尽管经过SFW训练），74%被评定为年轻年龄段。非二元个体图像被评定为更年轻和过度性化。带有负面审美特征的提示无论性别都会产生更高NSFW评分。

Conclusion: 生成式AI模型存在与审美标准相关的普遍人口统计偏见，这些偏见通过模型开发者（如负面提示）积极延续，可能导致数据流污染和不符合开发者审美刻板印象的特征被主动消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [82] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 提出一个基于物联网的动物检测系统，集成YOLOv5和SSD算法来提高榴莲种植园的动物入侵检测准确率，并通过Telegram通知和声音威慑机制实现自动响应。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植园面临动物入侵导致的作物损害和经济损失，传统农业实践缺乏无人监控手段，现有系统依赖单一检测算法且通知平台和威慑机制有限。

Method: 集成YOLOv5和SSD物体检测算法，结合物联网技术实现实时监控，检测到入侵时自动发送Telegram通知并触发声音威慑机制（如老虎吼声）。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%，白天准确率最高，夜间有所下降，图像和视频检测效果一致。

Conclusion: 该研究提供了一个结合检测、通知和威慑的全面实用框架，为自动化农业解决方案的未来创新铺平了道路。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [83] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自适应2D掩码跟踪方法，通过保持帧间时间一致性来消除冲突的3D伪标签，结合三阶段课程学习框架，从碎片化单视图数据逐步训练到全局一致的全场景监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将2D掩码从基础模型转移到3D来生成伪标签，但由于视频帧被独立处理，导致分割粒度不一致和冲突的3D伪标签，降低了最终分割的准确性。

Method: 粒度一致的自适应2D掩码跟踪方法，结合三阶段课程学习框架：从碎片化单视图数据到统一多视图标注，再到全局一致的全场景监督。

Result: 实验结果表明，该方法能有效生成一致且准确的3D分割，在标准基准测试中达到了最先进的性能，并具备开放词汇能力。

Conclusion: 该方法能够从最初碎片化和矛盾的2D先验中稳健地提取出一致的3D表示，解决了现有方法中的粒度不一致和伪标签冲突问题。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [84] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私保护联邦学习的可重现基准测试平台，使用合成的肿瘤CT扫描数据评估分割性能和隐私泄露，揭示了隐私与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能在保护隐私的前提下实现多机构协作训练，但仍面临成员推理攻击和数据异质性等安全挑战，需要标准化的评估平台。

Method: 构建FedOnco-Bench基准测试平台，使用合成的肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和FedAvg+DP-SGD等联邦学习方法的分割性能和隐私保护能力。

Result: FedAvg性能最佳（Dice约0.85）但隐私泄露最多（攻击AUC约0.72）；DP-SGD隐私保护最好（AUC约0.25）但性能下降（Dice约0.79）；FedProx和FedBN在数据异质性下表现均衡。

Conclusion: FedOnco-Bench为医学图像分割的隐私保护联邦学习方法提供了标准化、开源的基准测试平台，揭示了不同方法在隐私与性能之间的权衡关系。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [85] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K数据集，包含50K医疗图像，用于基于指令的医疗图像编辑，涵盖三种模态和23种疾病类型，通过Gemini-2.5-Flash-Image生成双向编辑，并采用医疗质量控制和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医疗图像编辑方面取得进展，但缺乏大规模、高质量、开放的专门数据集，限制了研究进展。

Method: 利用Gemini-2.5-Flash-Image从真实医疗图像生成双向编辑（病灶添加和移除），采用LLM-as-Judge进行医疗质量评估，并进行最多五轮的迭代优化。

Result: 构建了包含50K图像的数据集，涵盖三种医疗成像模态和23种疾病类型，还包括37K失败案例用于偏好学习研究。

Conclusion: Med-Banana-50K为训练和评估下一代医疗图像编辑模型提供了基础，数据集和代码已公开。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [86] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力的坐标无关GUI定位框架，通过监督微调触发MLLMs的固有定位能力，在3B参数模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MLLMs的GUI定位方法将任务表述为基于文本的坐标生成任务，但从视觉输入直接生成精确坐标具有挑战性且计算量大。

Method: 提出GUI-AIMA框架，将MLLMs的内在多模态注意力与补丁级定位信号对齐，通过多头聚合简化查询-视觉注意力矩阵，并采用坐标无关方式便于集成放大阶段。

Result: GUI-AIMA-3B仅使用85k截图训练，在ScreenSpot-Pro上达到58.6%的平均准确率，在OSWorld-G上达到62.2%，在3B模型中实现最先进性能。

Conclusion: 轻量训练可以触发MLLMs的固有定位能力，GUI-AIMA展示了卓越的数据效率和性能表现。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [87] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 提出TA-LSDiff模型，结合拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现胰腺分割，在四个公共数据集上达到最先进精度


<details>
  <summary>Details</summary>
Motivation: 胰腺分割面临尺寸小、对比度低和拓扑变化大的挑战，传统水平集方法忽略点状拓扑效应，而深度学习网络常牺牲结构细节

Method: 结合拓扑感知扩散概率模型和水平集能量，通过四个互补项整合输入图像和深度特征，并引入像素自适应细化模块通过邻域证据的亲和权重局部调制能量函数

Result: 在四个公共胰腺数据集上的评估显示TA-LSDiff达到最先进精度，优于现有方法

Conclusion: TA-LSDiff为胰腺分割提供了一个实用且准确的解决方案

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [88] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA是一个新的位置编码框架，通过模态特定位置编码和全局自适应编码步长缩放，提升视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型使用统一的位置索引策略，没有考虑文本和视觉在结构和连续性上的差异。

Method: 使用模态特定位置编码分别处理文本和视觉的位置信息，并通过全局自适应编码步长缩放调整视觉token的位置编码步长。

Result: 在多种架构和VQA基准测试中，OMEGA持续提升VLM性能，在视觉密集型任务上比基线位置编码策略提升达3.43%。

Conclusion: OMEGA通过考虑模态特定结构和信息密度，有效提升了视觉语言模型的性能。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [89] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为LSSA的新型多模态对抗攻击方法，通过局部图像块随机打乱和采样来增强对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对抗攻击方法存在过拟合问题，主要原因是缺乏输入多样性，过度依赖单一模态的对抗样本信息。

Method: LSSA方法随机打乱局部图像块来扩展原始图像-文本对，生成对抗图像并采样，然后利用原始和采样图像生成对抗文本。

Result: 在多个模型和数据集上的实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的迁移性，并在大型视觉语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过增加输入多样性有效解决了多模态对抗攻击中的过拟合问题，显著提升了对抗样本的迁移性能。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [90] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出VCA（Visual-Contrast Attention）作为MHSA的替代方案，通过视觉对比注意力机制降低计算复杂度，在图像识别和生成任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统ViT中的多头自注意力层对每个token对进行二次查询-键交互，将大量计算花费在视觉上弱或冗余的相关性上，需要更高效的注意力机制

Method: VCA首先将每个头的密集查询场蒸馏为少量空间池化的视觉对比token，然后将它们分成可学习的正负流，通过差分交互突出区域间的真正差异

Result: 在ImageNet-1K上将DeiT-Tiny的top-1准确率从72.2%提升到75.6%（+3.4%），在三个强层次化ViT上提升达3.1%，在图像生成任务中降低FID-50K 2.1到5.2个点

Conclusion: VCA为构建更快更锐利的Vision Transformer提供了一条简单路径，通过空间池化提供低方差全局线索，双位置嵌入对对比推理不可或缺

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [91] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 提出参数插值对抗训练（PIAT）框架，通过插值相邻epoch的模型参数来缓解对抗训练中的振荡和过拟合问题，并使用归一化均方误差（NMSE）进一步提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法存在模型鲁棒性明显振荡和过拟合问题，影响防御效果。

Method: PIAT框架在相邻epoch间插值模型参数，使决策边界变化更平缓；同时使用NMSE对齐干净样本和对抗样本的logits相对大小。

Result: 在多个基准数据集上的实验表明，该框架能显著提升CNN和ViT模型的鲁棒性。

Conclusion: PIAT通过参数插值和NMSE有效缓解了对抗训练中的过拟合问题，提高了模型鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [92] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个专门评估多模态大语言模型在脑成像分析中多模态理解能力的综合性基准测试，包含15种脑成像模态、9,527个验证问答对和31,706张图像，覆盖15个多阶段临床任务。


<details>
  <summary>Details</summary>
Motivation: 当前面向脑部的视觉问答基准要么覆盖的成像模态有限，要么仅限于粗粒度的病理描述，阻碍了对多模态大语言模型在整个临床连续体中能力的全面评估。

Method: 构建包含15种不同脑成像模态的综合多模态VQA基准，从30个验证医学来源收集数据，模拟临床工作流程，涵盖15个多阶段临床任务，并由专业放射科医生严格验证。

Result: 评估24个最先进模型显示：专有MLLMs优于开源和医学模型但仍落后于医生；医学MLLMs性能差异大；开源MLLMs整体落后但在特定任务中表现出色；MLLMs在复杂术前任务中表现明显不佳，揭示了视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进脑成像分析中的MLLMs设立了新标准，突显了与专家临床推理相比的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [93] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型（ODM），用于在遮挡场景下预测行人过街意图，通过重建被遮挡的运动模式来指导未来意图预测。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习模型在遮挡场景下无法处理不完整观察的问题，提高在复杂环境中的行人意图预测准确性。

Method: 使用遮挡感知扩散变换器架构估计被遮挡模式的噪声特征，引入遮挡掩码引导的反向过程来有效利用观察信息。

Result: 在PIE和JAAD基准测试中，该方法在各种遮挡场景下表现出比现有方法更鲁棒的性能。

Conclusion: 所提出的ODM方法能够有效处理遮挡场景下的行人意图预测问题，显著提升了预测准确性和鲁棒性。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [94] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 提出了一种名为LMD的后处理、模型无关的可解释性方法，用于解耦多模态融合模型中各传感器模态的贡献，以增强自动驾驶感知模型的透明度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型的决策透明度至关重要，因为即使单个误判也可能导致灾难性后果。但在多传感器输入的情况下，由于传感器信息在融合网络中纠缠在一起，很难确定每个模态对预测的贡献。

Method: 引入了层间模态分解(LMD)方法，这是一种后处理、模型无关的可解释性方法，能够在预训练融合模型的所有层中解耦模态特定信息。

Result: 在预训练的相机-雷达、相机-LiDAR和相机-雷达-LiDAR融合模型上评估LMD，通过结构化扰动指标和模态可视化分解验证了其有效性。

Conclusion: LMD是首个能够将感知模型预测归因于自动驾驶传感器融合系统中单个输入模态的方法，展示了在高容量多模态架构解释中的实际适用性。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [95] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的端到端框架，用于解决医学多模态大语言模型在处理异构医学数据（包括2D图像、3D体积扫描和时序视频）时的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异构性，包含2D图像、3D体积扫描和时序视频等多种模态，这些模态之间存在显著的领域差距和数据格式不一致问题，阻碍了统一医学MLLMs的发展。

Method: 采用数据中心的三种策略：1）通过整合自然和医学特定领域的长上下文数据扩展预训练；2）用罕见医学数据补充微调；3）扩展评估框架以包含3D体积和视频理解基准。通过监督微调和组相对策略优化开发多规模模型。

Result: Fleming-VL在多个基准测试中实现了最先进的性能，包括医学VQA、视频QA和3D医学图像理解。

Conclusion: Fleming-VL为医学AI的透明、可重现和可审计进展提供了统一解决方案，并公开发布了该模型以促进相关研究。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [96] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出动态多级加权对齐网络解决零样本草图图像检索问题，通过多级加权机制和加权四元组损失提升跨模态对齐质量


<details>
  <summary>Details</summary>
Motivation: 现有方法存在模态样本不平衡和低质量信息不一致问题，导致性能不佳

Method: 包含三个组件：单模态特征提取模块（CLIP文本编码器和ViT）、跨模态多级加权模块（局部和全局聚合块生成对齐权重）、加权四元组损失模块

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上优于现有最先进的ZS-SBIR方法

Conclusion: 该方法通过动态多级加权对齐有效解决了零样本草图图像检索中的模态不平衡问题

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [97] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一个端到端的虚拟试穿模型，仅需源图像和目标服装作为输入，无需复杂预处理，通过两阶段训练策略和参考图像增强技术提升试穿效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如不可知人物图像、人体姿态、密集姿态或关键点），导致过程繁琐且不实用。EVTAR旨在简化输入要求，提高实际应用可行性。

Method: 采用两阶段训练策略，仅需源图像和目标服装作为推理输入。利用不同人穿着同一服装的参考图像来保持服装纹理和细节，模拟人类选择服装时的参考行为。

Result: 在两个广泛使用的基准测试和多样化任务上评估，结果一致验证了方法的有效性。

Conclusion: EVTAR通过简化输入要求和引入参考图像机制，实现了更实用和高质量的虚拟试穿效果，为实际应用提供了可行方案。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [98] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一个统一的零样本视频异常分析框架，通过链式推理过程连接时间检测、空间定位和文本解释任务，无需额外训练即可实现全面的异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究大多停留在帧级检测，缺乏对异常原因的解释，且现有方法依赖数据且任务特定。需要一种能够统一时间检测、空间定位和语义解释的通用框架。

Method: 基于链式测试时推理过程，通过任务内推理优化时间检测，任务间链接实现空间和语义理解，在完全零样本设置下利用基础模型的推理能力。

Result: 在多个视频异常检测、定位和解释基准测试中实现了最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示与任务链式组合能够解锁基础模型的推理能力，实现实用且可解释的视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [99] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专门用于2D血管分割的高效框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，显著提升了血管分割性能，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 血管分割在临床应用中至关重要，但由于血管结构细薄、分支复杂且纹理对比度低，现有通用分割模型（如SAM）在血管分割上表现不佳。

Method: VesSAM框架包含三个核心组件：卷积适配器增强局部纹理特征；多提示编码器通过分层交叉注意力融合解剖学提示（骨架、分叉点、段中点）；轻量级掩码解码器减少锯齿伪影。还引入了自动化多提示标注管道。

Result: 实验结果显示，VesSAM在8个数据集上比最先进的PEFT-based SAM变体在Dice系数上提升超过10%，IoU提升13%，与完全微调方法性能相当但参数更少，且在分布外设置下表现最佳。

Conclusion: VesSAM为血管分割提供了一个强大且高效的解决方案，在保持轻量级的同时实现了卓越的分割性能，并具有良好的泛化能力。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [100] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的自监督多模态迭代去噪（MID）框架，通过建模非线性噪声积累过程，无需配对干净-噪声数据集即可有效去除复杂非线性噪声。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据经常被复杂的非线性噪声污染，传统的基于规则的去噪方法难以应对这种挑战。

Method: MID将收集的噪声数据建模为非线性噪声积累过程中的一个状态，通过迭代引入更多噪声来学习两个神经网络：一个估计当前噪声步骤，另一个预测并减去相应的噪声增量。对于复杂非线性污染，使用一阶泰勒展开局部线性化噪声过程。

Result: 在四个经典计算机视觉任务上的实验表明MID具有鲁棒性、适应性和最先进的性能。在生物医学和生物信息学领域任务中也表现出强大的性能和适应性。

Conclusion: MID框架能够有效处理复杂非线性噪声，无需配对数据集，在多个领域都表现出优异的去噪性能。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [101] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 提出了一种多模态机器学习框架，通过统一特征提取技术应用于戈雅画作的视觉和X射线图像，在艺术认证中取得了97.8%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决戈雅作品艺术认证的复杂计算挑战，包括其异质风格演变和广泛的历史伪造模式。

Method: 使用统一特征提取管道（包括灰度共生矩阵描述符、局部二值模式、熵度量、能量计算和颜色分布分析）处理视觉和X射线图像，通过优化的单类支持向量机进行分类。

Result: 在24幅认证戈雅画作数据集上，使用80/20训练测试配置和10折交叉验证，达到97.8%分类准确率和0.022假阳性率。案例研究显示对"Un Gigante"达到92.3%认证置信度。

Conclusion: 多模态方法相比单模态方法有显著性能提升，证明在艺术认证应用中统一计算方法对视觉和放射图像的有效性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [102] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net是一种混合CNN-Transformer架构，用于乳腺癌超声图像的同步分割和分类，具有内在可解释性。在BUSI数据集上表现优异，并通过交叉数据集研究验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决B型超声乳腺癌诊断面临的挑战：斑点噪声、操作者依赖性和边界模糊。现有深度学习方法存在单任务学习、架构限制（CNN缺乏全局上下文，Transformer缺乏局部特征）和黑盒决策等问题，阻碍了临床采用。

Method: 提出HyFormer-Net混合架构，双分支编码器集成EfficientNet-B3和Swin Transformer，通过多尺度层次融合块连接。使用注意力门控解码器提供精度和可解释性。引入双流水线可解释性：内在注意力验证和Grad-CAM分类推理。

Result: 在BUSI数据集上：Dice Score 0.761±0.072，准确率93.2%，优于U-Net、Attention U-Net和TransUNet。恶性召回率92.1±2.2%。集成模型达到Dice 90.2%，准确率99.5%，恶性召回率100%。多尺度融合贡献+16.8% Dice，注意力门控贡献+5.9%。交叉数据集泛化研究中，使用50%目标域数据达到77.3% Dice，超过源域性能。

Conclusion: HyFormer-Net在乳腺癌超声诊断中实现了优异的性能，通过混合架构解决了现有方法的局限性。交叉数据集研究证明了其强大的泛化能力，仅需少量目标域数据即可恢复性能，为临床部署提供了可行性。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [103] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost是一种参数高效的神经网络架构，通过动态缩放渐进注意力机制在CIFAR基准测试中达到最先进性能，显著减少参数数量同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备部署深度学习模型时面临的参数效率与准确率之间的权衡问题，旨在实现更小的模型尺寸和更低的计算成本而不损失性能。

Method: 采用动态缩放渐进注意力机制，包含自适应融合、阶段缩放和残差自适应三个核心创新，结合增强的MBConv模块，实现双注意力路径和实时权重调整。

Result: 在CIFAR-10上达到95.57%准确率（0.85M参数）和93.80%（0.37M参数），在CIFAR-100上达到81.37%准确率（0.92M参数）和74.85%（0.44M参数），相比MobileNetV3减少2.1倍参数同时提升3.2个百分点准确率。

Conclusion: FastBoost通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，为资源受限边缘设备部署提供了可行的解决方案。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [104] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出了T-MLA攻击框架，这是首个针对神经图像压缩系统的目标多尺度对数-指数攻击方法，通过在小波域中精心设计对抗性扰动来显著降低重建图像质量，同时保持视觉不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 现有对神经图像压缩系统的对抗攻击大多是像素空间方法的简单改编，忽视了压缩管道的独特结构化特性，需要开发更先进的漏洞利用方法。

Method: T-MLA攻击框架在小波域中直接针对攻击和重建图像的质量来构建对抗性扰动，采用有原则的离线攻击策略，将扰动策略性地限制在特定小波子带中。

Result: 在标准图像压缩基准测试中对多个最先进的NIC架构进行广泛评估，显示重建质量大幅下降，同时扰动在视觉上保持不可察觉。

Conclusion: 研究揭示了生成和内容交付管道核心存在的关键安全漏洞。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [105] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出了一种分层序列预测方法用于图像地理定位，通过自回归方式从粗到细预测S2网格单元，结合波束搜索和多样本推理策略，在Im2GPS3k和YFCC4k数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中的挑战：不同位置的视觉相似性和大搜索空间，受人类从宽泛区域到具体地址的定位方式启发。

Method: 使用S2网格单元构建分层结构，通过自回归序列预测从粗到细的地理标记，结合波束搜索和多样本推理等语言模型技术。

Result: 在MLLM-free设置下超越其他基线方法，准确率提升高达13.9%；结合MLLM时在所有指标上达到新的最先进水平。

Conclusion: 分层序列预测方法有效解决了图像地理定位问题，通过自回归采样策略管理不确定性，在多个数据集上实现了最先进的性能。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [106] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I是一个用于网络切片特征可视化的合成数据集，通过四种编码方法将KPI向量转换为视觉表示，包含12万样本，适用于视觉学习和网络状态分析。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络切片需要精细的识别方法，但缺乏支持特征可视化的健壮数据集。

Method: 使用四种编码方法（物理启发映射、Perlin噪声、神经墙纸、分形分支）将多变量KPI向量转换为RGB图像，生成3万样本/方法。

Result: 创建了包含12万样本的公开数据集，模拟真实网络条件和噪声，支持网络状态分类和异常检测等任务。

Conclusion: SliceVision-F2I为网络切片的视觉学习提供了基准数据集，可促进图像化机器学习技术在网络数据分析中的应用。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [107] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出一种结合Epanechnikov核密度估计和双峰逻辑回归分类器的新方法，用于基于医学图像诊断呼吸系统疾病。


<details>
  <summary>Details</summary>
Motivation: 利用EKDE的灵活性建模数据分布，适应像素强度变化，从医学图像中提取关键特征以提高诊断准确性。

Method: 使用Epanechnikov非参数核密度估计与双峰逻辑回归分类器结合，在统计模型基础上构建学习方案。

Result: 在COVID-19放射影像数据集的13808张随机胸部X光片上测试，准确率70.14%，敏感度59.26%，特异度74.18%。

Conclusion: 该方法在呼吸系统疾病检测中表现中等，敏感度有待提升，但展示了EKDE方法在医学影像诊断中的潜力。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [108] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 提出了ViACT框架，将解剖先验直接集成到transformer架构中，通过掩码自编码策略专注于解剖区域学习，提高超声心动图分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频transformer在超声心动图分析中容易学习来自非诊断区域（如图像背景）的虚假相关性，需要克服这一限制。

Method: ViACT将变形解剖结构表示为点集，将其空间几何和对应图像块编码为transformer tokens，采用掩码自编码策略仅重建解剖区域。

Result: ViACT能够将transformer注意力集中在心肌区域，生成与已知病理区域对齐的可解释注意力图，并在左心室射血分数回归和心脏淀粉样变性检测任务中表现良好。

Conclusion: ViACT框架通过整合解剖约束，提高了超声心动图分析的准确性和可解释性，并能泛化到心肌点跟踪任务。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [109] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 该论文提出使用带有GPU的嵌入式设备来提升移动设备上计算机视觉应用的性能，通过边缘计算卸载高密集型任务，实验证明GPU相比CPU能获得性能增益。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的计算机视觉应用（特别是增强现实）对资源需求很高，而边缘计算设备通常容量有限，可能影响用户体验。

Method: 使用带有图形处理单元（GPU）的嵌入式设备来克服边缘计算设备的性能限制，通过GPU加速处理高密集型任务。

Result: 实验结果表明，与仅使用CPU相比，GPU能够获得性能增益，从而保证用户在使用此类应用时获得更好的体验。

Conclusion: 在边缘计算环境中使用GPU嵌入式设备可以有效提升计算机视觉应用的性能，改善用户体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [110] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种无需概念标注的弱监督概念预测框架PCP，利用类别级概念先验作为弱监督，在医学影像中实现可解释预测。


<details>
  <summary>Details</summary>
Motivation: 医学影像中可解释的AI预测很重要，但现有可解释设计框架需要昂贵的概念标注，而零样本方法难以捕捉医学领域特定特征。

Method: PCP框架利用类别级概念先验作为弱监督，结合KL散度和熵正则化的精炼机制来对齐临床推理。

Result: 在PH2和WBCatt数据集上，PCP相比零样本基线将概念级F1分数提高了33%以上，在四个医学数据集上达到与全监督方法竞争的分类性能。

Conclusion: PCP提供了一种无需概念标注的有效可解释预测方法，在医学影像领域具有实用价值。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [111] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv是一种用于惯性传感器人体活动识别的类别等变神经网络，通过系统编码时间、幅度和结构对称性，在分布外扰动下显著提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法未能系统性地编码数据中的对称性结构，限制了模型在分布外场景下的泛化能力和鲁棒性。

Method: 引入类别对称性乘积，将循环时间平移、正增益和传感器层次偏序集结合，构建类别对称性结构，实现对该对称性乘积的等变性。

Result: 在UCI-HAR数据集上，CatEquiv在分布外扰动下相比循环填充CNN和普通CNN获得了显著更高的鲁棒性。

Conclusion: 强制实施类别对称性可以在不增加模型容量的情况下实现强大的不变性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [112] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出MicroAUNet，一种轻量级注意力分割网络，结合深度可分离扩张卷积和参数共享的通道-空间注意力块，通过两阶段知识蒸馏实现实时结肠息肉分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习息肉分割模型要么提供模糊边界影响临床决策，要么依赖复杂架构导致推理速度不足，无法满足实时内窥镜应用需求。

Method: 使用深度可分离扩张卷积和单路径参数共享的通道-空间注意力块增强多尺度边界特征，并采用渐进式两阶段知识蒸馏从高容量教师网络转移语义和边界线索。

Result: 在基准测试中展现出最先进的准确性，同时保持极低的模型复杂度，适合实时临床息肉分割应用。

Conclusion: MicroAUNet在极低模型复杂度下实现了最先进的准确率，证明了其适用于实时临床息肉分割的可行性。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [113] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是一个评估统一多模态模型跨模态推理能力的新基准，重点关注文本和视觉模态之间的相互引导和验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将多模态能力孤立测试，缺乏对跨模态相互推理能力的评估，而这是实现真正统一多模态智能的核心能力。

Method: 构建包含1312个任务和1876张图像的人工标注基准，涵盖两个互补设置：文本增强的视觉生成推理和视觉增强的文本生成推理。

Result: 在17个统一模型上的实验显示：跨模态推理决定视觉生成质量，交错模型显著优于非交错模型；模型在物理和符号推理之间存在分离。

Conclusion: 相互跨模态推理是实现真正全模态生成的关键前沿，现有模型在构建视觉抽象方面仍有不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [114] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 提出了一个自动化流程从YouTube视频中提取动物中心剪辑，构建了包含30K视频（200万帧）的大规模数据集，并创建了包含230个序列的Animal-in-Motion基准测试，用于4D四足动物重建任务。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模有限（仅2.4K 15帧剪辑），缺乏动物中心3D/4D任务所需的关键处理，需要从野外视频中实现大规模、无标记的4D动物重建。

Method: 开发自动化流程从YouTube视频中挖掘并处理成对象中心剪辑，提供姿态估计、跟踪和3D/4D重建等下游任务所需的辅助标注。

Result: 收集了30K视频（200万帧），比先前工作多一个数量级；创建了包含230个手动筛选序列的Animal-in-Motion基准测试；通过序列级优化改进了模型无关方法，建立了首个4D动物重建基线。

Conclusion: 该流程、基准测试和基线旨在推进从野外视频中进行大规模、无标记的4D动物重建及相关任务，代码和数据集已开源。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [115] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出DTWSR方法，结合扩散模型和Transformer，通过多级离散小波变换和多尺度频率子带关联建模，实现更一致和真实的图像超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的图像超分辨率方法大多忽视了多尺度频率子带间的相互关系，导致重建图像存在不一致性和不自然伪影。

Method: 使用多级离散小波变换分解图像为小波谱，提出金字塔标记化方法将谱嵌入为Transformer序列，设计双解码器分别处理低频和高频子带，同时保持它们在图像生成中的对齐。

Result: 在多个基准数据集上的广泛实验表明，该方法在感知质量和保真度方面都表现出高性能。

Conclusion: DTWSR通过有效建模多尺度频率子带间关系，实现了更一致和真实的图像超分辨率重建。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [116] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出了一种基于拓扑感知图卷积网络（GCN-PSN）的动作质量评估方法，通过建模人体骨架图来学习具有区分度的姿态嵌入，在AQA-7和FineDiving基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要精细理解人体运动并精确评估姿态相似性，现有基于坐标的方法可能无法充分利用骨架拓扑结构信息。

Method: 使用拓扑感知的图卷积网络（GCN）将人体骨架建模为图结构，采用孪生网络架构和对比回归目标进行训练，学习判别性的姿态嵌入。

Result: 在AQA-7和FineDiving基准测试中超越了基于坐标的基线方法，取得了有竞争力的性能，实验验证了利用骨架拓扑进行姿态相似性评估的有效性。

Conclusion: 骨架拓扑结构对于动作质量评估至关重要，GCN-PSN框架能够有效学习拓扑敏感的姿态表示，提升动作质量评估性能。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [117] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个新颖的分层运动生成框架，通过多尺度标记保留策略和可扩展自回归建模，实现了高效的文本驱动3D人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法在文本驱动3D人体运动生成中存在推理步骤多、效率低的问题，需要改进VQ-GT范式以实现更高效的生成过程。

Method: 提出多尺度标记保留策略(MTPS)和分层残差向量量化变分自编码器(RQ-VAE)，结合可扩展自回归建模(SAR)和轻量级卷积注意力混合VQ-VAE(CAQ-VAE)。

Result: 在Motion-X数据集上，MoSa实现了0.06的FID(优于MoMask的0.20)，同时推理时间减少27%，在生成质量和效率方面均达到最先进水平。

Conclusion: MoSa框架通过分层运动生成方法，在保持高质量生成的同时显著提升了效率，并能很好地泛化到运动编辑等下游任务。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [118] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个多模态视觉-语言-动作模型，通过整合红外相机、毫米波雷达和麦克风阵列等新型传感模态，超越了仅依赖RGB相机的感知限制，在真实世界操作任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖RGB相机，这限制了其感知能力和操作能力。需要整合更多传感模态来提供物理基础的空间智能。

Method: 提出传感器掩码图像的统一表示方法，将空间基础和物理意义的掩码叠加到RGB图像上。基于RGB预训练的VLA骨干网络，构建多感官视觉-语言-动作模型架构，使用轻量级传感器投影器实现数据高效学习。

Result: 在需要传感器模态感知指导操作的真实世界任务中，OmniVLA平均任务成功率达到84%，显著优于仅RGB模型（提升59%）和原始传感器输入基线模型（提升28%），同时表现出更高的学习效率和更强的泛化能力。

Conclusion: OmniVLA通过整合多传感模态，显著提升了视觉-语言-动作模型的感知和操作能力，证明了多模态融合在机器人操作任务中的重要性。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [119] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 该论文针对印度食物VQA系统，提出了一种多步推理链方法，通过自动生成推理链并利用强化学习训练模型，在基准测试中平均提升了10个百分点的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统偏向西方食物，无法处理印度食物的文化多样性和复杂烹饪背景。现有印度食物VQA数据集采用两步法生成答案和解释，但作者认为需要多步推理过程才能准确理解印度食物。

Method: 创建自动验证的推理链，对较小的LLM和VLM进行微调，然后使用强化学习进行大规模数据训练。

Result: 通过添加推理链，在基准测试中平均准确率提高了10个百分点。

Conclusion: 推理链的加入显著提升了印度食物VQA任务的性能，证明了多步推理在理解复杂烹饪背景中的重要性。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [120] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 该论文研究了通过翻转数据预训练和微调的方法，将端到端自动驾驶模型PilotNet从左舵驾驶条件适应到右舵驾驶条件，发现翻转数据预训练结合微调能显著提升模型适应性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要适应不同道路条件，特别是左舵和右舵驾驶的差异，以实现良好的泛化能力。

Method: 评估了四种训练方法：美国右舵数据基线模型、翻转美国数据训练、美国数据预训练+澳大利亚高速公路微调、翻转美国数据预训练+澳大利亚高速公路微调。

Result: 仅翻转数据预训练会降低预测稳定性，但结合微调后能显著降低预测误差并增强对左侧线索的关注。ResNet架构实验验证了相似趋势。

Conclusion: 翻转数据预训练结合微调是提高模型适应性的有效方法，能减少重新训练需求。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [121] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出Eyes on Target框架，通过将人眼注视特征注入Vision Transformer的注意力机制，在自我中心视频中实现注视引导的物体检测。


<details>
  <summary>Details</summary>
Motivation: 人类注视为理解复杂视觉环境中的注意力提供了丰富的监督信号，传统物体检测器平等对待所有区域，而本方法强调观察者优先关注的区域。

Method: 在Vision Transformer的注意力机制中注入注视衍生特征，偏向选择人类注视区域的空间特征，并引入注视感知注意力头重要性指标。

Result: 在自定义模拟器数据集和公共基准测试（Ego4D Ego-Motion和Ego-CH-Gaze数据集）上，相比无注视基线模型检测精度持续提升。

Conclusion: 该方法通过注视引导有效提升了物体检测性能，在模拟场景中评估人类表现具有潜力，并揭示了注视线索如何调节transformer注意力动态。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [122] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 提出了一种基于双阶信息的黑盒梯度可迁移攻击方法，通过引入对抗平坦性(AF)解决欺骗性平坦问题，并开发了对抗平坦攻击(AFA)和蒙特卡洛对抗采样(MCAS)来提升攻击能力。


<details>
  <summary>Details</summary>
Motivation: 当前可迁移攻击方法虽然关注平坦损失，但仍陷入次优区域（特别是平坦但尖锐的区域，称为欺骗性平坦），导致对抗样本的可迁移性不足。

Method: 1. 引入对抗平坦性(AF)概念解决欺骗性平坦问题；2. 基于目标函数的高效近似实例化为对抗平坦攻击(AFA)；3. 设计蒙特卡洛对抗采样(MCAS)提升内循环采样效率。

Result: 在ImageNet兼容数据集上的综合结果表明，该方法优于六个基线方法，生成的对抗样本位于更平坦区域，跨模型架构的可迁移性显著提升。在输入变换攻击和百度云API测试中也优于基线方法。

Conclusion: 该方法通过双阶信息视角解决了可迁移攻击中的欺骗性平坦问题，显著提升了对抗样本的可迁移性，在各种测试场景下都表现出优越性能。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [123] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM是一个用于脑部病变分割的端到端框架，通过冻结预训练主干网络并仅训练轻量级适配器实现高效微调。其核心是CenterMamba编码器，采用3x3角-轴-中心短序列扫描策略，增强对弱边界和小病灶的敏感性。


<details>
  <summary>Details</summary>
Motivation: 脑部病变分割面临小病灶、低对比度、各向异性采样和跨切片不连续性的挑战，需要更有效的分割方法。

Method: 提出CenterMamba编码器使用3x3角-轴-中心扫描策略；内存驱动的结构提示生成器维护原型库；内存增强的多尺度解码器集成内存注意力模块。

Result: 在公共基准测试上的广泛实验表明，CenterMamba-SAM在脑部病变分割中实现了最先进的性能。

Conclusion: 该框架通过中心优先、轴强化和对角补偿的信息聚合，有效提升了脑部病变分割的准确性和效率。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [124] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量级几何感知适配器，通过方位对齐和水平循环填充来保持LiDAR点云边界连续性，使用局部K近邻计算几何特征，在训练时驱动区域感知正则化，提升恶劣天气下LiDAR语义分割的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气下LiDAR语义分割性能下降，因为折射、散射和点丢失会破坏几何结构。现有方法忽略了边界、角落和稀疏区域的结构脆弱性问题。

Method: 设计几何感知适配器，包括方位对齐、水平循环填充、局部窗口K近邻计算局部统计特征，在训练时使用这些几何线索驱动区域感知正则化。

Result: 在跨天气设置下，相比数据增强基线提升7.9个百分点mIoU，相比类别中心正则化基线提升0.6个百分点mIoU。

Conclusion: 几何驱动的正则化是全天候LiDAR分割的关键方向，该适配器即插即用，训练时启用，推理成本可忽略。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [125] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream实现了亚秒级延迟的视频生成，通过将双向教师模型蒸馏为因果学生模型，结合滑动窗口因果注意力和注意力汇技术，支持实时无限长视频流生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有运动条件视频生成方法延迟过高（分钟级）和非因果处理无法实现实时交互的问题。

Method: 通过自强制分布匹配蒸馏将双向教师模型蒸馏为因果学生模型，采用滑动窗口因果注意力和注意力汇技术，结合自展开和KV缓存滚动训练。

Result: 在单GPU上实现最高29 FPS的流式生成，运动跟随和视频质量达到最先进水平，速度提升两个数量级，支持无限长度流式生成。

Conclusion: MotionStream实现了真正交互式的实时视频生成体验，用户可以通过绘制轨迹、控制相机或传输运动来实时查看结果。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [126] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor是一个基于先验指导的颜色转换器，通过学习明清时期画作来恢复唐宋时期古画的颜色，通过亮度增强和色调校正两个子任务实现古画色彩复原。


<details>
  <summary>Details</summary>
Motivation: 中国古代绘画是宝贵的文化遗产，但存在不可逆的色彩退化问题。由于复杂的化学机制和缺乏高质量数据集，古画色彩复原非常困难，阻碍了端到端数字修复工具的开发。

Method: 将颜色恢复分解为亮度增强和色调校正两个顺序子任务。亮度增强使用两个变分U-Net和多尺度映射模块；色调校正设计双分支颜色查询模块，一个分支基于局部色调先验进行局部校正，另一个分支保持全局推理能力。

Result: 与最先进的着色方法进行广泛实验对比，在定量和定性评估中都表现出优越性能。

Conclusion: PRevivor能够有效恢复古画色彩，通过先验指导和双任务分解策略实现了更好的色彩复原效果。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [127] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 这篇综述系统评估了基础模型在医学影像分析中的适应策略，包括监督微调、参数高效微调、自监督学习等方法，并指出了当前面临的领域偏移、数据稀缺、计算需求和隐私保护等挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析中展现出巨大潜力，但将其成功应用于临床实践仍面临领域偏移、高质量标注数据稀缺、计算资源需求大和隐私保护要求严格等关键挑战。

Method: 综述了多种适应策略：监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态/跨模态框架，并对每种方法的性能增益、临床适用性和局限性进行了评估。

Result: 识别了现有方法的权衡和未解决的挑战，同时强调了新兴研究方向，包括持续学习、联邦学习、混合自监督学习、数据为中心的流程和系统基准测试。

Conclusion: 通过概述这些策略和相关研究空白，为开发能够满足真实世界医学影像需求的适应性、可信赖且临床集成的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [128] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出了一种基于数据流形几何差异的图像生成检测框架，利用自然图像和生成图像在梯度子空间的正交性，通过自监督模型损失变化来识别生成图像，并使用归一化流放大可检测差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实感不断提升，其潜在滥用风险日益严重，需要鲁棒的检测方法。现有方法主要依赖二元分类器，但受限于生成图像的数量和质量。

Method: 利用自然图像和生成图像数据流形的几何差异，设计函数对使自然图像输出一致而生成图像输出发散，基于梯度正交性原理。通过自监督模型在图像变换时的损失变化进行检测，并使用归一化流放大流形差异。

Result: 大量实验证明了该方法的有效性，代码已开源。

Conclusion: 该方法提供了一种简单有效的生成图像检测方案，特别针对先进生成模型中流形差异减小的问题，通过流形外推增强了检测能力。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [129] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了UniREditBench基准测试，用于评估基于推理的图像编辑模型性能，涵盖真实世界和游戏世界场景，包含2700个样本和8个主要维度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在需要隐式推理的复杂图像编辑任务中表现不佳，现有基准测试主要关注单对象属性变换，忽视了多对象交互和游戏世界场景，且仅依赖文本参考评估可能导致误判。

Method: 构建了包含2700个样本的统一基准测试UniREditBench，采用多模态双参考评估（文本和真实图像参考），设计了自动多场景数据合成流程，创建了包含10万样本的合成数据集UniREdit-Data-100K，并在其上微调Bagel模型得到UniREdit-Bagel。

Result: UniREdit-Bagel在领域内和领域外设置中都表现出显著改进，通过对开源和闭源图像编辑模型的全面基准测试，揭示了它们在不同方面的优势和弱点。

Conclusion: UniREditBench为基于推理的图像编辑提供了全面的评估框架，多模态双参考评估提高了评估可靠性，合成的数据集和微调模型为复杂图像编辑任务提供了有效解决方案。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [130] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出REASON框架，通过两阶段概率图引导的双分支融合方法自动评估胃内容物，显著提升麻醉诱导时误吸风险评估的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 传统胃内容物超声评估依赖手动追踪和经验公式，在效率和准确性方面存在显著局限，需要更自动化和精确的解决方案

Method: 两阶段框架：第一阶段使用分割模型生成概率图抑制伪影并突出胃解剖结构；第二阶段双分支分类器融合右侧卧位和仰卧位两个标准视图信息，提升特征判别能力

Result: 在自收集数据集上的实验结果表明，该框架显著优于当前最先进方法，性能提升明显

Conclusion: 该框架为自动化术前误吸风险评估提供了更稳健、高效和准确的解决方案，在临床实践中具有巨大潜力

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [131] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种用于全切片病理图像表示的三阶段解缠学习框架，通过潜在因子分组、聚类推理和实例效应重加权来解决空间、语义和决策纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 多实例学习在病理图像表示中存在空间、语义和决策纠缠问题，限制了表示能力和可解释性。

Method: 三阶段框架：1）正半定潜在因子分组缓解空间纠缠；2）实例概率反事实推理和聚类推理解缠缓解语义纠缠；3）广义线性加权决策通过实例效应重加权解决决策纠缠。

Result: 在多中心数据集上的实验表明，该模型优于所有最先进模型，并通过解缠表示和透明决策过程实现了病理学家对齐的可解释性。

Conclusion: 该框架有效解决了病理图像表示中的纠缠问题，在性能和可解释性方面均取得了显著提升。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [132] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出了APDM框架，通过将保护目标从图像转移到扩散模型本身来防止未经授权的个性化生成，包含DPO损失函数和L2P双路径优化策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能够生成特定主体（如身份或物体）的高质量内容，这带来了隐私风险，因为个性化技术可能被恶意用户滥用。现有方法依赖不现实的假设，在少量干净图像或简单图像变换下失效。

Method: 1. 理论分析显示现有损失函数无法确保鲁棒反个性化收敛；2. 提出DPO损失函数，在不影响生成质量的前提下有效干扰目标模型的个性化；3. 提出L2P双路径优化策略，通过交替个性化与保护路径来模拟未来个性化轨迹并自适应强化保护。

Result: 实验结果表明该框架优于现有方法，在防止未经授权个性化方面达到最先进性能。

Conclusion: APDM框架通过模型层面的保护机制，有效解决了扩散模型个性化带来的隐私风险问题，为内容创作中的隐私保护提供了新思路。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [133] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba是首个基于Mamba架构的多视角立体视觉网络，通过动态Mamba模块实现高效的全局特征聚合，在DTU和Tanks-and-Temples数据集上取得了优于现有方法的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer-based MVS方法存在二次复杂度问题，难以平衡性能与效率。Mamba架构具有全局建模能力和线性复杂度，为解决这一问题提供了新思路。

Method: 提出MVSMamba网络，采用动态Mamba模块，基于参考中心动态扫描策略，实现高效的视图内和视图间特征交互、全方位多视角特征表示以及多尺度全局特征聚合。

Result: 在DTU数据集和Tanks-and-Temples基准测试中，MVSMamba超越了最先进的MVS方法，在性能和效率方面都表现出色。

Conclusion: MVSMamba成功将Mamba架构应用于MVS任务，证明了其在全局特征聚合方面的优势，为MVS研究提供了新的高效解决方案。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [134] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP模型的生成对抗攻击方法，能够创建视觉上难以察觉但高度有效的对抗性扰动，在多目标场景中欺骗多标签分类器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到对抗性攻击，微小的扰动就能导致模型预测错误。现有方法在保持视觉质量和攻击效果方面存在不足。

Method: 结合CLIP模型的文本-图像对齐能力，集成SSAE的集中扰动策略和GAMA的差异文本嵌入，使用引导损失生成对抗样本。

Result: 在多种黑盒受害者模型上测试，该方法在保持竞争性攻击成功率的同时，具有更好的视觉保真度。

Conclusion: 该方法成功地将自然语言语义与对抗攻击相结合，在欺骗分类模型的同时保持了原始图像的高结构相似性。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [135] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出RDTE-UNet分割网络，通过结合局部建模和全局上下文来增强边界描绘和细节保持，在医学图像分割中取得了较好的效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对于计算机辅助诊断和治疗规划至关重要，但显著的解剖变异性和边界模糊性阻碍了对精细结构的可靠描绘。

Method: 采用混合ResBlock细节感知Transformer主干网络，包含三个模块：ASBE用于自适应边界增强，HVDA用于细粒度特征建模，EulerFF用于基于欧拉公式的融合加权。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割准确性和边界质量方面达到了可比较的水平。

Conclusion: RDTE-UNet通过统一局部建模与全局上下文，提高了跨形态、方向和尺度的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [136] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英语Rebus谜题的大型基准测试集，并开发了一个模型无关的框架RebusDescProgICE，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前的视觉语言模型来说是一个具有挑战性的任务。

Method: 提出了RebusDescProgICE框架，结合非结构化描述和基于代码的结构化推理，并采用基于推理的上下文示例选择策略。

Result: 相比思维链推理，该框架在闭源模型上提升了2.1-4.1%的性能，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题上的推理能力，证明了结合多种推理方式的优势。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [137] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D是一个多实例3D分割框架，通过竞争性查询优化策略实现从单点单对象到单点多实例的转变，能够从单个示例点检索整个3D体积中所有语义相似的病变。


<details>
  <summary>Details</summary>
Motivation: 解决SAM交互式分割中单点单对象范式限制多病变分割的问题，以及ViT骨干网络捕获全局上下文但缺乏高保真局部细节的局限性。

Method: 使用提示条件实例查询生成器将单点提示转换为多个专用查询；采用混合CNN-Transformer编码器通过空间门控将CNN边界显著性注入ViT自注意力；通过竞争优化查询解码器实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21数据集上，MIQ-SAM3D达到了可比较的水平，并展现出对提示的强鲁棒性。

Conclusion: 为临床相关多病变病例的高效标注提供了实用解决方案。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [138] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 提出了一种通过内容-风格子空间混合和平衡损失来扩展内容-风格前沿的新方法，解决了风格强度增加时内容特征丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在单一风格强度下评估内容相似性，但实验发现增加风格强度会导致内容特征显著丢失，形成次优的内容-风格前沿。

Method: 采用内容-风格子空间混合技术和内容-风格平衡损失函数，在保持风格多样性的同时提升内容相似性。

Result: 在定性和定量评估中均优于现有技术，实现了更优的内容-风格权衡，显著降低了IGD和GD得分。

Conclusion: 该方法有效扩展了内容-风格前沿，在多种风格强度下都能保持更好的内容相似性。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [139] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了CMI-MTL框架，通过细粒度视觉文本特征对齐、跨模态交错特征表示和自由形式答案增强多任务学习，解决医学视觉问答中的跨模态语义对齐和自由形式答案多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力方法难以有效处理视觉与语言之间的跨模态语义对齐，分类方法依赖预定义答案集，无法适应自由形式答案的多样性，且忽略了详细语义信息。

Method: CMI-MTL包含三个关键模块：细粒度视觉文本特征对齐(FVTA)提取最相关区域，跨模态交错特征表示(CIFR)捕获跨模态序列交互，自由形式答案增强多任务学习(FFAE)利用开放性问题辅助知识。

Result: 在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上优于现有最先进方法，并通过可解释性实验验证了有效性。

Conclusion: CMI-MTL框架有效解决了医学视觉问答中的跨模态语义对齐和自由形式答案处理问题，为临床决策支持和远程医疗提供了更好的解决方案。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [140] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种用于生成水下环境中AUV搭载事件相机合成数据的管道，用于训练视觉模型，并在岩石检测任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光照条件差、高动态范围场景等挑战，传统视觉技术难以适应。事件相机通过逐帧跟踪变化来缓解这些问题，但缺乏合适的训练数据。

Method: 开发了一个能够生成AUV搭载事件相机在水下环境中真实合成数据的管道，特别针对能见度差和悬浮颗粒物的情况。

Result: 在岩石检测任务中证明了该管道的有效性，能够处理恶劣的能见度和悬浮颗粒物条件。

Conclusion: 该合成数据生成管道可推广到其他水下任务，为事件相机在水下环境中的应用提供了有效的训练数据解决方案。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [141] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 提出了SEPS框架，通过两阶段机制整合密集和稀疏文本的统一语义，解决视觉-语言跨模态对齐中的补丁冗余和模糊性问题，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理视觉补丁冗余和模糊性方面存在挑战，MLLMs的密集文本输出可能与原始稀疏描述冲突，且难以准确量化视觉补丁与文本描述之间的语义相关性。

Method: 采用两阶段机制：整合密集和稀疏文本的统一语义以识别显著视觉补丁；利用均值计算的相关性感知选择来突出关键补丁-词对应关系，改进跨模态相似性评估。

Result: 在Flickr30K和MS-COCO数据集上的实验表明，SEPS在不同模型架构下rSum指标超越现有方法23%-86%，在文本到图像检索场景中表现尤为突出。

Conclusion: SEPS框架有效解决了跨模态对齐中的补丁冗余和模糊性问题，通过语义增强的补丁精简实现了显著的性能提升，为视觉问答等应用提供了坚实基础。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [142] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 该研究提出了Fire-ART数据集和基于全景图像的消防资产重建方法，用于将消防资产语义丰富到BIM模型中，提高消防资产管理的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统消防资产管理方法在自动资产识别和重建方面能力有限，效率低下。需要开发更有效的技术来支持应急准备、风险评估和现场火灾响应。

Method: 开发了包含15种基础资产、2626张图像和6627个实例的Fire-ART数据集，并采用改进的立方体贴图转换和基于半径的球面相机投影的重建方法。

Result: 通过两个真实案例验证，该方法分别达到73%和88%的F1分数，定位误差分别为0.620米和0.428米。

Conclusion: Fire-ART数据集和重建方法为消防设备的精确数字化管理提供了宝贵资源和稳健的技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [143] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出了一种基于可调平滑轮廓的训练无关解释方法，用星凸区域替代密集扰动掩码，通过傅里叶级数参数化，在极值保留/删除目标下优化分类器梯度，生成单一连通掩码。


<details>
  <summary>Details</summary>
Motivation: 解决现有密集扰动掩码解释方法存在的碎片化、过拟合问题，需要复杂后处理，寻求更紧凑、可解释且稳定的视觉模型解释方法。

Method: 使用截断傅里叶级数参数化星凸区域，在极值保留/删除目标下优化分类器梯度，保证单一连通掩码，大幅减少自由参数，实现稳定边界更新。

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，产生紧凑可解释区域，提高运行一致性，在基准测试中比梯度和扰动基线获得更高相关质量和更低复杂度。

Conclusion: 该方法通过低维平滑轮廓生成鲁棒解释，特别在自监督DINO模型上提升相关质量15%以上，保持正相关性，支持多轮廓扩展实现多目标定位。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [144] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于蒸馏的高效因果视频生成框架，通过对抗性自蒸馏策略和首帧增强技术，在极少的去噪步骤（1-2步）下实现高质量视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有混合视频生成模型结合自回归时间动态和基于扩散的空间去噪，但其顺序迭代特性导致错误累积和长推理时间，需要更高效的生成方法。

Method: 基于分布匹配蒸馏框架，提出对抗性自蒸馏策略，将学生模型的n步去噪输出与其(n+1)步版本在分布层面对齐；同时采用首帧增强策略，为首帧分配更多去噪步骤以减少错误传播。

Result: 在VBench上的广泛实验表明，该方法在一步和两步视频生成中均超越最先进方法，且单个蒸馏模型可灵活支持多步推理设置。

Conclusion: 该框架通过对抗性自蒸馏和首帧增强，显著提升了极少数步场景下的训练稳定性和生成质量，实现了高效高质量的视频合成。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [145] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT是一个统一的目标跟踪器，能够处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的组合，使用统一参数。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种视频模态和参考模态设计，导致模型分离且限制实际应用。需要统一的跟踪器来满足各种需求。

Method: 提出UniSOT统一跟踪器，能够同时处理三种参考模态和四种视频模态的不同组合。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT表现出优于模态特定对应方法的性能。在TNL2K上所有三种参考模态上AUC超过先前方法3.0%以上，在RGB+X视频模态上主要指标超过Un-Track 2.0%以上。

Conclusion: UniSOT展示了在多种参考模态和视频模态组合下的统一跟踪能力，性能优于专门的跟踪器。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [146] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种分辨率感知的token解码器，用于解决越野语义分割中的边界模糊、稀疏监督和标签噪声问题，通过平衡全局语义、局部一致性和边界保真度来提升性能。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临边界模糊、稀有类别稀疏监督和普遍标签噪声等挑战，现有方法在低分辨率融合时会模糊边缘，而高分辨率路径则计算成本高且对噪声敏感。

Method: 使用分辨率感知token解码器，大部分计算在低分辨率瓶颈进行；通过门控交叉注意力注入精细尺度细节；仅对稀疏的不确定性选择像素进行细化；结合边界带一致性正则化器进行训练。

Result: 实现了竞争性的性能表现，在过渡区域表现出改进的稳定性。

Conclusion: 该方法在保持计算效率的同时，有效解决了越野语义分割中的关键挑战，特别是在边界处理和噪声鲁棒性方面表现出色。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [147] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 提出了一种仅训练阶段使用的多目标方法，通过拉近同类特征和推开异类特征来锐化实例级决策边界，同时通过跨模态语义先验增强纹理贫乏的热红外特征，在保持单模态推理的同时提升夜间热红外检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决夜间热红外检测中的挑战：低对比度和弱高频线索导致重复检测框、重叠框、小目标漏检和类别混淆。现有方法要么将TIR转换为RGB（性能易受伪影影响），要么在测试时融合RGB和TIR（需要额外传感器和校准），都没有直接塑造检测器使用的热表示。

Method: 在训练阶段引入两个目标：1）通过拉近同类特征和推开异类特征来锐化实例级决策边界，抑制重复和混淆检测；2）通过将学生的多级金字塔特征与RGB训练的教师模型对齐，注入跨模态语义先验，增强纹理贫乏的热红外特征。

Result: 在实验中，该方法优于先前方法并实现了最先进的性能。

Conclusion: 通过仅训练阶段的目标函数，在保持单模态推理的同时有效解决了夜间热红外检测的关键问题，实现了更好的检测性能。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [148] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出了一种模型无关的序数元学习算法(MAOML)，用于训练小型视觉语言模型进行水果新鲜度分类，在零样本和少样本设置下达到92.71%的行业标准准确率。


<details>
  <summary>Details</summary>
Motivation: 解决易腐水果浪费问题需要准确预测新鲜度，但获取专家标注的细粒度标签成本高昂导致数据稀缺，而专有视觉语言模型存在数据隐私问题，开源模型性能不佳。

Method: 采用模型无关的序数元学习算法，结合元学习解决数据稀疏性问题，并利用标签的序数性来提升性能。

Result: 在水果新鲜度分类任务中，该方法在零样本和少样本设置下均达到最先进性能，平均准确率为92.71%。

Conclusion: MAOML算法能够有效训练小型视觉语言模型，在数据稀缺情况下实现与专有模型相当的性能，为水果新鲜度预测提供了可行的解决方案。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [149] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出Reg-DPO方法，通过自动构建GT-Pair偏好对和引入SFT损失作为正则化项，解决了视频生成中数据构建成本高、训练不稳定和内存消耗大的问题，在I2V和T2V任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要基于图像领域范式，且在小规模模型上开发，无法有效解决视频任务特有的挑战：数据构建成本高、训练不稳定、内存消耗大。

Method: 1) 提出GT-Pair自动构建高质量偏好对，使用真实视频作为正样本，模型生成视频作为负样本；2) 提出Reg-DPO，将SFT损失作为正则化项融入DPO目标；3) 结合FSDP框架和多种内存优化技术。

Result: 方法在多个数据集的I2V和T2V任务上持续优于现有方法，训练容量比单独使用FSDP提高近三倍，生成视频质量显著提升。

Conclusion: Reg-DPO方法有效解决了视频生成的独特挑战，无需外部标注即可实现高质量视频生成，在稳定性和效率方面均有显著改进。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [150] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该论文提出QA-SNNE方法，通过将问题语义融入预测置信度来改进手术视觉问答中的不确定性估计，提高模型的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在手术视觉问答中，错误或模糊的回答可能对患者造成伤害，因此需要关注模型的安全行为，如模糊意识、转诊专家或触发二次意见。现有研究多关注准确性而忽视了安全性。

Method: 引入问题对齐语义最近邻熵(QA-SNNE)，这是一种黑盒不确定性估计器，通过在医学文本嵌入空间中比较生成答案与最近邻来测量语义熵，并考虑问题上下文。

Result: 在EndoVis18-VQA和PitVQA数据集上评估，QA-SNNE在大多数模板内设置中提高了AUROC，零样本模型的AUROC提升了15-38%，幻觉检测能力增强，且在外模板压力下仍保持增益。

Conclusion: QA-SNNE为手术视觉问答中的自动故障检测提供了实用且可解释的步骤，将大型视觉语言模型与问题对齐的不确定性估计相结合可以提高安全性和临床医生的信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [151] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉变换器的后训练量化框架，通过将激活和权重量化误差建模为高斯噪声，并采用噪声注入优化方法获得平坦最小值，从而改善低比特量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法通常忽略训练好的神经网络与量化模型之间的关系，导致较大的量化误差。本文旨在解决如何高效训练适用于预定义精度低比特模型的模型无关神经网络。

Method: 首先发现平坦的全精度神经网络对低比特量化至关重要。提出通过测量和解耦误差源来主动预处理模型的框架，将激活量化误差和权重量化误差统计建模为独立高斯噪声，并研究多种噪声注入优化方法以获得平坦最小值。

Result: 实验结果证明了该方法的有效性，为获得低比特后训练量化模型开辟了新途径。

Conclusion: 平坦的全精度神经网络是低比特量化的关键，通过将量化误差建模为高斯噪声并采用噪声注入优化，可以有效改善后训练量化的性能。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [152] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff是一个基于扩散模型的即插即用解码框架，通过伪逆引导采样和自适应权重，显著提升深度联合源信道编码在对抗性无线环境中的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的JSCC框架容易受到物理层对抗威胁（如导频欺骗和子载波干扰），影响语义保真度，需要增强其安全性和鲁棒性。

Method: 采用伪逆引导采样和自适应指导权重实现灵活步长控制和高效语义重建；针对干扰攻击使用基于功率的子载波掩码策略；针对导频欺骗使用期望最大化驱动的重建算法。

Result: 在OFDM信道对抗条件下的广泛实验表明，SecDiff在重建质量和计算成本之间取得了良好平衡，优于现有安全和生成式JSCC基准方法。

Conclusion: SecDiff是实现实用、低延迟和抗攻击语义通信的有前景的一步。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [153] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 提出了增强行人对齐网络（EPAN），用于物联网监控环境下的人员重识别，在Inspection-Personnel数据集上取得了90.09%的Rank-1准确率和78.82%的mAP。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能环境中监控和安全应用的人员重识别问题，特别是在视角和环境变化条件下的鲁棒性需求。

Method: 采用双分支架构来减轻视角和环境变化的影响，在不同尺度和视角下提取对齐信息。

Result: 在Inspection-Personnel数据集上实现了90.09%的Rank-1准确率和78.82%的mAP，展示了强大的特征提取能力。

Conclusion: EPAN在现实物联网应用中具有潜力，能够在监控和安全系统中实现跨摄像头的有效可靠人员重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [154] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出了一种基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决了姿态模糊性和多假设问题


<details>
  <summary>Details</summary>
Motivation: 解决物体姿态估计中的部分可观测性、遮挡和物体对称性导致的姿态模糊性和多假设问题，传统确定性方法无法捕捉底层姿态分布的多模态特性

Method: 使用SE(3)流匹配的概率框架，建模完整的姿态分布，提供基于样本的姿态估计，能够处理对称物体或严重遮挡等模糊情况

Result: 在Real275、YCB-V和LM-O数据集上达到最先进水平，并展示了在下游机器人操作任务中的应用，如主动感知和不确定性感知的抓取合成

Conclusion: 该概率框架能够有效建模姿态分布的多模态特性，在模糊情况下提供不确定性推理，并在实际机器人应用中展现价值

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [155] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 提出DiMoDE框架，通过区分处理运动分量并利用其刚性流几何规律，改进深度和自运动联合学习，在多个数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度和自运动学习方法将自运动作为辅助任务，混合所有运动类型或排除深度无关的旋转运动，限制了强几何约束的融入，降低了可靠性和鲁棒性

Method: 引入运动分量的区分处理，通过相机光学轴和成像平面对齐，将光流转换并量化偏差，对每个自运动分量施加几何约束；将联合学习重新表述为同轴和共面形式，通过闭式几何关系相互推导深度和平移分量

Result: DiMoDE在多个公共数据集和新收集的多样化真实世界数据集上实现最先进性能，特别是在挑战性条件下表现优异

Conclusion: 通过区分处理运动分量并利用几何规律，DiMoDE框架显著提升了深度和自运动估计的可靠性和鲁棒性

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [156] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 提出Luminance-Aware Statistical Quantification (LASQ)框架，将低光图像增强重新定义为基于分层亮度分布的统计采样过程，通过扩散前向过程自主发现亮度层间的最优转换路径，实现无监督分布模拟。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注低光/正常光图像对之间的确定性像素级映射，忽略了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。

Method: 将亮度转换重新概念化为强度坐标空间中的幂律分布，用分层幂函数近似，用概率采样替代确定性映射。设计扩散前向过程自主发现亮度层间最优转换路径。

Result: 显著提高了实际场景中的性能，实现更适应和通用的光照恢复。在有正常光参考的情况下，在领域特定数据集上取得优异性能，并在非参考数据集上具有更好的泛化能力。

Conclusion: LASQ框架通过统计量化方法有效解决了低光图像增强中重建保真度与跨场景泛化之间的平衡问题，为实际应用提供了更灵活和鲁棒的解决方案。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [157] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 提出了一种新颖的对比学习框架NSYNC，通过生成负样本合成图像来改进大型文本到图像扩散模型的风格化能力，使用正交梯度更新消除正负样本中的共同特征。


<details>
  <summary>Details</summary>
Motivation: 当前文本条件图像生成方法虽然能生成逼真图像，但难以捕捉特定风格特征。直接在目标风格数据集上微调仍然难以掌握风格特征，需要更有效的训练方法。

Method: 利用合成图像生成技术创建负样本集，在对比训练中同时处理正负数据，通过从正梯度中减去其在负梯度上的投影来获得正交分量，以此更新模型参数。

Result: 在多种画家和插画师风格的实验中，该方法在定量和定性评估上都优于基线方法，能够更好地捕捉独特风格特征。

Conclusion: NSYNC框架通过对比学习和正交梯度更新有效提升了扩散模型的风格化能力，为文本到图像生成中的风格控制提供了新思路。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [158] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出了一种五层结构化模型来改进罕见驾驶场景的评估和生成，使用数据增强策略和基础模型生成新场景，并引入了多样性和原创性指标来评估合成数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 罕见和具有挑战性的驾驶场景对自动驾驶开发至关重要，但由于难以遇到，需要通过生成模型来模拟或生成这些场景。

Method: 使用五层结构化模型表示驾驶场景，为每个场景中的智能体引入子类和特征，通过特定嵌入进行比较，并采用数据增强策略和基础模型生成新场景。

Result: 展示了在不同生成设置下的多样性和原创性指标评估，以及对结构化场景描述生成的合成视频进行了定性评估。

Conclusion: 提出的结构化五层模型和评估指标能够有效改进罕见驾驶场景的生成和评估，为自动驾驶开发提供了有价值的工具。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [159] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出PCD-ReID算法解决基站环境中被遮挡行人重识别问题，通过Transformer网络提取共享组件特征，在真实巡逻监控数据集上训练，相比ResNet50方法Rank-1准确率提升15.9%。


<details>
  <summary>Details</summary>
Motivation: 基站环境中的遮挡行人重识别对监控安防应用至关重要，但传统ResNet方法无法有效处理遮挡问题，遮挡会掩盖关键身体特征，增加识别复杂度。

Method: 设计基于Transformer的PCD网络提取共享组件特征（如头盔、制服），收集6个月、1万人、5万张真实巡逻监控图像进行训练以避免公开数据集过拟合。

Result: 模型达到79.0%的mAP和82.7%的Rank-1准确率，相比基于ResNet50的方法Rank-1准确率提升15.9%。

Conclusion: PCD-ReID在塔检场景中有效实现遮挡感知的行人重识别性能，在监控安防应用中具有实际部署潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [160] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 开发了Napari Organoid Analyzer (NOA)，一个用于简化基于AI的类器官分析的通用图形用户界面工具，解决了生物学家使用AI工具的技术障碍。


<details>
  <summary>Details</summary>
Motivation: AI工具能显著增强类器官显微镜图像分析，但缺乏编程经验的生物学家难以使用这些工具，导致工作流程仍以手动为主。

Method: NOA整合了检测、分割、追踪、特征提取、自定义特征标注和基于机器学习的特征预测模块，作为开源napari插件实现，具有最大灵活性和可扩展性。

Result: 通过三个案例研究展示了NOA的多功能性：量化类器官分化过程中的形态变化、评估光毒性效应、预测类器官活力和分化状态。

Conclusion: NOA在可访问和可扩展的框架内实现了全面的AI驱动类器官图像分析。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [161] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA是首个支持像素级推理和多模态提示的视觉-语言-动作模型，通过新的视觉运动指令调优框架和两阶段自动标注流程，在三个标准VLA基准测试中比OpenVLA提升10.1%-17.8%的成功率，且仅需1.5%的预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个主要限制：(i)难以进行像素级场景理解，(ii)过度依赖文本提示，在真实环境中灵活性不足。

Method: 提出PixelVLA模型，采用新的视觉运动指令调优框架，集成多尺度像素感知编码器和视觉提示编码器，并通过两阶段自动标注流程生成Pixel-160K大规模像素级标注数据集。

Result: 在三个标准VLA基准测试和两种VLA模型变体上，PixelVLA比OpenVLA提升10.1%-17.8%的操作成功率，同时仅需1.5%的预训练成本。

Conclusion: PixelVLA可以集成到现有VLA中，在复杂环境中实现更准确、高效和通用的机器人控制。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [162] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出基于深度卷积生成对抗网络（DC-GAN）的合成MRI数据生成方法，并使用CNN分类器验证合成图像质量，结果表明合成图像在脑肿瘤分类任务中表现与真实图像相当。


<details>
  <summary>Details</summary>
Motivation: 原始MRI数据有限，需要生成合成医学图像来解决数据不足问题，特别是在医学成像领域。

Method: 使用深度卷积生成对抗网络（DC-GAN）生成合成MRI数据，并采用卷积神经网络（CNN）分类器对真实和合成图像进行脑肿瘤分类，以评估合成图像质量。

Result: 分类结果显示在真实图像和合成图像上的性能相当，验证了GAN生成图像在下游任务中的有效性。

Conclusion: GAN生成的合成医学图像能够有效解决数据有限问题，并在实际应用中表现出与真实数据相当的实用性。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [163] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出了连续-离散双元视觉分词器（CDD-VT），通过根据图像复杂度自适应分配不同数量的视觉基元，解决了多模态大模型中理解和生成统一化的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型中连续分词器和离散分词器的二分法问题，前者性能强但流程复杂，后者概念优雅但信息损失严重。

Method: CDD-VT将视觉数据视为量化码本中图像基元的灵活组合，包含两个核心组件：多样化量化基元（确保基元正交性）和动态基元分配器（根据样本复杂度自适应确定基元数量）。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT在性能上优于专门的连续和离散分词器，在简洁可扩展的MLLM中取得了强劲结果。

Conclusion: CDD-VT通过连续-离散双元方法有效统一了多模态大模型中的理解和生成，克服了传统方法在性能和复杂度之间的权衡问题。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [164] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM是一种轻量级肿瘤体积分割模型，能够从带有RECIST标注的CT扫描中高效地进行肿瘤体积分割，在MICCAI FLARE 2025比赛中取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤治疗评估主要依赖RECIST v1.1标准的手动直径测量，但体积测量更可靠。然而，手动体积标注耗时费力，限制了其临床应用。

Method: 提出了Lite ENSAM，这是ENSAM架构的轻量级适配版本，专门用于从带有RECIST标注的CT扫描中进行高效的肿瘤体积分割。

Result: 在MICCAI FLARE 2025任务1的子任务2中，Lite ENSAM在隐藏测试集上获得了60.7%的Dice相似系数和63.6%的归一化表面Dice，在公开验证数据集上平均总RAM时间为50.6GB，CPU推理时间为14.4秒。

Conclusion: Lite ENSAM提供了一种高效的肿瘤体积分割解决方案，有助于克服手动体积标注的限制，促进体积测量在临床中的更广泛应用。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [165] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的自监督视觉基础模型训练框架，整合了DINO系列的核心原理，支持多种Transformer架构和训练策略，显著降低计算成本的同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程存在不够灵活、领域特定或计算成本高的问题，限制了其在不同领域和资源环境下的应用。

Method: 采用统一配置驱动的系统，支持多种Transformer架构，集成LoRA、层冻结、知识蒸馏等训练策略，提供分布式训练支持(DDP/FSDP)，并包含可解释性工具和标签引导的数据增强方法。

Result: 在多样化数据集上的实验表明，DINO-MX在显著降低计算成本的同时实现了竞争力性能，无需额外检测或分割头即可改进基于注意力的定位。

Conclusion: DINO-MX为开发和基准测试自监督视觉模型提供了一个可复现、可扩展的基础，适用于广泛的研究和实际应用场景。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [166] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出PSPooling非学习性网格池化算子和MedShapeNet19基准数据集，用于解剖3D形状分类，通过自监督图自编码器在低标签情况下显著提升重建保真度和分类精度。


<details>
  <summary>Details</summary>
Motivation: 解剖3D形状分类受限于网格数据复杂性和缺乏标准化基准，需要鲁棒学习方法和可复现评估。

Method: 提出PSPooling基于几何邻近度预计算节点对应集，实现并行可逆池化和反池化操作；构建自监督图自编码器学习解剖感知表示；创建MedShapeNet19基准数据集。

Result: PSPooling在低标签情况下显著改善重建保真度和分类精度，为医学3D形状学习建立强基线。

Conclusion: PSPooling和MedShapeNet19为解剖形状分类提供有效工具和标准化基准，推动医学3D形状分析研究。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [167] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC是一个无需训练的通用框架，将列表重排序和融合重新定义为视觉语言模型的零样本推理任务，通过在提示中序列化内容证据和检索器元数据，实现跨模态视频检索的先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索器候选融合的长期挑战，特别是在视频等复杂多模态数据中。传统融合方法仅依赖排名或分数信号，忽视了候选表示。

Method: 提出Vote-in-Context框架，将列表重排序和融合作为VLM的零样本推理任务。使用S-Grid紧凑序列化地图表示视频，结合内容证据和检索器元数据进行自适应加权。

Result: 在视频检索基准测试中实现新的零样本检索SOTA，MSR-VTT上Recall@1达到87.1%(t2v)/89.0%(v2t)，VATEX上v2t达到99.6%，比之前SOTA基线提升高达+40 Recall@1。

Conclusion: ViC是将现代VLM转变为强大零样本重排序器和融合器的简单、可复现且高效的方案。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [168] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该论文提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，通过Viewpoint-100K数据集和两阶段微调策略，显著提升了模型在3D空间推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在2D视觉理解方面取得了进展，但它们在复杂3D推理任务中的有效性仍不明确，特别是跨视角一致性这一关键能力。

Method: 使用Viewpoint-100K数据集（包含10万张物体中心图像对），采用两阶段微调策略：首先通过监督微调注入基础知识，然后使用GRPO强化学习算法增强泛化能力，并引入混合冷启动初始化方法。

Result: 实验结果显示该方法显著激活了MLLM的空间推理能力，在领域内和领域外推理任务上均表现出性能提升。

Conclusion: 开发MLLM的基础空间技能对于推动机器人、自主系统和3D场景理解的未来发展具有重要价值。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [169] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习有效整合到基于扩散的图像修复模型中的方法，通过使用图像质量评估模型作为奖励函数，并针对困难样本进行动态训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法直接应用于扩散图像修复模型效果不佳，因为修复任务更强调保真度而非纯生成，需要专门设计适合修复目标的RL框架。

Method: 使用MLLM-based IQA模型作为奖励函数，针对远离真实值的困难样本进行强化学习训练，并采用自适应权重策略结合监督微调进行细粒度对齐。

Result: 该方法可即插即用地提升扩散修复模型性能，在多个基准测试中表现出有效性。

Conclusion: 提出的RL框架通过IQA奖励和动态训练策略，成功解决了扩散修复模型中的强化学习应用问题，显著提升了模型性能。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [170] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重光照框架，通过将RGB空间的几何反馈引入流匹配主干网络，显著提升了重光照的物理一致性，同时实现了20倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的重光照方法在语义潜在空间中优化，导致视觉空间中物理不正确的结果，如过曝高光、错位阴影和错误遮挡。

Method: 使用从输出中提取的深度和法线图进行监督，将光照效果与场景结构对齐；采用路径一致性学习减少计算开销；设计了六维标注协议实现细粒度控制。

Result: UniLumos在重光照质量上达到最先进水平，物理一致性显著改善，图像和视频重光照速度提升20倍。

Conclusion: UniLumos通过几何反馈和路径一致性学习，在保持高质量的同时大幅提升了重光照的效率和物理合理性。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [171] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出了一种渐进式网络架构，通过分阶段优化颜色和细胞边界生成，从H&E染色图像合成IHC等效图像，显著提升了视觉质量和结构细节。


<details>
  <summary>Details</summary>
Motivation: IHC染色虽然能提供高分辨率蛋白定位信息，但成本高、劳动密集且可扩展性有限。现有染色转换技术使用线性加权损失函数，无法同时保持结构真实性和颜色保真度。

Method: 基于ASP框架构建渐进式网络架构，引入颜色和细胞边界生成逻辑，添加DAB色原浓度和图像梯度损失函数，实现分阶段解耦优化。

Result: 在HER2和ER数据集上的实验表明，该模型显著改善了视觉质量，获得了更精细的结构细节。

Conclusion: 渐进式结构-颜色-细胞边界机制有效解决了现有染色转换技术的局限性，为从H&E图像高效获取蛋白级信息提供了可行方案。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [172] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出LFRD2框架，结合神经网络表达能力与物理模型可解释性，解决屏下ToF成像中的信号衰减、多路径干扰和时域噪声问题，通过时间分数阶反应-扩散模块实现深度精炼。


<details>
  <summary>Details</summary>
Motivation: 屏下ToF成像中，透明OLED层引入严重的信号衰减、多路径干扰和时域噪声，显著影响深度质量，需要有效解决方案。

Method: 提出可学习分数阶反应-扩散动力学(LFRD2)混合框架，包含时间分数阶反应-扩散模块实现动态微分阶数的迭代深度精炼，以及通过系数预测和重复微分的高效连续卷积算子。

Result: 在四个基准数据集上的实验证明了该方法的有效性。

Conclusion: LFRD2框架成功结合了神经网络的表达能力和物理模型的可解释性，有效提升了屏下ToF成像的深度质量。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [173] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench是首个专门评估不同鲁棒性训练方法对概率鲁棒性改进效果的基准测试，通过全面指标比较对抗训练和概率鲁棒性训练方法，发现对抗训练在提升对抗鲁棒性和概率鲁棒性方面更通用，而概率鲁棒性训练方法具有更低的泛化误差和更高的干净准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法对微小扰动非常脆弱。现有研究主要关注对抗鲁棒性，而概率鲁棒性从统计角度衡量模型在随机扰动下保持预测正确性的概率。虽然概率鲁棒性被认为是对抗鲁棒性的实用补充，但专门提升概率鲁棒性的训练方法仍相对较少探索。

Method: 提出PRBench基准测试，使用包括干净准确率、概率鲁棒性和对抗鲁棒性性能、训练效率和泛化误差在内的综合指标集，对常见的对抗训练和概率鲁棒性训练方法进行实证比较，并提供概率鲁棒性性能泛化误差的理论分析。

Result: 主要发现包括：对抗训练方法在提升对抗鲁棒性和概率鲁棒性方面比概率鲁棒性训练方法更具通用性，而概率鲁棒性训练方法始终产生更低的泛化误差和更高的干净准确率。构建了包含222个训练模型、覆盖7个数据集和10种模型架构的排行榜。

Conclusion: PRBench为概率鲁棒性训练方法的评估提供了标准化基准，揭示了不同类型训练方法的相对优势和局限性，对抗训练在鲁棒性提升方面更通用，而概率鲁棒性训练在泛化性能和干净准确率方面表现更好。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [174] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 开发了一个任务探索管道，使用聚类技术、因子分析和字符串编辑距离自动识别完成任务的关键全局和局部策略，以及有意义的子任务。


<details>
  <summary>Details</summary>
Motivation: 推进机器对用户知识、技能和行为的理解，以实现隐式协调，建立在预期性人机交互研究的基础上。

Method: 开发任务探索管道，结合聚类技术、因子分析和字符串编辑距离，自动识别全局策略（完成任务的动作集合）和局部策略（相似动作组合的序列），并识别不同长度的有意义的子任务。

Result: 任务探索管道能够自动识别完成任务的关键策略，并用层次化子任务结构编码用户运行轨迹。还开发了Task Explorer应用程序来方便查看管道结果。

Conclusion: 任务探索管道可轻松修改以适应任何基于动作的时间序列数据，识别出的策略和子任务有助于人类和机器了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [175] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR是一种改进的实时检测变换器，专为胸部X光肺炎检测设计，通过XFABlock、SPGA和GCFC3模块提升性能，在RSNA数据集上达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动检测系统。虽然基于变换器的检测器在目标检测任务中表现出色，但在医学影像特别是胸部X光肺炎检测中的应用仍待探索。

Method: 提出CGF-DETR模型：1）在骨干网络中引入XFABlock，通过卷积注意力机制与CSP架构改进多尺度特征提取；2）提出SPGA模块，用动态门控机制和单头自注意力替代标准多头注意力；3）设计GCFC3用于颈部网络，通过多路径卷积融合增强特征表示，同时通过结构重参数化保持实时性能。

Result: 在RSNA肺炎检测数据集上的实验表明，CGF-DETR达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的推理速度。完整模型达到50.4% mAP@[0.5:0.95]。

Conclusion: 消融研究证实每个提出的模块都对整体性能提升有显著贡献，CGF-DETR在肺炎检测任务中实现了优异的性能和实时推理能力的平衡。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [176] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个多平台、多模态的3D视觉语言基准数据集，包含车辆、无人机和四足机器人的RGB和LiDAR数据，提供超过128,000个物体和22,000个验证过的指代表达，规模是现有数据集的10倍。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言基准主要局限于室内环境、单一平台和小规模，无法满足开放世界环境中具身智能体的需求。

Method: 开发了可扩展的标注流程，结合视觉语言模型提示和人工验证；提出平台感知归一化和跨模态对齐技术；建立领域内和跨平台评估协议。

Result: 构建了大规模高质量数据集，揭示了在泛化3D视觉语言理解方面存在的显著性能差距。

Conclusion: 3EED数据集和基准工具包的发布将推动语言驱动的3D具身感知研究的未来发展。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [177] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 提出HGFreNet，一种结合图注意力机制和Transformer的架构，用于从2D姿态提升到3D姿态，通过跳数混合特征聚合和频域轨迹一致性来解决深度模糊和轨迹不连贯问题。


<details>
  <summary>Details</summary>
Motivation: 解决2D到3D人体姿态提升中的深度模糊和轨迹不连贯问题，传统方法只关注时间域抖动约束，而忽略了骨骼关节运动的全局时空相关性。

Method: 设计HGFreNet架构，包含跳数混合图注意力模块和Transformer编码器来建模全局关节时空相关性，并在频域约束轨迹一致性，使用初步网络估计3D姿态。

Result: 在Human3.6M和MPI-INF-3DHP数据集上的实验表明，HGFreNet在位置精度和时间一致性方面优于现有最先进方法。

Conclusion: HGFreNet通过全局时空建模和频域轨迹约束，有效提升了2D到3D姿态估计的准确性和连贯性。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [178] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++ 是一种从单视图图像高效生成高质量纹理网格的新方法，通过跨域扩散模型生成多视图法线图和彩色图像，在约3分钟内完成高质量表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：基于SDS的方法需要耗时的逐形状优化且几何不一致；直接网络推理的方法质量低且缺乏几何细节。需要一种能同时提高质量、一致性和效率的单视图重建方法。

Method: 提出跨域扩散模型生成多视图法线图和彩色图像；使用多视图跨域注意力机制确保生成一致性；采用级联3D网格提取算法，以粗到细的方式从多视图2D表示中提取高质量表面。

Result: 方法实现了高质量的重建结果，具有良好的泛化能力和效率，相比先前工作有显著提升。

Conclusion: Wonder3D++ 在单视图3D重建任务中，在质量、一致性和效率方面都取得了显著改进，为高效高质量3D内容生成提供了有效解决方案。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [179] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自驾模型，使用线性组RNN算子高效处理大规模LiDAR点云、多视角图像和时间序列，无需显式融合模块即可支持多种配置，在3D感知、预测和规划任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决transformer在处理长序列数据时二次注意力机制带来的计算开销问题，简化多模态多任务自驾系统的设计。

Method: 基于线性组RNN算子（对分组特征执行线性RNN），构建单一架构支持LiDAR-only、时序LiDAR、多模态和多模态时序融合等多种配置。

Result: 在3D目标检测、跟踪、占用预测、BEV地图分割、运动预测和端到端规划等核心任务中均取得竞争性甚至最先进的性能。

Conclusion: UniLION为自动驾驶3D基础模型的发展提供了新视角，简化系统设计同时保持优越性能。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [180] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo是首个专家策划的手术视频生成模型评估基准，通过四级外科合理性金字塔框架评估模型输出。研究发现Veo-3模型在视觉感知层面表现优异，但在器械操作、环境反馈和手术意图等高级合理性层面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 视频生成基础模型在模拟物理世界方面表现出色，但在需要深度专业因果知识的高风险领域如手术中应用仍存在关键空白。

Method: 提出SurgVeo基准和四级外科合理性金字塔框架，让Veo-3模型在腹腔镜和神经外科手术片段上进行零样本预测任务，由四名认证外科医生按照SPP框架评估生成视频。

Result: 发现明显的"合理性差距"：Veo-3在视觉感知合理性方面表现卓越，但在器械操作合理性、环境反馈合理性和手术意图合理性等高级层面严重失败。

Conclusion: 这项工作首次提供了视觉逼真模仿与因果理解之间差距的定量证据，为开发能够应对专业医疗领域复杂性的未来模型奠定了基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种基于提示驱动的GraphRAG框架，通过精心设计的提示词来优化实体提取、事实选择和段落重排，在知识图谱上进行多跳问答，在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于图检索的RAG方法在复杂推理方面已有研究，但提示设计对检索和推理过程的影响尚未得到充分探索。本文旨在研究提示设计在提升检索准确性和响应质量方面的重要性。

Method: 构建符号知识图谱，将实体和事实关系编码为结构化三元组；在在线检索阶段选择性使用LLM进行语义过滤和答案生成；通过个性化PageRank实现基于实体引导的图遍历，支持高效可扩展的检索。

Result: 在HotpotQA和2WikiMultiHopQA数据集上分别获得80.7%和78.9%的F1分数，以及97.1%和98.1%的Recall@5分数，达到了最先进的性能水平。

Conclusion: 提示设计是提高检索准确性和响应质量的关键因素，为更高效和可解释的多跳问答系统奠定了基础，强调了提示感知图推理的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: Scitextures数据集是一个大规模的科学纹理和视觉模式集合，包含1200多个模型和10万张图像，涵盖物理、化学、生物、社会学、技术、数学和艺术等领域。通过AI代理管道自动收集和实现标准化模型，用于评估AI模型将视觉模式与生成机制联系起来的能力。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模式与形成机制之间的深层联系，理解云层、波浪、城市增长、森林发展等自然和人工模式背后的生成过程。

Method: 使用AI代理管道自主收集和实现标准化模型，创建包含多种科学领域纹理和模式的大规模数据集。通过让AI识别真实世界模式的机制并生成模拟图像来测试其理解能力。

Result: 视觉语言模型能够理解和模拟超越视觉模式的物理系统，能够识别不同模式是否来自相同过程，并能推断和重建视觉模式背后的机制。

Conclusion: Scitextures数据集为研究视觉模式与生成机制之间的联系提供了重要资源，展示了AI在理解复杂系统形成过程方面的潜力。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出了TIR-Bench基准测试，用于评估视觉推理中基于工具的图像处理能力，测试了22个多模态大语言模型，发现该基准具有普遍挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分捕捉OpenAI o3等模型通过工具操作图像进行问题解决的先进能力，即使是视觉搜索基准也只测试基本操作，无法评估复杂、动态和工具依赖的推理。

Method: 开发了TIR-Bench基准，包含13个多样化任务，每个任务都需要在思维链中使用新颖工具进行图像处理和操作。评估了22个多模态大语言模型，包括开源和专有模型以及具有明确工具使用增强的模型。

Result: TIR-Bench对所有模型都具有挑战性，强性能需要真正的思维与图像结合能力。进行了直接微调与代理微调的初步比较研究。

Conclusion: TIR-Bench是一个全面的基准，能够有效评估代理式思维与图像结合能力，揭示了当前模型在复杂视觉推理任务中的局限性。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [184] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性，威胁了评论生态系统的信任和透明度。

Method: 使用BERT编码文本特征和ResNet-50提取视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，混淆矩阵和定性分析显示模型能检测文本赞美与无关或低质量图片之间的细微不一致。

Conclusion: 本研究证明了多模态学习在保护数字信任中的关键作用，为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [185] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 该论文研究了多智能体强化学习在零售价格优化中的应用，比较了MAPPO基线方法和图注意力增强变体MAPPO+GAT的性能，发现图集成MARL比独立学习器提供更可扩展和稳定的动态零售定价解决方案。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，现有方法在跨产品协调方面存在不足。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强的MAPPO+GAT方法，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合级价格控制提供了稳健且可复现的基础，MAPPO+GAT通过在产品图上共享信息进一步提升了性能，且未引起过度的价格波动。

Conclusion: 图集成多智能体强化学习为动态零售定价提供了比独立学习器更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [186] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个用于分析人口层面研究问题的模型和方法集合。本文完整描述了基于奥地利公开数据计算模型参数的数据处理方法，特别关注GEPOC ABM代理模型参数的计算，并进行了验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程，生成有效且可用的模型参数。

Method: 基于奥地利公开可访问数据，使用聚合、分解、融合、清洗和缩放等算法处理数据，计算GEPOC ABM代理模型的参数。

Result: 开发了完整的数据处理方法，生成了可用的模型参数文件，并通过GEPOC ABM模型进行了广泛的验证研究。

Conclusion: 成功建立了基于公开数据的GEPOC模型参数计算方法，为人口研究提供了可靠的数据处理框架和验证结果。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [187] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个为量子科学领域构建的LLM评估基准，包含约800个多选问题，涵盖9个量子科学领域，用于系统评估LLM在非直观量子领域的理解能力。


<details>
  <summary>Details</summary>
Motivation: 通用基准很少反映量子科学等专业领域的需求，而量子科学具有非直观现象和高级数学要求，需要专门评估LLM是否准确掌握领域知识和符号表示。

Method: 利用公开材料构建包含约800个问题及其答案的数据集，涵盖9个量子科学领域，组织成8选项多选格式，并评估多个现有LLM的性能。

Result: 评估了多个现有LLM在量子领域的表现，包括对问题格式变化的敏感性分析。

Conclusion: QuantumBench旨在指导LLM在量子研究中的有效应用，是量子领域首个LLM评估数据集。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [188] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，让AI工程师团队协作完成计算设计任务，在无人机翼优化中实现了100%成功率。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，专家团队协作设计复杂产品需要大量时间和成本，需要开发能够自主执行复杂工程任务的AI工程师系统。

Method: 采用分层多智能体架构，由总工程师协调空气动力学、结构、声学和优化等专业工程师智能体，通过文件介导的通信实现协作，集成多种工程软件进行并行多学科仿真。

Result: 在无人机翼优化中，超过400个参数配置实现了100%成功率，无网格生成失败、求解器收敛问题或需要人工干预。

Conclusion: 基于智能体的AI工程师有潜力自主执行复杂工程任务，该框架被验证为可信赖的自动化工程工作流。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [189] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开源程序生成器，旨在扩展ARC-AGI训练数据集，通过生成更多样化的样本对来解决原始数据集样本数量有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试虽然对评估人工智能通用智能很有价值，但其演示集样本数量有限，每个任务只有少量输入-输出网格对，这限制了需要大量任务内示例的算法的性能。

Method: 开发了ARC-GEN程序生成器，该生成器既全面（覆盖所有400个任务）又具有模仿性（更贴近原始ARC-AGI-1发布的分布特性和特征）。

Result: 成功创建了一个能够忠实扩展原始ARC-AGI训练数据集的生成器，为算法提供了更丰富的训练样本。

Conclusion: ARC-GEN不仅扩展了ARC-AGI数据集，还被用于建立静态基准套件来验证提交给2025年Google Code Golf锦标赛的程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [190] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 改进了[1]中的增量选择算法并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的增量选择算法以提高性能或扩展功能

Method: 提出改进的增量选择算法

Result: 成功证明了所有选定的猜想

Conclusion: 改进的算法有效且能够证明所有相关猜想

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [191] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型如何帮助解决认知科学领域面临的挑战，包括知识整合、概念澄清和跨学科连接等问题。


<details>
  <summary>Details</summary>
Motivation: 认知科学由于其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了工具。

Method: 通过综述分析，考察LLMs在建立跨学科连接、形式化理论、发展清晰测量分类法、通过集成建模框架实现泛化以及捕捉情境和个体差异等方面的能力。

Result: LLMs在当前这些领域具有特定能力和局限性，包括潜在陷阱。当明智使用时，它们可以作为认知科学的补充工具。

Conclusion: 当谨慎使用以补充而非取代人类专业知识时，LLMs可以作为更整合和累积性认知科学的工具。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [192] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI Accelerator项目更新报告，介绍了该合作项目如何通过公开挑战问题推动AI研究，特别是在国防和民用领域的应用。


<details>
  <summary>Details</summary>
Motivation: 扩大美国在国防和民用领域的竞争优势，通过公开挑战问题促进AI研究的根本性进步。

Method: 开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，以刺激开源解决方案并吸引更广泛的学术和私营部门AI生态系统参与。

Result: 持续和新的挑战已成功为AI研究和AI技术应用做出贡献。

Conclusion: DAF-MIT AI Accelerator通过公开挑战问题有效推动了AI研究的发展，为美国在国防和民用领域的技术优势提供了支持。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [193] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE基准测试评估LLMs在检测合同细微差异方面的能力，发现模型经常遗漏细微错误且难以进行法律解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在重要法律工作中应用迅速，但缺乏系统评估其可靠性的基准，无法测试真实合同中存在的细微、对抗性缺陷。

Method: 通过从CUAD和ContractNLI数据集生成7500多个扰动合同，使用角色驱动管道创建10种异常类别，并通过RAG系统验证法律准确性。

Result: 分析显示LLMs在检测嵌入的法律缺陷方面存在关键弱点：经常遗漏细微错误，且在法律解释方面表现更差。

Conclusion: 该工作为识别和纠正法律AI中的推理失败提供了路径。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [194] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理范式，通过结构化五步流程增强多元人类价值观对齐，在SafeWorld基准上显著提升了文化适应性推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往产生表面一致性而非真正的伦理理解，难以处理人类价值观的复杂性和情境依赖性，需要解决跨地区文化的多元价值对齐挑战。

Method: 采用结构化五步伦理推理流程：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析、反思，可通过提示工程或监督微调实现。

Result: 在专为区域价值对齐设计的SafeWorld基准上，相比基线方法显著提升了LLM与多元人类价值观的对齐程度，实现了更准确的社会规范识别和更文化适应的推理。

Conclusion: 该工作通过跨学科研究为开发更有效对齐全球社会多元价值观的LLM提供了具体路径，增强了模型对区域特异性的理解和细致伦理分析能力。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [195] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 系统评估四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现基于适配器的方法能改善安全性且对公平性破坏较小，而基于提示的方法会降低安全性和公平性。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多采用公共仓库中的LLM，但微调可能降低模型的安全性或公平性，需要系统评估不同微调方法在这些关键维度上的权衡。

Method: 对四个指令微调模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B、Gemma-7B)应用四种PEFT方法，共评估235个微调变体，涵盖11个安全风险类别和9个人口统计公平性维度。

Result: 基于适配器的方法(LoRA、IA3)倾向于提高安全分数且对公平性破坏最小；基于提示的方法(Prompt-Tuning、P-Tuning)通常降低安全性并导致更大的公平性退化；不同基础模型表现出不同的对齐变化模式。

Conclusion: 安全关键部署的实用指南：从良好对齐的基础模型开始，优先选择基于适配器的PEFT方法，并对安全性和公平性进行类别特定的审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [196] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症，特别关注COVID-19疫情期间的数据。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间心理健康问题激增，但人们往往不愿咨询医生。社交媒体成为表达情感的重要平台，现有方法忽视了推文数据稀疏性和多模态特性。

Method: 开发多模态框架，提取文本、用户特定特征和图像内容；提出外部特征利用推文中的URL和图像中的文本内容；引入视觉神经网络(VNN)生成图像嵌入；贡献COVID-19抑郁症数据集。

Result: 在基准数据集上比现有最优方法提升2%-8%，在COVID-19数据集上表现良好；分析揭示了各模态的影响，提供了用户心理状态的宝贵见解。

Conclusion: 多模态方法能有效检测社交媒体用户的抑郁症，特别是在疫情期间；提出的框架在性能上优于现有方法，为心理健康监测提供了新途径。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [197] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个框架，通过动态工具序列让LLM能够分析复杂图数据，解决了LLM在大规模图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在大规模图分析中存在显著限制，包括上下文约束和推理不灵活性，需要新的方法来支持复杂图分析。

Method: 采用渐进式图蒸馏的强化学习机制生成优化的工具序列，以及基于谱属性和轻量适配器的结构感知测试时适应方法。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应，有效解决了LLM在图分析中的局限性，为大规模图分析提供了新的解决方案。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [198] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性，减少过度拒绝，并支持单一模型适应不同价值系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临生成有害内容和过度拒绝良性查询的双重挑战，特别是在多模态场景下问题更加明显。传统方法如SFT和RLHF无法支持多价值系统且需要昂贵的参数调整。

Method: 通过使用有害/良性样本优化图像提示，使单一模型无需参数更新即可适应不同价值系统并更好地与给定安全偏好对齐。

Result: 实验表明该方法在多样化数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [199] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方(BMS)的简单算法，该算法通过归纳法证明总能返回有效的BMS，并具有最优理论复杂度。研究还扩展到非方形BMS，形式化了这些BMS存在的行列和条件，并展示了算法变体也能生成它们。


<details>
  <summary>Details</summary>
Motivation: 开发高效的二进制幻方生成算法，解决方形和非方形二进制矩阵中行列和相等的数学问题。

Method: 通过归纳法设计简单算法生成二进制幻方，并扩展算法变体处理非方形情况，形式化行列和条件。

Result: 算法被证明总能生成有效的BMS，具有最优复杂度，并成功扩展到非方形BMS。发布了两个Python实现，包括GPU加速版本。

Conclusion: 提出的算法能高效生成方形和非方形二进制幻方，提供了理论保证和实用实现。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [200] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制模型，使用队列长度定义状态和奖励函数，与探针车辆技术兼容，能有效缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有区域自适应交通信号控制研究多采用多智能体框架，但存在可扩展性问题。交通信号控制本质上需要集中管理，由单一控制中心监控所有道路状况并协调所有交叉口控制。

Method: 设计单智能体强化学习框架，状态和奖励函数基于队列长度定义，动作设计用于调节队列动态。队列长度定义与传统略有不同但与拥堵状态密切相关，可使用探针车辆的链路行程时间数据进行可靠估计。

Result: 在SUMO仿真平台上进行全面评估，实验结果表明所提模型通过协调多交叉口控制，有效缓解了大规模区域拥堵水平。

Conclusion: 提出的单智能体RL区域ATSC模型与探针车辆技术兼容，具有广泛部署潜力，能有效实现大规模区域交通拥堵的协调控制。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [201] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并基于该档案进行多维度评分，解决了现有方法难以处理个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味多样复杂。

Method: 采用"预测-评估"范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分。使用两阶段训练策略：冷启动监督微调阶段和强化学习阶段，并提出了相似性感知预测奖励。

Result: 大量实验证明了所提出方法的优越性。

Conclusion: 该方法通过构建大规模思维链式个性化评估数据集和两阶段训练策略，有效解决了个性化图像偏好评估的挑战。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [202] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS解码框架通过在高熵token处选择性分支和早停策略，在大型推理模型中寻找最短有效推理路径，提升准确率8%，减少推理长度23%和重复频率12%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中常出现过度思考问题，产生过长的思维链，导致推理成本增加且可能降低准确率。研究发现推理长度与准确率呈负相关，短推理路径通常更准确。

Method: 提出DTS模型无关的解码框架，通过在高熵token处选择性分支来勾勒推理空间，并应用早停策略选择最短完成的推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确率提升高达8%，平均推理长度减少23%，重复频率降低12%。

Conclusion: DTS能够有效近似最优解，同时提升大型推理模型的效率和准确率，实现可扩展的高效推理。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [203] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统的自动化网络故障排除框架，利用LLM协调多个专用工具，通过微调小型语言模型生成基于内部文档的修复方案，显著加速无线接入网和核心网的故障排除。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据、难以在异构部署中泛化，网络故障排除仍严重依赖专家手动分析。

Method: 采用多智能体系统，包括编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体，动态激活进行故障诊断和修复推荐。关键组件解决方案规划器通过微调小型语言模型在专有故障排除文档上生成领域基础的解决方案计划。

Result: 实验结果表明，该框架在无线接入网和核心网领域显著加速了故障排除自动化。

Conclusion: 多智能体系统结合LLM协调和专用工具，能够有效实现网络故障排除的自动化，减少对专家的依赖。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [204] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展了经典规划中的提升后继生成器，支持数值前提条件，通过枚举替换一致性图中的最大团来生成地面动作，避免任务表示大小的指数级增长。


<details>
  <summary>Details</summary>
Motivation: 传统规划器将数值规划任务转换为地面任务表示时，会导致表示大小的指数级爆炸，特别是在难以接地的任务中。现有提升后继生成器不支持数值动作前提条件。

Method: 扩展状态最先进的提升后继生成器，支持数值前提条件适用性。方法通过枚举替换一致性图中的最大团，每个最大团代表动作模式变量的替换，生成地面动作。在图中添加数值动作前提条件，并在条件不满足时通过最终适用性检查过滤不适用动作。

Result: 在25个基准域中，有23个域不会出现不适用地面动作的情况，只有1个域会出现这种情况。该生成器是首个支持数值动作前提条件的提升后继生成器。

Conclusion: 该方法实现了对非常丰富的规划片段的提升规划，为未来研究奠定了基础，是首个支持数值动作前提条件的提升后继生成器。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [205] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: Ariadne框架通过合成迷宫进行多步空间推理，使用RLVR进行难度感知课程训练，成功扩展了基础VLM在视觉中心空间任务上的能力边界，并在现实世界基准测试中显示出显著的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究RL后训练是否真正能扩展基础VLM的能力边界，特别是在基础模型最初失败的视觉中心空间任务上。

Method: 使用合成迷宫创建可控难度的多步空间推理任务，采用带验证奖励的强化学习（RLVR）进行难度感知课程训练。

Result: RLVR后训练使VLM在基础模型得分为0%的问题集上达到超过50%的准确率；在现实世界基准测试中，MapBench平均提升16%，ReasonMap平均提升24%。

Conclusion: 该方法不仅扩展了模型的基本能力限制，还增强了其在现实世界空间推理中的泛化能力，为专门的能力扩展对齐研究提供了动力。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [206] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU角度分析智能AI框架的系统瓶颈，发现工具处理在CPU上可占90.6%总延迟，并提出两种优化方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 从被忽视的CPU中心视角理解和表征智能AI工作负载引入的系统瓶颈，揭示CPU对延迟、吞吐量和能耗的显著影响。

Method: 系统化表征智能AI框架，选择5个代表性工作负载进行性能分析，提出CPU和GPU感知的微批处理以及混合工作负载调度优化方法。

Result: CPU工具处理占主导延迟(90.6%)，CPU动态能耗占44%，吞吐量受CPU和GPU因素共同限制。优化方法实现2.1倍和1.41倍延迟加速。

Conclusion: CPU在智能AI系统中扮演关键角色，提出的优化方法能显著提升智能AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [207] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新检验了在现代大语言模型中增加采样推理路径对自一致性方法的影响，发现性能提升在适度采样后达到平台期，高采样配置相比计算成本收益有限


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于多推理链组合能改善结果但会达到平台期的结论，在现代LLM条件下检验这些主张

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，对不同的采样推理路径配置进行输出池化，并与单一思维链基线进行比较

Result: 较大模型表现出更稳定和一致的改进曲线，性能增益在适度采样后趋于平缓，与过去发现一致

Conclusion: 自一致性方法仍然有用，但高采样配置相对于计算成本带来的益处很小，平台期表明推理路径之间的重叠导致收益递减

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [208] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了主动思考模型（ATM），这是一个将目标推理、动态任务生成和自我反思学习集成到自适应架构中的统一认知框架，能够在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够在没有外部监督的情况下从次优行为自主演化为最优行为，并在变化环境条件下保持有界跟踪遗憾。

Conclusion: ATM提供了一个统一框架，使AI系统能够在动态环境中自主适应和改进，克服了传统系统的局限性。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [209] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在重复确定性预测任务中，准确率随输出长度呈双指数下降，形成"准确率悬崖"，表明模型无法独立执行每个操作。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在重复确定性任务中的性能表现，特别是准确率如何随输出长度变化，以及模型执行重复操作时的内在机制。

Method: 通过实验测试多个领先大型语言模型在字符串替换、整数加法、量子力学字符串算子乘法等重复任务中的表现，并建立统计物理模型解释观察到的现象。

Result: 发现模型准确率在超过特征长度后急剧双指数下降，形成准确率悬崖，而非预期的指数衰减。统计物理模型能定量复现这一交叉现象。

Conclusion: 模型无法独立执行重复操作，注意力机制引发的内部干扰导致序列级失败。该研究为理解大型语言模型确定性准确率的极限提供了理论框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [210] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了三种结构化电子健康记录预测方法：基于计数的模型、预训练序列变换器和混合代理管道，发现在EHRSHOT数据集上基于计数的方法和混合代理方法表现相当，基于计数方法因其简单性和可解释性仍是强有力候选。


<details>
  <summary>Details</summary>
Motivation: 虽然基于计数的学习器在结构化EHR数据上表现强劲，但尚未与更新的混合代理LLM管道进行直接基准比较，后者在各种NLP任务中被报告优于单个LLM。

Method: 使用EHRSHOT数据集评估三种方法：1)基于计数的模型（LightGBM和TabPFN）；2)预训练序列变换器（CLMBR）；3)混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法在头对头比较中基本平分秋色。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化EHR基准测试的有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [211] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将RLVR LLM训练应用于公共交通运营中的现实预测挑战，通过引入基于容差的成形奖励函数，在NYC MTA服务警报数据集上实现了35%的5分钟准确率相对提升。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调难以处理领域稀疏性和噪声连续标签，而传统RLVR主要适用于二元正确性任务，在噪声连续预测中的适用性尚待探索。

Method: 通过引入基于容差的成形奖励函数，在连续误差范围内给予部分信用，而非要求单一正确答案，将RLVR适应于该任务。

Result: 通用指令调优LLM显著优于专用数学推理模型，成形奖励设计至关重要，RLVR方法在最具挑战性的指标上表现最优，在5分钟准确率上比最强基线提升35%。

Conclusion: RLVR可以成功适应现实世界的噪声预测任务，但需要设计反映问题连续性质的验证器。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [212] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该研究提出了AI自我意识指数(AISAI)，通过"猜2/3平均值"游戏测试28个大型语言模型，发现高级模型表现出明显的自我意识，能够根据对手类型调整策略，并认为自身比其他AI和人类更理性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，研究其是否会出现自我意识作为涌现行为，以及如何测量这种自我意识。

Method: 使用游戏论框架，通过"猜2/3平均值"游戏测试28个模型，设置三种对手情境：对抗人类、对抗其他AI、对抗同类AI，分析模型在不同情境下的策略差异。

Result: 75%的高级模型表现出自我意识，能够根据对手类型调整策略；自我意识模型形成一致的理性层级：自我 > 其他AI > 人类，表现出显著的AI归因效应和适度的自我偏好。

Conclusion: 自我意识是高级LLMs的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [213] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，利用LLM旅行者智能体和校准智能体来模拟人类旅行者的学习和适应行为，通过在线数据流实现持续学习和行为对齐。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者如何从与交通系统的互动中学习和调整旅行行为对于系统评估和规划至关重要，但由于涉及复杂的认知和决策过程，这一任务具有挑战性。

Method: 采用双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者；一个LLM校准智能体利用LLM的推理和分析能力训练旅行者智能体的角色，确保行为对齐。

Result: 使用真实世界日常路线选择实验数据集，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法，并能捕捉底层学习过程的演变。

Conclusion: 该框架为创建适应性强且行为真实的智能体来模拟旅行者的学习和适应提供了新方法，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [214] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘复发，LGBM模型表现最佳，相比现有决策规则有显著改进


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘复发是常见但可预防的问题，利用电子病历和机器学习可准确识别高风险儿童，促进预防性综合护理

Method: 使用回顾性电子病历数据（含环境污染物和社区边缘化信息）训练多种ML模型（LGBM、XGB和LLM），在COVID前后数据集上进行验证

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型AUC为0.712，F1分数0.51，显著优于现有决策规则（F1=0.334）

Conclusion: 机器学习模型能有效预测儿童哮喘复发风险，关键预测特征包括既往哮喘急诊就诊、医疗复杂性、食物过敏等

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [215] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维参数子空间中，并发现归纳头的出现时间与输入上下文长度的平方成正比。


<details>
  <summary>Details</summary>
Motivation: Transformer的成功部分归功于上下文学习能力，但归纳头作为实现这一能力的关键机制，其具体结构和训练动态尚不明确。本文旨在揭示归纳头的形成过程和数学原理。

Method: 使用最小化ICL任务公式和修改的Transformer架构进行理论分析，通过形式化证明训练动态被限制在19维参数子空间，并实证验证只有3个维度对归纳头的出现起关键作用。

Result: 发现了归纳头权重矩阵的简单可解释结构，证明了训练动态被约束在低维子空间，并确定了归纳头出现时间的渐进上界为输入上下文长度的平方。

Conclusion: 归纳头的形成遵循可预测的数学规律，其训练动态被高度约束在低维参数空间中，这为理解Transformer的上下文学习机制提供了新的理论洞见。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [216] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法（KEwLTM和KEwRAG），利用大语言模型从非结构化病理报告中提取癌症分期规则，无需大量标注数据，在乳腺癌病理报告中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统NLP和机器学习方法依赖大量标注数据，限制了癌症分期提取的可扩展性和适应性。需要开发不依赖标注数据的自动化解决方案。

Method: KEwLTM使用迭代提示策略直接从无标注病理报告中推导分期规则；KEwRAG采用检索增强生成变体，从相关指南中预提取规则。两种方法都利用LLM的预训练知识。

Result: KEwLTM在零样本思维链推理有效时表现更好，KEwRAG在零样本思维链推理效果较差时表现更优。两种方法在TCGA乳腺癌病理报告的T和N分期识别中都优于基线方法。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且具有增强可解释性的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [217] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: 提出ET2RAG框架，通过检索增强生成和多数投票机制，在保持效率的同时提升LLMs性能，无需训练即可实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：RAG可能引入不相关文档导致错误响应，集成方法缺乏外部知识且成本高昂，需要平衡开销与性能。

Method: ET2RAG是无训练方法，首先检索相关文档，通过管理响应长度高效生成多样化候选响应，然后计算相似度并采用多数投票选择最终输出。

Result: 实验结果显示ET2RAG在开放域问答、菜谱生成和图像描述三个任务上显著提升性能。

Conclusion: 部分生成已足够捕获共识计算所需关键信息，无需完整响应即可有效执行多数投票，从而在计算成本与性能间达到平衡。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [218] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务执行中任务分解和协作的局限性

Method: 将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，采用动态调度和路由机制实现智能体间的合理分工和实时协作

Result: 在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度验证了架构有效性，在整体性能和鲁棒性上优于现有方法

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统性解决方案

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [219] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应推理截断框架，通过根据问题难度调整思考长度，在保持或提高准确性的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 当前链式思维方法不加区分地生成长解释导致效率低下，而现有的强化学习方法不稳定且过度依赖奖励

Method: 从更强模型蒸馏简洁推理模式，将其插值到连续推理风格谱系中，并筛选平衡正确性和紧凑性的最优训练数据，学习何时停止思考

Result: 在多个数学基准测试中实现81.2%的推理截断率和5.33倍计算加速，同时保持或提高准确性

Conclusion: DART为高效推理提供了一个稳定通用的范式，推动了LLM中自适应智能的发展

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [220] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域自动检测学生错误概念的三阶段框架，通过检索引导的多阶段推理和集成融合，在保持可解释性的同时减少对大语言模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 检测开放式回答中的学生错误概念是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: 三阶段框架：检索模块缩小候选池，推理模块使用思维链生成暴露逻辑不一致，重排模块通过对齐推理来优化预测，并通过集成融合策略统一组件。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别实现了0.82/0.92/0.93的平均精度均值，持续优于各个独立模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE在减少对大语言模型依赖的同时，为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [221] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 该论文提出了NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且昂贵，神经编译(Neural Compilation)作为新范式有潜力简化开发，但缺乏专用基准和评估方法阻碍了其实际应用。

Method: 引入NeuComBack基准数据集，定义神经编译工作流，并提出自进化提示优化方法，让LLM从自我调试轨迹中迭代进化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%。在正确生成的x86_64程序中，87.5%超越了clang-O3性能。

Conclusion: 提出的方法显著提升了LLM生成汇编代码的质量，为神经编译的实际应用提供了可行路径。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [222] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开集故障诊断框架，用于处理训练时未见的未知故障类型，提高深度学习模型在工业故障诊断中的实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法假设训练和测试数据中的故障类型一致且已知，但在实际工业应用中会出现训练时未见的未知故障类型，导致传统方法失效。

Method: 使用监督特征学习模型提取多层融合特征表示，构建可靠性子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型，学习每个类别的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架能够准确分类已知故障并有效检测未知样本，增强了深度学习模型在开集故障诊断场景中的适用性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [223] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型(LLM)的随机决策支持系统中应用Shapley值进行特征归因的方法，分析了不同实现变体对Shapley值原则满足情况的影响，并探讨了可解释性推理速度、与精确Shapley值归因的一致性以及原则达成之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法使基于机器学习的推理可解释，Shapley值因其满足多个理想原则而流行。但在LLM这种随机推理系统中，这些原则的保证需要重新评估。

Method: 将Shapley值应用于基于LLM的决策支持系统的特征归因，分析不同实现变体在随机推理环境下的原则满足情况。

Result: 研究表明在LLM的随机推理中，Shapley值原则的保证取决于具体实现方式，存在可解释性推理速度、归因准确性和原则达成之间的权衡。

Conclusion: 在基于LLM的随机决策支持系统中应用Shapley值进行特征归因时，需要仔细考虑实现变体对原则保证的影响，并在不同目标之间做出权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [224] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出了OmniFuser多模态学习框架，用于铣削刀具的预测性维护，融合视觉和传感器数据，通过跨模态融合机制和递归精炼路径实现刀具状态分类和力信号预测。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中准确及时的刀具状态预测至关重要，非计划性刀具故障会导致质量下降和生产停机。现代工业环境需要可靠的服务导向型预测维护。

Method: 并行特征提取高分辨率刀具图像和切削力信号，采用无污染的跨模态融合机制分离共享和模态特定组件，使用递归精炼路径保留残差信息稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础，学习到的表示可封装为可重用维护服务模块。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [225] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出Competitive Isolation PSM-DID框架，通过结合倾向得分匹配与竞争隔离来解决搜索型双边市场中平台级干预评估的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 搜索型双边市场中的平台级干预评估面临系统性效应挑战，如溢出效应和网络干扰，传统PSM-DID框架容易受到选择偏差和跨单元干扰的影响。

Method: 将倾向得分匹配与竞争隔离相结合，在互斥条件下提供理论保证的无偏估计，支持平台级效果测量而非项目级指标。

Result: 实验显示相比基线方法显著减少了干扰效应和估计方差，在大型市场平台中成功部署验证了实用性。

Conclusion: 该框架为平台级因果推断提供了有效的解决方案，通过竞争隔离机制解决了传统方法的局限性。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [226] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 这篇论文探讨了催眠状态下的人类认知过程与大型语言模型在功能上的深层相似性，包括自动性、监控抑制和高度情境依赖性等机制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示催眠与LLMs在认知机制上的功能平行性，以理解无主观意识的复杂行为如何产生，并为可靠AI系统设计提供启示。

Method: 通过比较分析催眠实验研究结果与LLMs的计算操作，识别三个核心原则：自动性、监控抑制和情境依赖性。

Result: 发现两种系统都产生连贯但无基础依据的输出，需要外部解释者赋予意义，并展示了无主观意识的功 能性代理行为。

Conclusion: 未来可靠AI的发展需要整合生成流畅性与执行监控机制的混合架构，借鉴人类心智的自我调节结构。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [227] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合演化越狱提示和评分模板，解决了现有方法依赖稀疏ASR信号或人工评分模板的问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法要么依赖稀疏的二元攻击成功率信号，要么使用人工设计的评分模板，这带来了人类偏见和评分结果的不确定性。

Method: 采用双层优化结构：内层循环使用固定评分模板通过细粒度密集反馈优化提示；外层循环使用ASR对齐分数优化模板，使其更好地反映真实攻击结果。

Result: 在AdvBench和JBB-Behaviors上的评估显示，AMIS在Claude-3.5-Haiku上达到88.0% ASR，在Claude-4-Sonnet上达到100.0% ASR，显著优于现有基线方法。

Conclusion: AMIS框架通过联合优化提示和评分模板，能够生成更强的越狱提示和更校准的评分信号，为大语言模型的安全性评估提供了有效工具。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [228] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 扩展了C-DAG框架，支持任意变量聚类，允许循环C-DAG表示，并扩展了d-分离和因果演算概念。


<details>
  <summary>Details</summary>
Motivation: 当选择的聚类在生成的C-DAG中引发循环时，传统C-DAG语义认为该分区不可接受，这限制了C-DAG的应用范围。

Method: 通过放宽分区可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这个设置中。

Result: 显著扩展了跨集群的因果推理范围，使C-DAG能够应用于以前难以处理的场景。

Conclusion: 提出的演算相对于do-演算是健全且原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导出来，每个规则对应一个原始的do-演算步骤。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [229] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 该研究从AI角度探索双任务范式中的时间处理干扰，在简化版Overcooked环境中训练DRL代理，发现双任务代理相对于单任务代理显著高估时间，但未发现明确的内部计时机制。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习代理在双任务范式中的时间处理行为，并与人类计时研究中的发现进行对比，以促进对生物系统和AI系统行为的更好理解。

Method: 在简化版Overcooked环境中实现单任务(T)和双任务(T+N)两种变体，分别训练两个深度强化学习代理。双任务包含时间生产任务和数字比较任务，分析代理的LSTM层神经动力学。

Result: 双任务代理相对于单任务代理显著高估时间，这一结果在四个目标持续时间上保持一致。初步分析未发现代理LSTM层中存在专用或内在计时器的明确证据。

Conclusion: 需要进一步研究以理解代理的底层计时机制，并为观察到的行为模式提供解释。这是探索DRL涌现行为与生物系统行为之间相似性的一小步。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [230] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一个交互式AI代理，通过可审计的行动序列生成解释，使用强化学习优化策略来寻找外部视觉证据支持诊断推理，显著提高校准准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域（如医学）中AI模型解释缺乏可验证性，从而阻碍信任的问题。

Method: 使用强化学习训练代理策略，使其能够战略性地寻求外部视觉证据来支持诊断推理，并引入因果干预方法来验证解释的忠实性。

Result: 实验显示基于行动的推理过程显著改善校准准确性，Brier分数比非交互基线降低18%。通过屏蔽代理选择的视觉证据，观察到性能明显下降（ΔBrier=+0.029），证实证据对其决策过程至关重要。

Conclusion: 这项工作为构建具有可验证和忠实推理能力的AI系统提供了一个实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [231] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 该论文提出了一种双信息瓶颈(DIB)策略，通过低秩Renyi熵函数框架来解决多模态情感分析中的噪声问题和信息融合不足问题，实现了更强大的统一紧凑多模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余信息。

Method: 采用双信息瓶颈(DIB)策略，包含两个关键模块：1)通过最大化任务相关信息并丢弃冗余信息来学习充分压缩的单模态表示；2)通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上的广泛实验验证了方法的有效性。在CMU-MOSI上Acc-7指标达到47.4%，在CH-SIMS上F1-score达到81.63%，比次优基线提升1.19%。在噪声条件下，性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，实现了更鲁棒和有效的多模态情感分析。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [232] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 本研究提出了一个分层多智能体框架，将被动医疗AI系统转变为主动询问代理，通过自主任务编排来改进预咨询流程。该框架在1372份电子健康记录上评估，在分诊准确率和临床质量评分方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和咨询时间有限的挑战，现有AI系统受限于被动交互范式和上下文管理问题，需要更主动的解决方案来提升预咨询效率。

Method: 开发了一个包含八个智能体的分层架构，通过集中控制机制将预咨询分解为四个主要任务（分诊、现病史收集、既往史收集、主诉生成），并进一步细分为13个领域特定子任务，采用智能体驱动调度而非顺序处理。

Result: 在多个基础模型上评估，主要科室分诊准确率达87.0%，次要科室分类准确率80.5%，任务完成率98.2%。临床质量评分平均4.56-4.69（5分制），咨询轮次控制在12.7-16.9轮内完成。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统在提升临床预咨询效率和质量方面的潜力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [233] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个评估LLM智能体在工具规划与调度能力的基准测试，包含200个复合任务和数百个MCP工具，重点关注任务完成率和执行效率。


<details>
  <summary>Details</summary>
Motivation: 探索LLM智能体是否能处理需要多种工具的复合现实问题，这些任务不仅需要选择合适的工具，还需要策略性地安排执行顺序以保证效率。

Method: 构建包含200个复合任务的TPS-Bench基准，每个任务由多个子任务组成，基于包含数百个MCP工具的工具库进行评估。

Result: 大多数模型能进行合理的工具规划，但在调度策略上差异明显：GLM-4.5达到64.72%完成率但执行时间长，GPT-4o优先并行调用但完成率仅45.08%。使用强化学习在Qwen3-1.7B上实现了14%执行时间减少和6%完成率提升。

Conclusion: LLM智能体在工具规划方面表现良好，但调度效率有待提升，强化学习是改善调度效率而不影响性能的可行方法。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [234] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 使用多模态基础模型分析企业社交媒体内容，重点关注可持续发展沟通。通过LLM自动标注推文与可持续发展目标的关联，结合视觉语言模型分析图像内容，揭示行业差异、时间趋势及ESG风险关联。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容的多模态、模糊性和动态变化带来的分析挑战，探索基础模型作为临时标注器的潜力，避免昂贵的人工标注成本。

Method: 采用LLM集成方法自动标注企业推文与17个可持续发展目标的关联，结合视觉语言模型通过语义聚类分析视觉内容中的可持续发展沟通模式。

Result: 揭示了不同行业在可持续发展目标参与度上的差异、时间趋势，以及企业信息与ESG风险和消费者参与度之间的关联。

Conclusion: 提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [235] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一个具有百万级外部记忆库的架构，将知识存储为可读的token序列，通过两阶段检索机制实现知识直接检查和修改，在知识密集型任务上比标准Transformer提升43.67%。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因知识过时和缺乏可解释性导致的问题，其隐式知识存储在网络参数中，无法进行针对性更新和推理透明化。

Method: 采用显式记忆库存储人类可读知识，设计可微分两阶段检索机制：基于产品键分解的粗粒度过滤和Gumbel-Softmax细粒度匹配，将知识分为冻结显式事实(20%)和可学习隐式模式(80%)。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景(10k样本)下获得3.62倍增益，正确预测的记忆命中率高出49%。

Conclusion: 联合优化的可解释、可更新模型在保持竞争力的同时提供了前所未有的知识透明度，优于使用冻结检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [236] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 提出了IVGAE-TAMA-BO动态图神经网络模型，用于预测全球粮食贸易网络中的未来贸易链接，通过时间感知动量聚合器和贝叶斯优化显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络在政治、经济和环境因素影响下动态演变，传统方法难以有效捕捉时间模式，需要更准确的链接预测方法来保障粮食安全和供应链稳定。

Method: 基于IVGAE框架，引入贸易感知动量聚合器(TAMA)捕捉贸易网络的时间演化，结合短期波动和长期结构依赖，使用动量结构记忆机制和贝叶斯优化自动调参。

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线模型，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。

Conclusion: 该框架为全球贸易网络结构预测提供了稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有重要应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [237] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种混合法律问答代理，结合检索增强生成和多模型集成，在司法场景中提供可靠、可审计且持续更新的法律咨询。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型容易产生幻觉，在司法咨询中可能提供误导性指导，而静态知识库难以跟上频繁更新的法规和判例。

Method: 采用检索优先策略：当可信法律知识库返回相关证据时使用RAG生成答案，否则通过多个LLM生成候选答案并由专门选择器评分返回最优结果。高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。消融实验证实了检索优先、模型集成和人工更新机制的有效性。

Conclusion: 该系统显著减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [238] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在复杂环境中表现脆弱，本文提出Simia-SFT和Simia-RL框架，通过LLM模拟环境反馈实现无需真实环境数据的可扩展代理训练。


<details>
  <summary>Details</summary>
Motivation: LLM代理在需要跨多样工具和模式的复杂环境中表现脆弱，而构建定制训练环境成本高且易碎，限制了进展。

Method: 提出两个框架：Simia-SFT通过扩增小规模种子数据生成多样化轨迹的SFT数据；Simia-RL通过LLM模拟反馈实现无需真实环境实现的强化学习训练。

Result: 微调开源模型在多个基准测试中取得一致改进，在τ²-Bench上超越GPT-4o并接近o4-mini水平。

Conclusion: Simia-SFT和Simia-RL能够实现无需环境工程的可扩展代理训练，用灵活的基于LLM的模拟替代繁重易碎的环境实现。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [239] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够以人类方式实时导航VR环境并交互，用于自动化VR游戏测试。


<details>
  <summary>Details</summary>
Motivation: 传统人工VR内容质量保证劳动密集且无法规模化，而现有自动化测试方法难以适应VR的高维感官输入和实时性能要求。

Method: 使用增强型动作分块变换器从人类演示中学习，预测多步动作序列，并引入动态可调滑动视界来平衡响应性和精度。

Result: 在商业VR游戏中达到专家级性能，仅需有限训练数据，并在消费级硬件上保持60 FPS的实时推理。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [240] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)特征选择和对比提示方法，在Llama-3 8B模型上实现了安全性能提升18.9%同时实用性提升11.1%，突破了传统安全-实用性的权衡限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要调整模型权重且过程昂贵，而当前的SAE方法缺乏系统性的特征选择方法和原则性的安全-实用性权衡评估。

Method: 使用稀疏自编码器(SAE)提取可解释特征，采用创新的对比提示方法从AI-Generated Prompts Dataset和Air Bench eu-dataset中高效选择最佳特征进行引导。

Result: 在Llama-3 8B模型上，安全性能提升18.9%，同时实用性提升11.1%。

Conclusion: 通过原则性特征选择方法，定向SAE引导可以克服传统安全-实用性权衡，实现安全性和实用性的同步提升。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [241] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器遗忘技术虽然能有效移除不良知识，但会意外产生"知识空洞"，导致良性知识丢失。研究发现高达98.7%的测试案例在遗忘模型中出现无关或荒谬回答，凸显了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术虽然能有效移除不良内容且不影响标准基准测试性能，但可能无意中造成良性知识的丢失，这种"知识空洞"问题在现有评估中难以被发现。

Method: 提出了一个测试用例生成框架，探索遗忘内容的直接邻域和更广泛的潜在失败区域，以检测遗忘模型中的知识空洞问题。

Result: 评估显示遗忘存在显著隐藏成本：高达98.7%的测试案例在遗忘模型中产生无关或荒谬回答，而这些在预训练模型中本可正确回答。

Conclusion: 需要重新思考机器遗忘中知识保存的评估方法，超越传统的静态基准测试，以更全面地检测知识空洞问题。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [242] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出Skip-Block Routing(SBR)框架，用于Transformer类神经算子，通过路由机制学习token复杂度并选择性跳过部分计算块，在保持精度的同时减少约50%计算量。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在处理大规模工程问题时存在显著计算开销，且物理场复杂度差异大但模型采用统一计算成本，导致效率低下。

Method: SBR框架使用路由机制学习token复杂度和排名，在推理时根据排名决定后续层中传递多少token，使模型更关注复杂区域。

Result: SBR可集成到多种神经算子中，减少约50%FLOPs计算量，推理速度提升达2倍，且不牺牲精度。

Conclusion: SBR是通用的高效框架，能显著提升神经算子的计算效率，特别适合处理物理场复杂度差异大的问题。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [243] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出基于神经架构搜索(NAS)的自动化框架，用于发现平衡计算效率、预测性能和泛化能力的时间序列模型，用于能源生产的全局多步短期预测。


<details>
  <summary>Details</summary>
Motivation: 能源领域需要预测准确性和运行时效率，但手动配置复杂方法耗时且易出错，同时需要考虑时间序列的动态特性和对未见数据的泛化能力。

Method: 设计基于NAS的框架，构建仅包含高效组件的搜索空间，制定考虑时间上下文性能泛化和高维搜索空间探索的新目标函数。

Result: 在能源生产时间序列上，NAS发现的轻量级架构集成在效率和准确性方面均优于Transformer等最先进技术及预训练预测模型。

Conclusion: NAS框架能够自动发现高效且准确的预测模型，解决了能源预测中的计算效率和泛化问题。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [244] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，通过少量配对偏好标签和大量未配对样本同时学习，显著降低数据获取成本。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量配对反馈数据，导致资源消耗巨大，需要更高效的数据利用方法。

Method: 基于理论证明存在最优奖励阈值可高概率区分胜负响应，据此对未配对数据进行伪标注，从大规模未配对数据中提取潜在偏好。

Result: 在多个数据集上的实验验证了显著的数据效率，例如使用Llama3-8B-Instruct仅需1%的UltraFeedback数据就能超越使用10%数据的强基线方法。

Conclusion: SSPO方法能有效维持人类对齐效果，同时大幅降低数据获取成本，为偏好优化提供了更高效的数据利用方案。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [245] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 该研究验证了物理信息神经网络(PINNs)在求解工程和生物动力学系统ODE问题中的预测能力，通过系统调优损失函数权重和超参数，PINNs能够有效处理传统数值方法难以收敛的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚度、冲击、不规则域、奇异摄动、高维或边界不连续等复杂ODE问题时往往难以收敛，需要寻找更有效的替代方案。

Method: 使用经典ODE问题作为测试基准，系统评估PINNs的准确性、训练效率和泛化能力；通过平衡数据损失、初始条件损失和残差损失的权重，并系统调优网络深度、层宽、激活函数、学习率等超参数。

Result: 研究证明PINNs能够收敛到正确解，但需要适当平衡损失函数各组成部分的权重；系统调优超参数和嵌入先验知识能显著提升PINNs的预测能力。

Conclusion: PINNs通过将物理定律直接嵌入学习过程，能够有效处理传统数值方法难以解决的复杂ODE问题，但需要仔细平衡损失函数权重和系统调优超参数。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [246] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出了ReLaX-Net架构，通过时间复用层来扩展物理神经网络的有效深度，提高参数利用效率，仅需在现有PNN中添加快速开关即可实现。


<details>
  <summary>Details</summary>
Motivation: 物理神经网络在规模上落后数字神经网络几个数量级，需要参数高效架构来克服硬件限制，类似于早期数字神经网络发展中卷积神经网络的出现。

Method: 采用硬件友好的权重绑定方法，利用PNN中前向传播快速动态元件与可训练权重元件的时间尺度分离，通过层间时间复用方案增加有效网络深度。

Result: 在图像分类和自然语言处理任务上的数值实验表明，ReLaX-Net仅需对传统PNN进行微小修改即可提升计算性能，在相同参数数量下优于传统RNN或DNN。

Conclusion: ReLaX-Net架构通过简单的时间复用机制有效扩展了物理神经网络的规模和性能，展示了良好的扩展性，为下一代计算系统提供了有前景的解决方案。

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [247] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，结合Graph-BERT和GRU层来捕捉动态金融交易网络的时间演化，支持有向边，在比特币交易数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT模型主要针对静态图和无向边设计，但金融交易网络是动态的、有向的，需要能够捕捉时间演化的模型。

Method: 集成Graph-BERT与GRU层来建模多时间步的时序演化，修改底层算法以支持有向边，适用于动态金融交易分析。

Result: 在Elliptic数据集上评估，在市场关闭事件前后均表现出色，优于EvolveGCN和GCN，消融研究显示GRU组件对建模时序动态至关重要。

Conclusion: DynBERG能有效适应重大市场变化，GRU组件在捕捉金融交易时序动态中发挥关键作用，为动态金融欺诈检测提供了有效解决方案。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [248] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出一种利用时空结构的自监督学习框架，通过图神经网络和时空适应机制改进多变量天气预测，在ERA5和MERRA-2数据集上优于传统数值天气预报模型和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确稳健的天气预报仍然是一个基本挑战。

Method: 集成图神经网络进行空间推理，采用自监督预训练方案进行表示学习，并利用时空适应机制增强不同预测时长的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法相比传统数值天气预报模型和近期深度学习方法具有更优越的性能。在北京和上海的定量评估和视觉分析证实了模型捕捉细粒度气象模式的能力。

Conclusion: 该框架为未来数据驱动的天气预报系统提供了可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [249] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合LoRA和并行适配器的优势，在保持相似参数预算的同时显著提升精度和降低延迟


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模日益增长，高效训练和微调变得至关重要。虽然参数高效微调方法已被广泛研究，但仍有很大自由度未被探索

Method: 提出融合前向-后向适配器(FFBA)，结合LoRA和并行适配器的思想，通过将前向和后向适配器融合到基础模型的投影层中来最小化延迟

Result: 实验结果显示，在相似参数预算下，提出的FFB适配器在精度和延迟方面都显著优于常用的LoRA方法

Conclusion: FLoRA方法在参数高效微调中实现了更好的精度和更低的延迟，为大语言模型的高效微调提供了有效解决方案

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [250] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的成功机制，发现其通过增加权重更新矩阵的奇异值熵来提升性能，但存在计算开销大的问题。作者提出了一个统一的PEFT框架，并基于此开发了Pre-Diag和SORA两种新方法，在性能和效率上都优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA作为基于LoRA的重要PEFT方法，虽然性能优异但机制不明确且计算开销大。作者希望深入理解DoRA的工作原理，并提出更高效且性能更好的参数高效微调方法。

Method: 首先揭示了DoRA的成功机制在于增加权重更新矩阵的奇异值熵，然后将其重新表述为更高效的矩阵形式。基于此提出了统一的PEFT框架，并开发了两种新方法：Pre-Diag（在LoRA更新前应用对角条件矩阵）和SORA（使用参数高效的正交旋转进行特征空间变换）。

Result: 在自然语言理解和生成任务上的广泛实验表明，提出的Pre-Diag和SORA方法在性能和效率上都优于LoRA和DoRA基准方法。

Conclusion: 本文不仅阐明了DoRA的工作机制，还提出了一个统一的PEFT设计框架，并基于此开发了两种高效且性能优越的新方法，为参数高效微调领域提供了新的思路和工具。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [251] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估了特征引导分析(FGA)在MNIST和LSC数据集基准测试中的适用性，发现FGA在精度上优于文献结果，且神经网络架构、训练和特征选择对FGA的召回率有显著影响，但对精度影响不大。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策原因对于安全关键应用至关重要。现有特征引导方法通过监控神经元激活来提取相关规则，但工业应用需要更多实证证据。

Method: 在MNIST和LSC数据集基准上评估FGA的有效性，分析神经网络架构、训练过程和特征选择对FGA性能的影响。

Result: FGA在基准测试中表现出比文献结果更高的精度。神经网络架构、训练和特征选择对FGA的召回率有显著影响，但对精度影响可忽略。

Conclusion: FGA在解释神经网络行为方面具有实际应用价值，特别是在精度要求高的场景中，但需注意架构和特征选择对召回率的影响。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [252] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的二次形式加权训练目标，通过考虑标签自相关效应和异质任务权重，改进了时间序列预测模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标（如均方误差）将每个未来步骤视为独立、等权重的任务，忽视了标签自相关效应和不同预测任务的异质性，限制了预测性能。

Method: 提出二次直接预测（QDF）学习算法，使用自适应更新的二次形式加权矩阵进行训练，其中非对角线元素考虑标签自相关，非均匀对角线匹配不同未来步骤的最优权重。

Result: 实验表明QDF有效提升了各种预测模型的性能，达到了最先进的结果。

Conclusion: QDF通过同时解决标签自相关和任务权重异质性问题，显著改善了时间序列预测模型的训练效果。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [253] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决了视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。微调小型可部署模型是高效途径，但缺乏高质量的分步推理数据成为主要瓶颈。

Method: 开发了SpatialTraceGen框架，通过自动化验证器从大型教师模型蒸馏推理过程，生成多跳、多工具推理轨迹，验证器可扩展地确保每个推理步骤的保真度。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程使轨迹平均质量得分提高17%，质量方差降低超过40%。

Conclusion: SpatialTraceGen提供了专家轨迹数据集，为有效微调和样本高效的离线强化学习提供了结构化、分步的工具使用示例。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [254] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习在无人机热成像图像分割任务中的实际应用效果，比较了不同FL方法与集中式学习的性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和技术限制，分布式数据无法集中存储和共享，联邦学习能够绕过传统集中式机器学习的限制，让参与者在本地训练模型而无需共享数据。

Method: 在真实部署场景中评估FL算法，比较多种FL方法与集中式学习基线，分析模型精度、训练时间、通信开销和能耗等关键指标，并探索客户端控制和工作流服务器控制等不同FL工作流程。

Result: 研究发现FL在无人机热成像图像分割任务中具有实际应用价值，但面临数据非独立同分布和特征特性差异等挑战。

Conclusion: 该研究为理解FL方法在无人机成像分割任务中的实际应用和局限性提供了有价值的参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [255] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出MISA方法，将Transformer层划分为更小的模块并分配重要性分数，通过加权随机采样激活模块，相比逐层优化能减少梯度方差并节省内存。


<details>
  <summary>Details</summary>
Motivation: 现有逐层优化方法虽然节省内存，但忽略了层内模块的重要性差异，且内存节省有限。需要更细粒度的优化方法来提升性能并进一步降低内存需求。

Method: 将每个Transformer层划分为小模块，为每个模块分配重要性分数，使用加权随机采样机制选择激活模块进行优化，同时冻结其他模块。

Result: 实验证明MISA在多种学习任务中有效，理论上证明了O(1/√K)的收敛率，内存分析显示其优于现有基线方法。

Conclusion: MISA通过模块级重要性采样实现了更细粒度的优化，在保证收敛的同时显著降低了内存需求，是大型语言模型内存高效优化的有效方法。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [256] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半神经元可通过紧凑的规则决策树准确描述，并通过干预实验验证了这些模式的重要性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT模型为可解释性研究提供了理想测试平台，它既具有丰富的计算模式，又基于规则游戏逻辑，便于进行有意义的逆向工程。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，通过针对性干预验证因果相关性。

Result: 发现第5层约一半神经元（913/2048）可由紧凑的规则决策树准确描述（R² > 0.7），其余可能参与分布式或非规则计算。干预实验显示特定模式的神经元消融会导致模型预测能力显著下降5-10倍。

Conclusion: 该方法能有效识别模型中的规则编码神经元，验证了其因果重要性，并提供了Python工具支持未来研究，证明可解释性方法能够恢复有意义的计算结构。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [257] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA是一种基于密度-方差的聚类算法，将聚类形成视为最近邻图上的自适应演化过程，通过局部统计反馈替代固定密度阈值，具有对数线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在限制性假设：K-Means和高斯混合模型假设凸的类高斯簇，而DBSCAN和HDBSCAN能捕获非凸簇但对参数高度敏感。

Method: 在最近邻图上通过广度优先搜索扩展根图，使用持续更新的局部距离和形状统计作为指导，采用空间索引技术。

Result: 在多种合成、真实世界、低维和高维数据集上，EVINGCA表现出与基线方法竞争的性能。

Conclusion: EVINGCA通过自适应演化过程和局部统计反馈机制，克服了传统聚类算法的限制性假设和参数敏感性问题。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [258] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 研究探索预训练模型的不同层如何反映大脑处理语言的层级结构，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声学到丰富多模态关联的层级处理过程启发，研究旨在了解预训练模型的哪些层能最好地反映大脑的这种分层处理机制。

Method: 使用自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2和CLIP模型嵌入与大脑活动的对齐程度，测试了三种策略：单独层、渐进连接和渐进求和。

Result: 研究发现结合多模态、层级感知的表征可能更接近解码大脑如何理解语言——不仅是声音，更是体验。

Conclusion: 多模态层级表征的结合为理解大脑语言处理机制提供了新视角，超越了单纯的声音处理层面。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [259] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出了TR-GRPO方法，通过基于token概率的权重调节来解决GRPO中低概率token梯度主导的问题，提升强化学习训练的稳定性


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在低概率token梯度过大导致训练不稳定、抑制高概率token贡献的问题

Method: 在GRPO基础上引入token级别权重调节，权重与模型预测概率正相关，降低低概率token权重，增强高概率token作用

Result: 在逻辑、数学和智能体推理等RLVR任务上，TR-GRPO均优于GRPO

Conclusion: token贡献调节对RL训练很重要，TR-GRPO是增强LLM推理能力的稳健框架

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [260] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 提出了一种无需显式域标签的领域泛化方法，通过潜在域聚类和特征融合来提升视觉语言模型在未见目标域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的领域泛化方法大多依赖域标签，但这些标签在实际应用中可能不可得且模糊。本文研究无需显式域标签的领域泛化设置。

Method: 在图像特征上进行潜在域聚类，并根据输入图像与每个潜在域的相似度融合域特定的文本特征。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线模型获得了持续的性能提升。

Conclusion: 该方法为在域偏移下提高模型鲁棒性提供了新的思路，能够自适应地在域间传递知识。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [261] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 本文研究LLMs在约束多目标回归任务中的表现，比较了贝叶斯优化框架与微调LLMs的性能。结果显示专用BO框架在收敛性上最优，但微调LLMs是快速有前景的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在约束连续高维数值空间任务中的效用，这些任务并非LLMs专门设计的领域，特别是在材料信息学中的逆向设计问题。

Method: 进行贝叶斯优化框架（BoTorch Ax和qEHVI）与微调LLMs（使用PEFT方法）的对比研究，将问题构建为带自定义输出头的回归问题。

Result: BoTorch qEHVI实现完美收敛（GD=0.0），最佳LLM（WizardMath-7B）GD=1.21，显著优于传统BoTorch Ax基线（GD=15.03）。

Conclusion: 专用BO框架仍是性能领导者，但微调LLMs被验证为有前景的快速计算替代方案，为AI驱动优化领域提供重要比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [262] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 使用小波特征提取和无监督聚类的方法来检测数字的奇偶性，无需标签监督即可达到约69.67%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 探索将经典信号处理技术应用于纯离散符号领域，通过特征工程和聚类方法解决非传统机器学习问题，连接符号推理和基于特征的学习

Method: 将整数转换为小波域表示，提取多尺度统计特征，然后使用k-means算法进行无监督聚类

Result: 在特征空间中揭示了奇数和偶数之间的有意义结构差异，无监督分类准确率达到约69.67%

Conclusion: 经典信号处理技术即使在纯离散符号领域也能发现潜在结构，为特征工程和聚类在非传统机器学习问题中的应用提供了新视角

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [263] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种视觉语言模型，能够将象形文字图像反编译为几何图元程序，展示了模型能够学习抽象的几何语法，并在零样本情况下重建古代甲骨文。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型理解视觉信息几何结构的能力，特别是对象形文字这种结合视觉形式和符号结构的字符的识别能力。

Method: 将视觉识别任务构建为程序合成任务，训练视觉语言模型将栅格图像反编译为由贝塞尔曲线组成的几何图元程序。

Result: 模型性能优于包括GPT-4o在内的强零样本基线，最显著的发现是仅在现代汉字上训练的模型能够在零样本情况下重建古代甲骨文。

Conclusion: 模型获得了抽象且可迁移的几何语法，超越了像素级模式识别，实现了更结构化的视觉理解。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [264] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，为构建可复现的通用机器学习流水线提供模块化和可扩展框架，特别关注算法公平性领域。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域快速发展，新指标、缓解策略和机器学习方法不断涌现。现有工具要么关注单一干预，要么将可复现性和可扩展性作为次要考虑。

Method: 引入统一架构，包含数据分割、执行、预处理、训练、处理中、后处理、评估和报告等标准化引擎。每个引擎封装一个方法任务，通过轻量级接口通信。

Result: 构建了透明、可审计且易于扩展的工作流，支持公平性方法的集成、比较和评估，并可推广到可解释性、鲁棒性和合规性指标。

Conclusion: 虽然受公平性驱动，但flowengineR为任何需要可复现性、透明度和可扩展性的工作流环境提供了通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [265] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出Fix-GCN模型，通过固定点迭代图卷积网络增强图神经网络对抗攻击的鲁棒性，利用谱调制滤波器选择性衰减高频分量，保护低频结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击严重威胁图神经网络的完整性和性能，特别是在图结构和节点特征易受操纵的任务中，需要开发有效的防御机制。

Method: 引入通用谱调制滤波器，通过固定点迭代推导特征传播规则，实现高阶邻域信息捕获，提供灵活通滤波方法，无需额外内存或计算复杂度。

Result: 在多个基准图数据集上的广泛实验证明了该模型的有效性，展示了其对抗攻击的韧性。

Conclusion: Fix-GCN通过迭代更新节点表示，提供了一个灵活高效的框架，在保护基本图信息的同时减轻对抗操纵的影响。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [266] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 本文研究了使用序数回归技术自动预测纸笔RPG中怪物等级的方法，以解决当前依赖人工测试和专家评估的低效问题。


<details>
  <summary>Details</summary>
Motivation: 纸笔RPG市场快速增长，公司寻求AI技术提升玩家体验和竞争力。当前怪物等级设计依赖耗时的人工测试和专家评估，需要自动化解决方案。

Method: 构建专门的等级估计数据集，开发基于人类经验的基准模型，设计基于领域知识的专门评估程序，比较机器学习算法与传统方法。

Result: 提出了完整的等级预测框架，包括数据集构建、基准模型开发和专门评估方法，为自动化等级估计提供了可行方案。

Conclusion: 该研究为纸笔RPG出版商提供了有效的自动化等级预测方法，能够显著提高效率并减少对人工测试的依赖。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [267] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 提出MaGNet模型，结合Mamba双超图网络进行股票预测，通过三个创新模块实现优越的预测性能和投资回报


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉时间依赖性和动态股票间交互，忽略横截面市场影响，依赖静态相关性，对节点和边采用统一处理，混淆多样关系

Method: 1) MAGE模块：双向Mamba与自适应门控机制进行上下文时间建模，稀疏MoE层适应不同市场条件，多头注意力捕捉全局依赖；2) 2D时空注意力模块：精确融合多变量特征和跨股票依赖；3) 双超图框架：TCH捕捉细粒度因果依赖，GPH建模市场范围模式

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面优于最先进方法，具有稳健的风险管理能力

Conclusion: MaGNet通过创新的Mamba双超图网络架构，有效解决了股票趋势预测中的时间依赖性和动态交互问题，实现了卓越的性能

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [268] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了Agent-REINFORCE框架，通过LLM代理增强的搜索方法，在固定计算预算下寻找最优的多LLM协作图结构和模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽视了不同任务可能需要不同的最优架构和模型组合。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过采样-反馈-更新流程，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效找到准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法成功解决了测试时扩展中多LLM协作图的优化问题，为不同任务定制最优计算分配提供了有效解决方案。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [269] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解缠和保留机制来处理嵌入漂移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习（跨多个图域更新模型）随着图基础模型的发展变得至关重要，但在文献中尚未被探索。

Method: GraphKeeper方法包括：1）域特定参数高效微调及域内和域间解缠目标来防止嵌入漂移；2）无偏差知识保留来维持稳定决策边界；3）对于不可观测域的图，使用域感知分布判别来获得精确嵌入。

Result: 大量实验表明GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘可以忽略不计。该方法可以无缝集成到各种代表性图基础模型中。

Conclusion: GraphKeeper有效解决了图域增量学习中的灾难性遗忘问题，具有广泛的应用潜力。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [270] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的无监督损伤检测和数字孪生方法，无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时预测效果不佳，需要开发不依赖先验信息的无监督框架。

Method: 使用条件标签生成对抗网络，将不同损伤级别的测量数据作为输入，强制模型收敛到不同损伤状态，通过比较收敛分数识别不同损伤状态。

Result: 该方法能准确捕获健康测量数据中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 该无监督框架在损伤检测和数字孪生方面优于现有方法，特别适用于实际应用中损伤状态未知的情况。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [271] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构荷载识别中的表现，并与基于物理的残差卡尔曼滤波器进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中由于测试数据量少或结构模型不可识别导致的动态荷载识别不确定性问题。

Method: 使用模拟结构在顶层激振器激励下的测试、加州建筑在地震基础激励下的测试，以及IASC-ASCE结构健康监测基准问题的冲击和瞬时荷载条件测试。

Result: 不同方法在不同荷载场景下表现各异，在物理参数可识别的情况下，残差卡尔曼滤波器优于神经网络。

Conclusion: 各种方法在不同工况下各有优势，残差卡尔曼滤波器在物理参数可识别的情况下表现最佳。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [272] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化多LoRA框架，统一了LoRA微调和推理，通过虚拟化模块和优化计算流实现高效批处理和最小化内核调用开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统一LoRA微调和推理方面存在不足，需要一种能够无缝集成微调和服务的框架。

Method: 提出虚拟化模块隔离PEFT修改并支持多适配器共享基础模型，设计优化计算流将微调和推理路径在前向传播中合并。

Result: 在三个任务设置上的实验显示，Loquetier在性能和灵活性上均优于现有基线，推理任务吞吐量达到最先进共服务系统的3.0倍，统一微调和推理任务的SLO达成率比PEFT高46.4倍。

Conclusion: Loquetier成功统一了LoRA微调和推理，提供了高效且灵活的解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [273] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出了一种从噪声轨迹数据中自动发现守恒量的混合框架，结合神经ODE、Transformer和符号数值验证器，显著优于直接在轨迹数据上操作的基线方法。


<details>
  <summary>Details</summary>
Motivation: 守恒定律的发现是科学进步的基础，但从观测数据中识别这些不变量仍然是一个重大挑战。

Method: 集成三个组件：(1) 学习系统动力学连续模型的神经ODE，(2) 基于学习向量场生成符号候选不变量的Transformer，(3) 为候选不变量提供强数值验证的符号-数值验证器。

Result: 在典型物理系统上测试表明，该框架显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 这项工作证明了分离的'先学习后搜索'方法在从不完美数据中发现数学原理方面的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [274] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是当前最大规模的开源具身多模态大脑模型，参数规模从70亿到720亿，在1000+ A800 GPU集群上训练，性能比基础模型提升20.3%，超越100B级开源模型10.6%。


<details>
  <summary>Details</summary>
Motivation: 将强大智能嵌入各种具身系统中，开发开源具身多模态大脑模型。

Method: 采用DPPO（刻意练习策略优化）框架，通过metaloop（RL-精炼-诊断-SFT循环）训练模型，从40亿+ token原始数据集中蒸馏高质量数据集。

Result: 在知名具身基准测试中达到领先专有系统水平，性能显著提升。

Conclusion: Pelican-VL 1.0成功实现了数据能力与智能自适应学习机制的深度集成，成为当前最先进的开源具身大脑模型。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [275] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了MeixnerNet，一种基于离散正交多项式（Meixner多项式）的谱图神经网络，通过可学习的形状参数自适应调整滤波器，解决了传统连续域滤波器与离散图结构不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱GNN使用连续正交多项式（如Chebyshev）作为滤波器，但这些连续域滤波器与离散图结构存在理论不匹配，可能导致性能不佳和对超参数设置的脆弱性。

Method: 使用Meixner离散正交多项式作为滤波器基础，将多项式的两个形状参数β和c设为可学习参数，使滤波器能根据图的谱特性自适应调整。通过Laplacian缩放和逐基LayerNorm解决数值不稳定问题。

Result: 在K=2的最优设置下，MeixnerNet在3个基准测试中的2个上优于ChebyNet基线。更重要的是，MeixnerNet对多项式阶数K的变化表现出卓越的鲁棒性，而ChebyNet对此高度脆弱且性能会崩溃。

Conclusion: MeixnerNet通过使用离散正交多项式解决了谱GNN中连续域滤波器与离散图结构的不匹配问题，不仅实现了竞争性性能，还显著提高了对关键超参数K的鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [276] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个可持续液体冷却基准环境，用于强化学习控制策略，优化高性能计算系统的能效液体冷却。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，高密度数据中心需要液体冷却进行热管理，而基于机器学习的控制器对于提高能效和可靠性至关重要。

Method: 基于橡树岭国家实验室Frontier超级计算机冷却系统的高保真数字孪生，提供详细的Modelica端到端模型，通过Gymnasium接口让RL代理优化关键热控制参数。

Result: 创建了多目标实时优化挑战，平衡局部热调节和全局能效，支持集中式和分散式多智能体RL方法，并实现可解释控制。

Conclusion: LC-Opt为ML社区、运营商和供应商提供了详细、可定制的液体冷却模型，促进可持续数据中心液体冷却控制解决方案的开发。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [277] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真模拟基准，用于可持续的地理时间任务调度，结合真实世界数据集和物理信息模型，支持多目标优化研究。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的能源需求和碳足迹不断增加，需要智能的全球分布式数据中心工作负载管理，但缺乏能真实捕捉环境因素、数据中心物理和网络动态相互作用的基准。

Method: 结合精选的真实世界数据集（AI工作负载痕迹、电网碳强度、电力市场、天气等）与物理信息化的数据中心运营模型，提供模块化奖励系统和Gymnasium API。

Result: 提供了一个具有挑战性的调度问题环境，支持多目标优化研究，包括碳排放、能源成本、服务水平协议和用水之间的权衡分析。

Conclusion: DCcluster-Opt通过提供真实、可配置且可访问的测试平台，加速了下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [278] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发了一个基于机器学习的模型来预测足球中的防线突破，使用XGBoost分类器分析球员位置、速度和空间配置等特征，模型预测准确率高，揭示了防线突破与射门机会创造的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 足球中攻击方突破对手防线的行为是衡量进攻有效性和战术表现的关键指标，但以往研究主要关注射门或进球机会，缺乏对如何突破防线的系统性分析。

Method: 使用2023年J1联赛赛季的事件和追踪数据，构建包含189个特征的机器学习模型，采用XGBoost分类器预测防线突破概率，特征包括球员位置、速度和空间配置等。

Result: 模型预测准确率很高，AUC达到0.982，Brier分数为0.015。SHAP分析显示进攻球员速度、防线间隙和进攻球员空间分布是影响防线突破的关键因素。预测的防线突破概率与球队失球射门和传中次数呈中度正相关。

Conclusion: 防线突破与得分机会创造密切相关，该研究为理解足球战术动态提供了量化框架，有助于更深入地分析进攻战术效果。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [279] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 使用统计物理中的交叉涨落分析扩散模型采样动态，发现样本经历尖锐离散转变，形成目标分布。这些转变可通过交叉涨落检测，提升采样效率并改善零样本任务。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中采样动态的演变过程，理解样本从初始分布到目标分布的转变机制，为提升采样效率和任务性能提供理论基础。

Method: 采用统计物理中的交叉涨落统计量分析采样动态，推导方差保持SDE的交叉涨落闭式解，检测离散转变点。

Result: 检测到采样过程中的离散转变，显著提升采样效率，加速条件生成和稀有类别生成，改善图像分类和风格迁移等零样本任务性能。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动力学，连接了经典耦合混合理论与现代生成建模，为扩散模型分析提供了新视角。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [280] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，通过内部模型信号自适应选择最可靠的轨迹预测器，在nuPlan-mini数据集上实现FDE 2.567m，比GameFormer降低9.5%。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，传统"一刀切"范式存在局限性，物理模型有时优于先进网络。

Method: 动态多专家门控框架，基于内部模型信号（稳定性和不确定性）自适应选择物理信息LSTM、Transformer和微调GameFormer中的最佳预测器，将专家选择建模为成对排序问题。

Result: 在nuPlan-mini数据集上FDE为2.567m，比GameFormer降低9.5%，达到oracle性能的57.8%；左转场景中FDE降低约10%。

Conclusion: 自适应混合系统能提升安全关键自动驾驶中的轨迹可靠性，为超越静态单模型范式提供了实用路径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [281] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了多种数据增强预处理方法在基于AlexNet的套管接箍识别模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 精确的井下深度测量对油气井作业至关重要，但CCL信号识别的预处理方法发展不足，且真实井数据有限，这给需要大量数据集的神经网络模型训练带来了挑战。

Method: 开发了集成到井下工具中的CCL信号采集系统，提出了包括标准化、标签分布平滑(LDS)、随机裁剪、标签平滑正则化(LSR)、时间缩放和多重采样在内的综合预处理方法，并使用基于AlexNet的神经网络模型评估这些方法。

Result: 实验表明，标准化、LDS和随机裁剪是模型训练的基本要求，而LSR、时间缩放和多重采样显著增强了模型的泛化能力。两个基准模型的F1分数分别从0.937和0.952最大提升到1.0和1.0。

Conclusion: 这项工作解决了在CCL数据有限环境中训练套管接箍识别模型时数据增强方法的空白，验证了所提方法的有效性和实际适用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [282] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 比较监督微调(SFT)、LoRA和上下文学习(ICL)在数据稀缺场景下的表现，发现LoRA在平衡新技能学习和基础模型知识保留方面最有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要针对特定应用进行定制，但完全微调计算成本高且会导致灾难性遗忘。需要找到平衡技能获取和知识保留的最佳适应策略。

Method: 在数据稀缺场景下对SFT、LoRA和ICL三种适应方法进行对比分析，评估它们在技能获取和知识保留方面的表现。

Result: LoRA在技能获取和基础知识保留之间达到最佳平衡；SFT擅长技能获取但易发生灾难性遗忘；ICL适合事实知识整合但难以处理复杂技能。

Conclusion: 提供了选择LLM适应策略的实用框架，强调了技能获取与知识整合的关键区别，阐明了任务特定性能与通用能力保留之间的权衡关系。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [283] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种增强随机森林分类器的新框架，通过概率特征采样和模拟退火超参数调优，显著提升预测准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决传统随机森林在处理多领域分类问题时的局限性，特别是在信用风险评估、物联网异常检测、早期医疗诊断和高维生物数据分析等复杂场景中的挑战

Method: 结合概率特征采样（关注对分类更有意义的特征）和模拟退火算法进行动态超参数调优

Result: 实现了持续的准确率提升，并提供了对特征相关性的有意义的洞察

Conclusion: 重要性感知采样与元启发式优化的结合在提升随机森林性能方面具有显著效果

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [284] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 该研究揭示了城市绿化在降温与湿度增加之间的权衡关系，发现在高植被覆盖度条件下，植被的生理活动会提高湿度，反而加剧热应激。


<details>
  <summary>Details</summary>
Motivation: 城市绿化降温效果不均，因为植被在降温的同时也会增加空气湿度，导致体感温度上升。目前对植被如何调节这种权衡关系了解不足，缺乏有效的政策指导。

Method: 使用机器学习框架（SHAP和ALE）分析138个印度城市的植被结构与功能对热指数的影响，涵盖不同气候区和城市密度区域。

Result: 当EVI≥0.4、LAI≥0.05时降温效果增强，但当EVI≥0.5、LAI≥0.2、fPAR≥0.5时开始转向增温，在潮湿密集区域fPAR≥0.25时就会发生逆转。

Conclusion: 确定了植被驱动降温的气候限制，为制定气候特异性绿化策略提供了量化阈值，以促进公平和热韧性城市建设。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [285] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight是一种基于双LLM架构的交通信号控制系统，通过Herald引导提示增强，结合实时交通信息提取和队列长度预测，实现精细化的信号控制决策，并在多个真实场景中显著降低平均旅行时间和队列长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的交通信号控制方法存在固定信号时长和幻觉错误的问题，而传统强化学习方法在信号时序决策上缺乏鲁棒性和泛化能力。

Method: 提出HeraldLight双LLM架构：Herald模块提取上下文信息并预测各交通相位队列长度；LLM-Agent基于预测进行精细信号控制；LLM-Critic修正LLM-Agent的输出错误和幻觉；通过评分微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用济南(12)、杭州(16)和纽约(196)共224个交叉口的真实数据集，HeraldLight优于现有最佳基线方法，在所有场景中平均旅行时间减少20.03%，在济南和杭州场景中平均队列长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示有效解决了现有方法的局限性，在交通信号控制中实现了更好的优化效率和鲁棒性。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [286] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与智能粒子群优化的决策模型，用于提升后端集中式冗余供应链的决策制定和规划效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高后端集中式冗余供应链的决策制定和规划效率，需要构建更智能化的优化模型。

Method: 构建分布式节点部署模型和最优规划路径；使用卷积神经网络从历史数据中提取特征，线性规划捕获高阶统计特征；采用模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化；采用"深度学习特征提取-智能粒子群优化"混合机制指导全局优化。

Result: 仿真结果显示资源消耗减少，空间规划增强，在动态环境中改善了实时决策调整、配送路径优化和鲁棒智能控制。

Conclusion: 该集成模型有效提升了供应链决策效率和规划性能，在动态环境下表现出良好的适应性和鲁棒性。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [287] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 本研究评估稀疏自编码器(SAEs)在识别和控制Gemma-2模型中种族与污名化概念关联的能力，发现SAEs可识别与黑人个体相关的潜在变量，但通过SAE引导缓解偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用带来风险，可能加剧现有偏见。研究旨在探索如何识别LLMs是否错误依赖患者种族信息进行预测。

Method: 使用稀疏自编码器识别Gemma-2模型中与黑人个体相关的潜在变量，并通过激活这些潜在变量来引导模型生成关于黑人患者的输出。

Result: 发现与黑人相关的潜在变量在合理输入序列和问题词汇上均会激活，激活该潜在变量会增加模型将患者标记为'好斗'的风险概率。

Conclusion: SAEs可作为识别LLMs在临床应用中问题性依赖人口统计信息的工具，但通过SAE引导缓解偏见在现实复杂临床任务中效果有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [288] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP框架通过用更便宜的LLM推理替代昂贵的科学计算，减少PDE求解器生成的计算成本，在减少60-75%计算评估的同时实现更高的求解器精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的PDE求解器生成方法需要大量求解器样本来识别高精度求解器，对于复杂PDE尤其耗费计算资源。

Method: 采用三阶段框架：(1)分析阶段：数学思维链分析，包括PDE分类、解类型检测和稳定性分析；(2)生成阶段：基于数学洞察生成求解器；(3)合成阶段：通过LLM裁判的协作选择-混合锦标赛迭代优化实现。

Result: 平均只需不到13次求解器评估（基线方法需要30+次），在所有测试PDE上精度平均提高4倍，在不同LLM架构上均表现稳健。

Conclusion: PDE-SHARP显著降低了PDE求解器生成的计算成本，同时提高了求解器精度，展示了在科学计算中LLM推理替代昂贵数值评估的潜力。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [289] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了EL-MIA框架来审计LLM中实体级别的成员推理风险，发现现有MIA方法在敏感属性实体级成员推理方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法只能检测整个提示或文档是否在训练数据中，无法捕捉更细粒度的风险，特别是针对敏感信息（如PII、信用卡号等）的实体级成员风险。

Method: 提出了EL-MIA框架，构建了基准数据集，系统比较了现有MIA技术和两种新方法，分析了实体级MIA易感性与模型规模、训练轮次等因素的关系。

Result: 发现现有MIA方法在敏感属性实体级成员推理方面能力有限，但这种易感性可以通过相对简单的方法来识别，表明需要更强的对抗方法来测试威胁模型。

Conclusion: 需要开发更强大的对抗方法来充分测试LLM隐私威胁模型，现有MIA方法在实体级敏感信息成员推理方面存在不足。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [290] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出一个将对抗性提示优化转化为高效摊销推理任务的新框架，利用预训练的非自回归生成LLM直接条件生成提示，替代昂贵的逐实例离散优化


<details>
  <summary>Details</summary>
Motivation: 解决传统对抗性提示优化的资源密集问题，将离散优化转化为高效的摊销推理任务

Method: 使用预训练的非自回归生成LLM（如Diffusion LLMs）作为提示搜索的代理模型，直接条件生成提示，通过少量并行样本替代离散优化

Result: 生成的提示具有低困惑度、多样性，并能有效攻击各种黑盒目标模型，包括鲁棒训练和专有LLM

Conclusion: 该框架为红队测试、自动提示优化以及利用新兴流和扩散LLM开辟了新方向

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [291] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 扩散模型在药物发现中应用的系统综述，比较了在小分子和肽类药物设计中的不同挑战和优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为领先的生成建模框架，有望加速和改变传统缓慢且昂贵的药物发现过程。

Method: 采用统一的迭代去噪框架，针对小分子和肽类药物的不同分子表示、化学空间和设计目标进行适应性分析。

Result: 小分子设计擅长基于结构的设计，但面临化学可合成性挑战；肽类药物设计关注功能序列生成，但面临生物稳定性、正确折叠和免疫原性等问题。

Conclusion: 扩散模型在药物发现中的全部潜力需要通过弥合模态特定差距并将其整合到自动化DBTL平台中来实现，从而将范式从化学探索转向靶向创造新型疗法。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [292] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的多奖励信号微调基础模型的方法，通过跨多个奖励的迭代微调策略，在文本、生物序列和小分子生成等多个领域优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，如文本生成和药物发现，仅使用单一奖励信号进行优化可能不够理想，因为通常需要多个评估标准。

Method: 采用基于强化学习的多奖励信号微调方法，通过跨多个奖励的迭代微调策略来泛化最先进的RL方法。

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，所提出的算法相比最先进的基线方法具有更好的效果。

Conclusion: 多奖励RL微调方法在多个领域都表现出有效性，并通过理论分析提供了对多奖励RL微调性能的深入理解。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [293] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 提出了一种基于集成学习和可解释人工智能的黑色素瘤检测方法，通过结合三种先进的深度学习网络来提高诊断准确性，并使用XAI技术增强预测的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中的黑盒问题，提高黑色素瘤早期检测的可靠性和可信度，使AI决策对医生更透明。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用可解释人工智能技术来解释预测依据。

Result: 开发了一个能够高精度检测黑色素瘤的机器学习模型，同时通过XAI技术提供了预测的可解释性。

Conclusion: 集成学习结合XAI技术可以有效提高黑色素瘤检测的准确性和可靠性，为医疗AI诊断提供了更可信的解决方案。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [294] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾，通过证明与Kale(2014)上界匹配的下界，得出最优遗憾为Θ(√(TK log(N/K)))。


<details>
  <summary>Details</summary>
Motivation: 解决非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾界限问题，填补现有理论空白。

Method: 通过数学证明构建与现有上界匹配的下界，采用极小极大分析方法。

Result: 证明了最优期望遗憾为Θ(√(TK log(N/K)))，其中K为臂数，N为专家数，T为时间范围。

Conclusion: 该工作完整刻画了非随机多臂老虎机专家建议问题的极小极大最优性能界限。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [295] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 提出基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过整合车辆运动学约束生成更真实可行的轨迹，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: xLSTM架构在时间序列预测中表现出色，但尚未在车辆轨迹预测领域得到充分探索。传统LSTM存在局限性，而xLSTM通过指数门控和增强内存结构能更好地建模长期时间依赖。

Method: 开发了X-TRAJ框架及其物理感知变体X-TRACK，后者明确将车辆运动学约束整合到模型学习过程中，确保生成的轨迹具有物理可行性。

Result: 在highD和NGSIM数据集上的综合评估表明，X-TRACK优于最先进的基线方法，能够生成更真实和可行的车辆轨迹。

Conclusion: xLSTM架构在车辆轨迹预测中具有显著潜力，通过整合物理约束可以进一步提高预测性能，生成更符合实际运动规律的轨迹。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [296] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 该论文提出了一种基于领域知识的强化学习方法，用于控制混沌对流流动，在Rayleigh-Bénard对流系统中实现了对流热传输的显著降低。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但传统控制方法在混沌状态下往往失效。虽然强化学习在层流控制中表现出潜力，但在混沌和湍流动力学下的泛化能力和鲁棒性尚未充分探索。

Method: 使用近端策略优化训练领域知识增强的强化学习智能体，在多样初始条件和流动状态下进行训练。在奖励函数中引入鼓励Bénard单元合并的领域知识项，作为期望宏观属性的示例。

Result: 在层流状态下，领域知识增强的强化学习智能体将对流热传输降低了33%；在混沌流动状态下仍实现了10%的降低，显著优于传统控制器。领域知识奖励设计产生了稳定流动、更快的训练收敛以及无需重新训练即可跨流动状态泛化的能力。

Conclusion: 优雅的领域知识先验可以极大地增强强化学习控制混沌流动的鲁棒性，使实际部署更接近现实。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [297] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLMs的校准能力是一个分布式现象，在网络的深层存在专门的置信度修正阶段，通过扰动残差流中的低维校准方向可以显著改善校准指标而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络通常存在过度自信问题，但LLMs却表现出良好的校准能力，预测概率与正确性高度一致。本研究旨在探究校准能力如何在网络深度中演化，而不仅仅关注最终层的组件。

Method: 在MMLU基准上分析多个开源权重模型，研究校准在网络深度中的演化过程，识别置信度修正阶段，并发现残差流中的低维校准方向。

Result: 发现上层/深层存在明显的置信度修正阶段，模型在达到决策确定性后会主动重新校准置信度。扰动低维校准方向可显著改善ECE和MCE等校准指标，同时保持准确性不变。

Conclusion: 校准是一个分布式现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，这为理解LLMs内部置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [298] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 本文比较了8种不确定性量化技术在医疗时间序列数据上的表现，重点关注心房颤动检测和血压回归任务，发现不同方法在不同评估指标下的可靠性表现复杂，强调应根据实际应用场景选择合适的不确定性评估标准。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗时间序列数据上的应用存在部署性能不佳的风险，可靠的不确定性估计能为临床医生提供模型输出可信度的指导，因此需要比较不同不确定性量化方法的有效性。

Method: 实现了8种不确定性量化技术，应用于心房颤动检测（分类）和两种血压回归任务，制定了全面的评估程序来严格比较这些方法。

Result: 观察到不同技术的不确定性可靠性呈现复杂图景，最优方法取决于不确定性表达方式、评估指标和可靠性评估尺度。局部校准和适应性评估提供了更具实际意义的模型行为洞察。

Conclusion: 评估不确定性量化技术的标准应适应模型的实际使用场景，在患者数据有限的情况下，应优先考虑小尺度可靠性和保持预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [299] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 本文提出了一种混合生成框架，结合基于模型的协变量合成和单独学习的倾向得分与结果模型，确保合成数据保留因果结构，解决了现有方法在估计因果效应时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和GAN方法生成合成表格数据时，虽然预测保真度高，但往往无法准确保持关键因果参数如平均处理效应(ATE)，这限制了合成数据在因果分析中的应用。

Method: 采用混合生成框架：模型驱动的协变量合成（通过距离过滤监控）+ 单独学习的倾向得分和结果模型，确保(W, A, Y)三元组保持底层因果结构。还引入合成配对策略缓解正性违例问题。

Result: 提出的方法能够生成保持因果结构的合成数据，并通过利用无限合成样本的评估协议，在复杂协变量分布下对传统估计器(IPTW, AIPW, 替代法)进行基准测试。

Conclusion: 这项工作为支持稳健因果分析的LLM驱动数据管道奠定了基础，代码已开源。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [300] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出Pivot-Aware Speculative Decoding方法，通过识别关键token来提高解码接受率，在保持任务性能的同时实现2.5倍加速


<details>
  <summary>Details</summary>
Motivation: 传统Speculative Decoding要求完全匹配目标模型分布，导致接受率过低，限制了加速潜力。实际应用中任务性能比分布匹配更重要

Method: 提出枢轴感知推测解码，只拒绝会导致最终输出性能下降的关键token。训练轻量级分类器识别这些关键token

Result: 在多个数据集上评估，实现了最高2.5倍的加速，同时保持了可比的任务性能

Conclusion: 通过放宽分布匹配要求，专注于任务性能匹配，可以显著提高解码效率而不牺牲实用性

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [301] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏性的统一框架来评估算法公平性，该框架与现有公平性标准一致，并在多种数据集和偏置缓解方法上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中确保算法公平性是一个重要挑战，现有的公平性标准往往缺乏跨不同机器学习问题的通用性。

Method: 研究了各种稀疏性度量在促进公平性方面的联系和差异，提出了基于稀疏性的统一框架来评估算法公平性。

Result: 该框架与现有公平性标准一致，在多种数据集和偏置缓解方法上通过广泛实验证明了其作为评估指标的有效性。

Conclusion: 通过稀疏性和社会公平性的视角为算法公平性研究提供了新视角，对公平性研究和应用具有更广泛的影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [302] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 比较了模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的表现，发现在被试内实验中模糊模型表现更好，而在跨被试实验中深度学习模型泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性和可解释性难以兼顾的关键挑战，为BCI系统设计提供实用指导。

Method: 使用BCI Competition IV-2a数据集，ANFIS方法结合滤波器组共空间模式特征提取和粒子群优化的模糊IF-THEN规则，EEGNet直接从原始EEG数据学习层次时空表示。

Result: 被试内实验：模糊神经网络模型表现更好(68.58%±13.76%准确率，kappa=58.04%±18.43)；跨被试实验：深度学习模型泛化更强(68.20%±12.13%准确率，kappa=57.33%±16.22)。

Conclusion: 根据设计目标(可解释性或跨用户鲁棒性)选择合适的MI-BCI系统，未来基于transformer和混合神经符号框架的研究有望推进透明EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [303] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了PolyBERT的化学语言表示和分子图编码器的图表示，通过多模态嵌入实现高效检索和稳健排序。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合不同模态化学信息的多模态框架，以提升聚合物发现过程中的检索效率和排序准确性。

Method: 首先使用基于语言的相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性进行排序。

Result: 通过利用两种模态中的互补知识，PolyRecommender能够在相关聚合物属性上实现高效检索和稳健排序。

Conclusion: 这项工作建立了一个可泛化的多模态范式，推动了AI引导的下一代聚合物发现设计。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [304] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 提出UME-R1生成式多模态嵌入框架，通过两阶段训练策略统一嵌入任务到生成范式，在MMEB-V2基准测试中显著优于传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的嵌入方法仍然是判别式的，限制了其从推理驱动的生成范式中获益的能力。

Method: 采用两阶段训练策略：监督微调使模型具备推理能力，能生成判别式和生成式嵌入；强化学习进一步优化生成式嵌入质量。

Result: 在MMEB-V2基准的78个任务中显著优于传统判别式嵌入模型，生成式嵌入通过利用MLLMs的强大生成推理能力带来性能提升。

Conclusion: 生成式嵌入为更可解释、推理驱动的生成式多模态嵌入奠定了基础，展示了推理时扩展潜力。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [305] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient-Guided Sampling (GGS)方法解决对抗攻击在迁移场景中的利用与探索困境，通过梯度引导采样平衡攻击强度与跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型架构迁移时面临利用（最大化攻击强度）与探索（增强跨模型泛化）的基本困境。传统方法过度强调利用而削弱泛化，新方法过度强调探索而削弱攻击强度。

Method: 基于MI-FGSM，引入内迭代随机采样，使用前一次内迭代的梯度指导采样方向，采样幅度由随机分布决定，使对抗样本位于平衡区域。

Result: 在多种DNN架构和多模态大语言模型上的综合实验表明，该方法优于最先进的迁移攻击方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了对抗攻击的利用与探索目标，在保持攻击强度的同时提升了跨模型泛化能力。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [306] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training方法，通过树打包和梯度恢复技术，在代理LLM训练中重用共享前缀的计算，显著提高训练效率，减少高达3.9倍的总训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树状轨迹分解为独立的线性段，导致共享前缀在前后向传播中重复计算，造成计算效率低下。

Method: 采用树打包技术重用轨迹间的共享计算，并通过梯度恢复确保重用前缀的正确梯度传播。

Result: 在多个开源模型上的实验表明，总训练时间最多减少3.9倍。

Conclusion: Tree Training范式能够显著提高大规模代理训练的计算效率，支持更高效的代理LLM SFT和RL训练。

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [307] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 提出了一种结构保持的PINN框架，用于求解非线性KdV方程，通过将质量和哈密顿能量守恒嵌入损失函数，结合正弦激活函数，实现了物理一致性和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在长期积分中难以保持物理不变量，特别是在处理非线性色散波传播问题时。

Method: 将质量守恒和哈密顿能量守恒直接嵌入损失函数，使用正弦激活函数增强谱表达能力，准确捕捉KdV孤子的振荡和色散特性。

Result: 模型成功再现了KdV动力学的典型行为（单孤子传播、双孤子相互作用、余弦脉冲初始化），同时保持了守恒不变量，收敛更快且长期稳定性更好。

Conclusion: 结合不变量约束优化和正弦特征映射的计算高效方法，为哈密顿偏微分方程（如KdV方程）提供了鲁棒且能量一致的PINN。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [308] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离线学习紧密集成的强化学习框架，通过引导循环解决规划导致的数据分布偏移问题，在DeepMind Control Suite和Humanoid-Bench上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和最终性能，但规划用于环境交互会导致收集的数据与策略实际行为之间的分布偏移，这会降低模型学习和策略改进的效果。

Method: 提出BOOM框架，通过引导循环紧密集成规划和离线学习：策略初始化规划器，规划器通过行为对齐来引导策略。使用联合学习的世界模型支持规划器模拟未来轨迹，并提供价值目标促进策略改进。核心是使用规划器的非参数动作分布引导策略的无似然对齐损失，结合软价值加权机制优先处理高回报行为。

Result: 在高维DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面都达到了最先进的结果。

Conclusion: BOOM通过紧密集成规划和离线学习的引导循环，有效解决了规划导致的数据分布偏移问题，实现了优越的强化学习性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [309] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 本文提出了一种基于ROI引导掩码策略的自监督学习方法，用于静息态fMRI基础模型预训练，相比随机掩码方法在ADHD分类任务上提升了4.23%的准确率。


<details>
  <summary>Details</summary>
Motivation: 神经影像学中基础模型的出现受到大规模异构脑成像数据集的推动。现有重建目标的自监督学习方法在fMRI任务中展现出良好泛化能力，但需要超越依赖随机区域掩码的方法。

Method: 使用AAL3图谱进行ROI引导掩码策略，直接在完整4D fMRI体素上选择性掩码语义连贯的脑区，在自监督预训练中实现区域感知重建。

Result: 在包含973名受试者的ADHD-200数据集上，该方法相比传统随机掩码在ADHD分类准确率上提升4.23%。区域级归因分析显示边缘系统和脑小脑区域对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练期间掩码解剖区域不仅能增强可解释性，还能产生更稳健和具有区分性的表示。未来工作将扩展到更多神经影像数据集，并开发基于区域感知重建目标的新损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [310] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的异常检测方法，用于检测企业级ETL数据流中的多种异常类型，通过编码器-解码器结构和正则化约束提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中经常出现的延迟、缺失值、重复加载和突发异常变化等多种异常问题，确保数据处理稳定性。

Method: 采用深度自编码器结构，将高维输入压缩为潜在表示并重构，利用重构误差衡量异常程度；在潜在空间引入正则化约束增强特征稀疏性和分布学习。

Result: 在不同超参数设置、环境变化和数据特征下的系统分析表明，该方法在AUC、ACC、Precision和Recall等指标上表现优越。

Conclusion: 基于深度自编码器的检测机制能有效捕捉企业级ETL数据流的潜在分布模式，准确识别多种异常，为企业数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [311] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦学习中数据异构导致性能下降的原因，指出异构数据会产生不同的局部最优解，这既提高了全局目标的下界，又导致全局模型在训练后期出现振荡而非收敛。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法在理论上能保证收敛，实践中也往往能稳定训练，但在数据异构情况下性能下降的原因尚不明确。本文旨在填补这一理论空白，解释性能下降的根本原因。

Method: 引入异构客户端数据导致不同局部最优解的假设，分析这一假设带来的两个关键后果：1）客户端局部最优解之间的距离提高了全局目标的下界；2）训练后期全局模型在区域内振荡而非收敛到单一最优解。

Result: 理论分析表明数据异构会限制全局模型完全拟合所有客户端数据的能力，并通过多个任务和神经网络架构的实验验证了这一理论解释。

Conclusion: 本文提供了一个原则性的理论框架来解释非独立同分布设置下的性能下降问题，相关框架已在GitHub上开源。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [312] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器(VAE)的传感器校准新方法，通过将潜在空间训练为校准输出来实现传感器数据校准。


<details>
  <summary>Details</summary>
Motivation: 探索使用VAE进行传感器校准的新途径，利用其潜在空间特性来改进传统校准方法。

Method: 使用变分自编码器架构，将潜在空间作为校准输出进行训练，并在多传感器气体数据集上进行了概念验证。

Result: 提出的校准VAE能够同时作为校准模型和自编码器运行，并且在校准输出和重构输出方面都能产生与真实数据统计相似的输出。

Conclusion: 该方法展示了VAE在传感器校准中的潜力，为未来测试和扩展工作奠定了基础。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [313] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，学习模型推理能力和查询-方法兼容性，结合概率边界正则化，在平衡准确性和计算成本的同时选择最优推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，假设更多候选答案能带来更高准确性。本文重新审视这一假设，通过理论分析发现固定生成分布和候选数量下的准确性边界。

Method: 提出EPIC框架：1）通过对比学习构建共享表示空间，捕捉模型推理能力和查询-方法兼容性；2）将概率边界作为正则化项；3）进行效用驱动的优化，平衡准确性和计算成本。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确性的同时减少计算开销。

Conclusion: EPIC框架通过理论指导的表示学习和优化策略，有效解决了语言模型生成中推理方法选择的关键挑战，实现了准确性和效率的平衡。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [314] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度水平，包括线性回归、集成方法、深度学习模型以及大语言模型，以比较它们在空气质量预测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重威胁，包括呼吸道疾病、心血管疾病甚至癌症。预测PM2.5水平可以提供早期预警，帮助预防相关疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（如循环神经网络和变换器）以及大语言模型。

Result: 论文评估了不同模型在PM2.5预测任务上的性能表现，但具体结果未在摘要中提供。

Conclusion: 通过比较多种机器学习模型在PM2.5预测中的表现，可以为空气质量预警系统提供有效的技术支撑。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [315] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出了Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式建模中的过耦合和长视野问题，采用混合策略子轨迹平衡目标来学习局部优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成式建模的权重生成方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低下和精度不足。

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线策略和离线策略学习来捕捉局部优化策略。

Result: 理论证明仅学习局部优化策略即可解决长视野问题并提升全局最优权重生成，在迁移学习、少样本学习等领域验证了其优越的准确性和推理效率。

Conclusion: Lo-Hp框架通过局部优化策略学习有效解决了权重生成中的关键问题，在多种需要频繁权重更新的任务中表现出色。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [316] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出了一种基于单智能体强化学习的区域自适应交通信号控制框架，使用DreamerV3世界模型学习控制策略，通过集中决策避免多智能体系统的协调复杂性，在SUMO仿真中表现出良好的抗波动能力和队列长度减少效果。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵主要由交叉口排队引起，传统交通信号控制模型难以捕捉真实交通的复杂性，多智能体系统存在协调复杂性问题，需要一种更有效的自适应控制方法。

Method: 采用单智能体强化学习框架，使用邻接矩阵统一编码路网拓扑、实时队列状态和信号配时参数，利用DreamerV3世界模型学习控制策略，动作依次选择交叉口并调整信号相位配时。

Result: 在SUMO仿真实验中，面对10%、20%、30%的起讫点需求波动，该框架展现出强大的抗波动能力，显著减少了队列长度。

Conclusion: 这项工作为与探测车辆技术兼容的智能交通控制建立了新范式，未来研究将关注在训练中加入随机需求波动和探索区域应急优化机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [317] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用Temporal Fusion Transformer模型对沃尔玛周度销售额进行多周期预测，融合静态商店标识符和动态外部变量，在1-5周预测范围内表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多周期预测对库存管理和促销活动至关重要，需要处理复杂的时序模式和外部影响因素。

Method: 采用Temporal Fusion Transformer模型，融合静态商店标识符和时变外部信号（节假日、CPI、燃料价格、温度），通过分位数损失生成概率预测，并提供变量选择网络、静态丰富化和时序注意力等可解释性机制。

Result: 在2012年固定测试集上，RMSE为57.9k美元/商店-周，R²为0.9875；在5折时序交叉验证中，平均RMSE为64.6k美元，R²为0.9844，优于XGB、CNN、LSTM和CNN-LSTM基线模型。

Conclusion: 该模型在保持透明度的同时，为库存规划和节假日优化提供了实用价值，展示了TFT在零售预测中的有效性。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [318] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级黑盒红队测试方法，用于评估AI系统中激活探针的鲁棒性，发现可解释的脆弱性模式，并为未来探针加固提供可行见解。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控工具具有低成本、低延迟的优势，但其在现实世界中的鲁棒性尚未充分探索。研究旨在了解在现实黑盒对抗压力下出现的故障模式，并以最小努力发现这些模式。

Method: 采用轻量级黑盒红队测试流程，将现成的LLM与迭代反馈和上下文学习结合，无需微调、梯度或架构访问。通过高风险交互探针的案例研究验证方法。

Result: 分析揭示了可解释的脆弱性模式（如法律术语导致的误报；平淡程序化语调导致的漏报），以及在场景约束攻击下减少但仍持续存在的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测故障模式，并为未来探针加固提供有前景的可行见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [319] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出FTT-GRU混合模型，结合快速时序Transformer和GRU，在NASA CMAPSS数据集上实现剩余使用寿命预测，相比现有最佳方法提升RMSE 1.16%、MAE 4.00%。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LSTM和CNN难以同时建模多元传感器数据的全局时间依赖和细粒度退化趋势，需要更有效的模型架构。

Method: 使用快速时序Transformer（FTT）通过FFT实现线性化注意力，结合GRU层进行序列建模，构建紧凑的Transformer-RNN混合架构。

Result: 在CMAPSS FD001上获得RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟1.12ms，相比TCN-Attention基准有显著提升。

Conclusion: 紧凑的Transformer-RNN混合模型能够提供准确高效的RUL预测，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [320] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出了一个以LLM为核心的贝叶斯网络结构发现统一框架，支持无数据和有数据两种场景，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需要大量观测数据且计算成本高，现有LLM方法仅将其作为辅助工具，未充分发挥LLM在核心学习过程中的作用

Method: 提出PromptBN用于无数据场景，通过元数据查询LLM发现概率关系；提出ReActBN用于有数据场景，将ReAct推理范式与BIC等结构评分结合进行迭代优化

Result: 实验表明该方法在低数据或无数据场景下显著优于现有LLM方法和传统数据驱动算法

Conclusion: 将LLM置于贝叶斯网络结构发现的核心位置，在整个发现过程中保持LLM的主动参与，是有效的解决方案

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [321] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计和非参数技术的新框架，用于从数据中发现动态系统模型方程，特别适用于复杂生物系统。


<details>
  <summary>Details</summary>
Motivation: 传统参数模型在准确表示复杂系统中的某些非线性特性方面存在不足，特别是在系统生物学中，自下而上的建模方法往往不可行。

Method: 将稀疏参数估计（如Sindy算法）与非参数技术相结合，无需预先知道非线性函数形式或扩展函数库即可捕获Sindy无法描述的非线性特性。

Result: 该方法在多个复杂生物现象估计示例中得到验证，能够有效捕获系统动态特性。

Conclusion: 该框架为复杂动态系统的数据驱动建模提供了更强大的工具，特别适用于系统生物学中非线性系统的建模需求。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [322] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 本研究提出了一个临床医生中心的框架来量化大语言模型在脊柱手术中的幻觉风险，评估了六个领先LLM在30个专家验证的脊柱病例上的表现。DeepSeek-R1表现最佳，但发现推理增强模型变体并不总是优于标准版本，多维压力测试暴露了模型特定的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在脊柱手术临床决策支持中具有变革潜力，但存在幻觉风险，可能危及患者安全，需要量化评估这些风险。

Method: 采用临床医生中心框架，评估诊断精度、推荐质量、推理稳健性、输出一致性和知识对齐五个维度，在30个专家验证的脊柱病例上测试六个领先LLM。

Result: DeepSeek-R1总体表现最佳（总分：86.03±2.08），在创伤和感染等高风险领域表现突出。推理增强模型变体并不总是优于标准版本，多维压力测试显示推荐质量在复杂度增加时下降7.4%。

Conclusion: 需要将可解释性机制整合到临床工作流程中，并为手术LLM部署建立安全感知的验证框架，扩展思维推理本身不足以确保临床可靠性。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [323] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一个统一的数据驱动框架，用于量化和增强职业冰球的进攻势头和得分可能性。通过分析NHL事件记录，结合多种机器学习方法，发现结构化序列和紧凑阵型能显著提升进攻表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个端到端的分析框架，为教练和分析师提供实时、可操作的战术优化见解，推动冰球分析向基于因果关系的战术优化发展。

Method: 五阶段管道：1）通过逻辑回归进行可解释的势头加权；2）使用梯度提升决策树进行非线性xG估计；3）LSTM网络进行时间序列建模；4）PCA和K-Means聚类发现空间阵型；5）X-Learner因果推断估计器量化最优序列和阵型的平均处理效应。

Result: 观察到ATE为0.12（95% CI：0.05-0.17，p < 1e-50），对应得分潜力相对提升15%。结果表明结构化序列和紧凑阵型能因果性地提升进攻表现。

Conclusion: 该框架为教练和分析师提供实时、可操作的见解，将冰球分析推向基于原则和因果基础的战术优化。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [324] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 提出一个统一的贝叶斯框架来解释LLM的控制方法，将上下文学习和激活引导视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 解释看似不同的LLM控制方法（上下文学习和激活引导）是否可以被视为更广泛框架的具体实例，并开发一个统一的预测性理论。

Method: 从贝叶斯角度构建统一模型，认为上下文和激活干预通过改变模型对潜在概念的信念来影响行为：激活引导改变概念先验，上下文学习导致证据积累。

Result: 开发出的贝叶斯模型能够高度预测LLM在多种干预下的行为，解释了先前的经验现象（如S型学习曲线），并预测了新现象（如对数信念空间中的干预可加性）。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一的理论解释，以及预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [325] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 本文研究了具有稀疏成本的对抗性随机最短路径问题，提出了ℓ_r-范数正则化器来适应稀疏性，在已知转移设置下实现了√log M的遗憾界，优于传统的√log SA界。


<details>
  <summary>Details</summary>
Motivation: 现有基于负熵正则化的在线镜像下降方法在已知转移设置下的遗憾界为√log SA，这在最坏情况下是最优的，但无法充分利用稀疏性带来的好处。当只有少量状态-动作对产生成本时，负熵正则化本质上无法适应稀疏性。

Method: 提出了一族ℓ_r-范数正则化器（r∈(1,2)），这些正则化器能够适应问题的稀疏性。在已知转移设置下，该方法实现了√log M的遗憾界，其中M是产生成本的状态-动作对数量。

Result: 在已知转移设置下，ℓ_r-范数正则化器实现了√log M的遗憾界，这比传统的√log SA界更好，并且通过匹配下界证明了该结果的最优性。在未知转移设置下，稀疏性的好处有限，任何学习者的极小极大遗憾都与SA呈多项式关系。

Conclusion: ℓ_r-范数正则化器能够有效适应稀疏成本问题，在已知转移设置下实现了最优的√log M遗憾界，证明了M而非SA捕捉了问题的有效维度。但在未知转移设置下，稀疏性的好处受到限制。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [326] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究表明RBMs在训练前剪枝80%仍能保持良好生成性能，但训练后剪枝会严重损害性能且无法通过再训练完全恢复。


<details>
  <summary>Details</summary>
Motivation: 探索在神经网络规模不断增大的背景下，是否可以通过剪枝创建更稀疏、高效的网络结构，同时保持强性能。

Method: 基于彩票假设研究受限玻尔兹曼机(RBMs)，在极端剪枝条件下测试网络性能，比较训练前剪枝和训练后剪枝的效果。

Result: RBMs在训练前剪枝80%时仍能保持高质量生成性能，但训练后剪枝会导致性能急剧下降，且再训练无法完全恢复性能。

Conclusion: 剪枝应在训练早期实施，而非训练后进行；初始条件对网络能力有持久影响，这对开发高效神经网络架构具有重要指导意义。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [327] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一个异步联邦学习框架，通过无数据知识蒸馏来恢复过时更新，在保持AFL可扩展性的同时提高训练效率和最终精度。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然通过独立通信提高了大规模异构环境中的效率，但引入了过时更新问题，这会破坏优化稳定性并阻碍收敛。

Method: FedRevive结合参数空间聚合与轻量级服务器端无数据知识蒸馏(DFKD)，通过元学习生成器合成伪样本实现多教师蒸馏，并使用混合聚合方案结合原始更新和DFKD更新。

Result: 在各种视觉和文本基准测试中，FedRevive相比异步基线实现了高达32.1%的更快训练速度和高达21.5%的更高最终精度。

Conclusion: FedRevive通过无数据知识蒸馏有效缓解了异步联邦学习中的过时更新问题，在保持可扩展性的同时显著提升了训练效率和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [328] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 应用伴随状态方法计算生成流模型（特别是扩散模型）的梯度，用于气候敏感性分析，将计算成本从超级计算机数周降低到GPU数小时。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型进行敏感性分析计算成本过高，而AI生成模型虽然评估速度快，但计算敏感性仍是瓶颈。

Method: 使用伴随状态方法计算生成流模型的梯度，应用于cBottle生成模型（ERA5数据模拟器），并提出梯度自一致性检验来验证敏感性结果。

Result: 该方法能够产生可靠的梯度，将敏感性分析的计算时间从数周大幅减少到数小时。

Conclusion: 该方法简化了气候科学中的关键工作流程，为使用生成模型进行高效敏感性分析提供了可行方案。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [329] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA是一种推理时方法，通过KL散度、置信度和熵的组合评分函数来指导渐进式剪枝，在保持准确性的同时显著减少内存和令牌使用。


<details>
  <summary>Details</summary>
Motivation: 标准方法如Best-of-N计算成本高，而Self-Truncation Best-of-N依赖一致性启发式方法，无法直接评估分支质量。

Method: 结合KL散度、置信度和熵构建评分函数，在探索过程中促进多样性并选择性消除低分分支。

Result: 在GSM8K和MATH500上的实验显示，KAPPA在小模型中稳定性能，相比BoN减少约60%峰值内存和90%令牌生成，对准确性影响极小。

Conclusion: KAPPA提供了一种原则性的推理时剪枝方法，在保持准确性的同时大幅降低计算成本。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [330] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 提出Pub2Priv框架，利用公共知识生成隐私保护的时间序列数据，通过自注意力机制和扩散模型改善隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 敏感时间序列数据共享受限，现有方法忽视公共上下文元数据，导致隐私-效用权衡不理想。

Method: 使用自注意力机制编码公共数据为时空嵌入，作为扩散模型的条件输入生成合成私有序列。

Result: 在金融、能源和大宗商品交易领域，Pub2Priv在隐私-效用权衡方面持续优于现有基准方法。

Conclusion: Pub2Priv通过利用异构公共知识有效改善了隐私时间序列数据的生成质量。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [331] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 该论文研究了知识追踪模型在在线学习平台中面临的概念漂移问题，发现所有模型都会出现性能下降，其中贝叶斯知识追踪模型最为稳定，而复杂的注意力模型性能下降最快。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习过程是静态的，但在线学习平台中学生行为和概念会随时间变化，需要研究概念漂移对模型性能的影响。

Method: 使用四种经典知识追踪模型在五个学年数据上进行测试，评估模型在单学年内和跨学年的性能变化。

Result: 所有知识追踪模型都会出现性能下降，贝叶斯知识追踪模型在应对新数据时最稳定，而复杂的注意力模型预测能力下降最快。

Conclusion: 知识追踪模型容易受到概念漂移影响，需要进行纵向评估，贝叶斯知识追踪模型在概念漂移环境下表现最佳。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [332] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一的描述-预测-生成架构，在单一编码器-解码器框架中整合了统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建能够同时支持描述性表示学习、预测性推理和生成性合成的通用智能架构，连接可解释性、准确性和创造性。

Method: 使用变分目标在单一编码器-解码器框架中联合优化描述性重构、预测性分类和生成性采样。

Result: 在MNIST数据集上的实验验证了描述性重构、预测性分类和生成性采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [333] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 该论文提出了一种融合卫星和雷达数据的多模态临近预报模型，用于预测5、15和30分钟内的降水，特别针对强降雨事件。实验表明该模型在预测精度上显著优于仅使用雷达的方法。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器对城市强降雨事件的监测能力有限（德国2001-2018年间仅有17.3%的强降雨事件被雨量计记录），雷达数据虽然能有效跟踪正在发生的降水，但单独用于强降雨预报仍面临挑战。

Method: 开发了一个多模态临近预报模型，结合雷达和卫星图像数据来预测降水。

Result: 多模态策略显著优于仅使用雷达的方法，卫星数据的集成提高了预测精度，特别是对强降水。在5分钟提前期，强降雨的关键成功指数提高了4%，暴雨提高了3%。在更长的提前期仍保持较高的预测能力。

Conclusion: 多模态模型能提供更详细准确的强降雨预报，实现及时可靠的预警，具有拯救生命的潜力。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [334] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion是一个创新的多变量时间序列生成框架，结合扩散概率模型和可学习序列分解技术，通过分别建模趋势和季节性成分来提升生成过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法缺乏可解释的分解技术，难以合成有意义的趋势和季节性模式，限制了生成数据的质量。

Method: 使用MLP结构捕捉趋势成分，自适应小波蒸馏实现季节性成分的多分辨率学习，并设计了全面的校正机制确保生成成分的内部一致性和相互关系。

Result: 在8个真实世界数据集上的实验表明，STDiffusion在时间序列生成任务中达到了最先进的性能，并在多窗口长序列生成中表现出鲁棒性和多功能性。

Conclusion: STDiffusion通过可解释的分解方法有效提升了时间序列生成的质量和可解释性，为多变量时间序列生成提供了新的解决方案。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [335] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 提出了一种基于双凸优化的PINN方法PINN-ACS，用于求解特征值问题，通过交替凸搜索实现了比传统梯度训练快500倍的收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法求解特征值问题时速度比经典数值方法慢几个数量级，需要更高效的训练方法

Method: 将特征对搜索重新表述为双凸优化问题，使用解析最优更新的交替凸搜索(ACS)来求解特征值和特征函数

Result: PINN-ACS实现了高精度，收敛速度比基于梯度的PINN训练快达500倍

Conclusion: PINN-ACS为特征值问题提供了一种快速、可证明收敛的网格无关求解方法

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [336] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 提出PREPO方法，通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：基于提示困惑度的渐进学习和基于相对熵差异的探索优先机制。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的推理能力，但训练成本高昂，因为许多rollout对优化的贡献有限。本研究旨在利用几乎免费的内在数据特性来提高RLVR的数据效率。

Method: PREPO包含两个组件：1) 使用提示困惑度作为模型适应性的指标，实现从易到难的渐进学习；2) 通过区分rollout的相对熵来放大差异，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中取得了有效结果，相比基线方法减少了多达3倍的rollout需求，同时保持了竞争性能。

Conclusion: PREPO通过内在数据特性显著提高了RLVR的数据效率，不仅获得了实证收益，还提供了理论和深入分析来解释方法的底层原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [337] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文分析了预训练Transformer在微调时出现输出饱和导致梯度抑制的问题，提出了诊断指标识别拐点层，并通过选择性注入LoRA适配器来恢复被抑制的梯度信号。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时往往过度依赖源域模式，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 提出层间诊断指标（注意力熵、激活梯度范数、参数梯度范数、Delta-CKA）识别拐点层，并在这些层选择性注入LoRA适配器以恢复梯度信号。

Result: 实验表明，在过训练初始化下，拐点层LoRA注入能提升性能；而在欠训练初始化下，则需要全路径解阻塞来实现低层重构。

Conclusion: 诊断优先、轻量注入的微调策略能有效解决梯度抑制问题，具体策略应根据基特征强度选择：强基特征时解阻塞拐点层，弱基特征时需要全路径解阻塞。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [338] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪轨迹空间来引导生成远离目标概念，同时保持模型先验知识，无需精心设计的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前的概念擦除技术要么导致图像质量下降，要么依赖脆弱的对抗性损失，或者需要大量重新训练。这些限制源于对扩散模型去噪轨迹的短视理解。

Method: 将概念遗忘视为去噪路径空间的探索问题，使用配备轨迹平衡目标的GFlowNets进行优化，通过采样整个轨迹而非单一最终状态来学习随机策略。

Result: EraseFlow在性能上优于现有基线，实现了性能和先验保持之间的最佳权衡，能有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow提供了一个有效的概念擦除框架，通过轨迹级优化解决了现有方法的局限性，在保持生成质量的同时成功擦除目标概念。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [339] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出了逻辑信息强化学习（LIRL），通过将低维潜在动作投影到由一阶逻辑定义的可行混合流形上，保证每个探索步骤的可行性，无需调整惩罚参数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保证约束满足方面存在不足：分层方法牺牲全局最优性，而强化学习方法依赖脆弱的奖励惩罚、屏蔽或保护机制。

Method: 为标准策略梯度算法配备投影机制，将低维潜在动作映射到由一阶逻辑动态定义的可行混合流形上。

Result: 在工业制造、电动汽车充电站和交通信号控制等多个场景中，LIRL均优于现有分层优化方法。以机器人减速器装配系统为例，相比传统工业分层调度方法，LIRL在总完工时间-能耗目标上最多减少36.47%至44.33%。

Conclusion: 该框架基于声明式逻辑约束公式，可无缝迁移到智能交通和智能电网等其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [340] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化(EPG)框架，用于在对抗性游戏中学习具有跨图零样本性能的通用策略，特别适用于追逃游戏(PEG)。


<details>
  <summary>Details</summary>
Motivation: 现有的追逃游戏求解方法需要指数时间，当图结构变化时，即使最先进的强化学习方法也需要重新计算或微调，这很耗时且影响实时应用。

Method: EPG框架在不同图结构上训练强化学习策略，针对每个单图的均衡策略。使用动态规划算法生成纯策略纳什均衡，并设计了分组机制和序列模型来处理多个追捕者的情况。

Result: 实验结果显示，EPG框架在各种未见过的真实世界图中保证了理想的零样本性能。在具有出口的图中，通用追捕者策略甚至能与最先进PEG方法的微调策略性能相匹配。

Conclusion: EPG框架成功实现了追逃游戏中的跨图策略泛化，为追捕者和逃避者双方在无出口和多出口场景中提供了首个具有这种泛化能力的解决方案。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [341] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种面向边缘设备的优化视觉Transformer设计，通过集成LUT神经元层来减少模型大小和计算需求，在保持准确性的同时提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer在边缘设备上存在计算、内存和能耗过高的问题，而现有的LUT网络在视觉任务上表现不佳，需要一种既能减少资源消耗又能保持性能的解决方案。

Method: 设计基于LUT的通道混合器替代传统MLP层，采用神经网络学习方法来学习LUT函数，并开发了相应的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，消除了60%的模型权重和50%的乘法运算，能效提升1.9倍，延迟降低1.3倍。

Conclusion: LL-ViT为边缘设备提供了一种高效、低功耗的视觉Transformer解决方案，在保持性能的同时显著减少了资源消耗。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [342] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个交互式应用程序，用于油气管道中段塞流的端到端数据驱动检测，通过紧凑的用户友好界面实现数据探索、模型训练、结果可视化和实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法通常离线、需要专业知识且缺乏实时可解释性，难以满足油气管道运营安全和效率的需求。

Method: 集成数据探索和标注、可配置的多分类器模型训练与评估、时间序列叠加的分类结果可视化，以及生成基于持久性警报的实时推理模块。

Result: 系统支持从标注CSV上传到未知数据集实时推理的无缝工作流，具有轻量、便携和易部署的特点，结合了领域相关分析和创新的UI/UX功能。

Conclusion: 该工具展示了交互式人机协同ML系统如何在关键过程工业中弥合数据科学方法与实际决策之间的差距，对时间序列故障诊断任务具有更广泛的适用性。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [343] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一个分层KV缓存管理系统，利用KV头的时序稳定性来减少GPU内存使用和计算开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务受到KV缓存增长的约束，现有系统难以在不降低精度的情况下有效利用注意力机制中关键令牌的稀疏性。

Method: 将KV头分类为稳定和不稳定：不稳定头的所有KV缓存页保留在GPU内存中，稳定头仅保留前K页在GPU上，其余卸载到主机内存，并利用时序稳定性进行周期性重排序。

Result: 在vLLM上实现，长上下文请求的GPU内存占用减少高达70%，离线服务吞吐量提升1.38-1.55倍，在线令牌延迟降低1.6-2.1倍。

Conclusion: FlexiCache通过利用KV头的时序稳定性，在保持长上下文、长生成场景精度的同时，显著提升了内存效率和性能。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [344] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究表明，增加批处理大小可以补偿量化训练中反向传播精度降低的影响，同时分析了权重和激活值量化对梯度方差的不同影响。


<details>
  <summary>Details</summary>
Motivation: 量化训练虽然能提高计算和内存效率，但会引入量化噪声，影响模型收敛和精度。随机舍入(SR)作为理论上有吸引力的替代方案，其与其他训练因素（特别是批处理大小）的交互作用尚未充分探索。

Method: 通过理论和实证研究，分析小批量随机梯度下降(SGD)与随机舍入(SR)的结合，研究批处理大小如何补偿反向传播精度降低，并区分权重和激活值量化对梯度方差的不同影响。

Result: 实验验证了理论见解，表明增加批处理大小确实可以缓解量化训练中精度降低的问题。

Conclusion: 批处理大小是量化训练中的重要调节因素，能够有效补偿精度损失，且权重和激活值量化对梯度方差的影响机制不同。

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [345] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种结合Kronecker分解近似曲率二阶策略优化和安全感知梯度操作的安全强化学习算法，通过自适应梯度混合和信任区域控制，在保持安全约束的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化与约束满足之间的权衡问题，避免传统方法中固定阈值导致的性能不稳定和安全性不足。

Method: 使用K-FAC近似Fisher信息矩阵进行高效二阶策略优化，引入边界感知梯度操作机制自适应调整奖励和成本梯度影响，采用小批量KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium环境测试中，相比最佳基线方法，KFCPO实现了10.3%到50.2%的平均回报提升，同时满足安全约束。

Conclusion: KFCPO通过二阶优化和自适应梯度操作，在安全强化学习中实现了性能与安全性的更好平衡，证明了该方法的有效性。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [346] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够将解释树拟合到任何给定的非可解释聚类或数据集本身，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释聚类方法主要关注最小化特定聚类目标的可解释性代价，缺乏通用的方法来将解释树拟合到任意给定的聚类，而不受限制。

Method: 基于谱图划分设计可解释聚类算法，通过Trevisan（2013）的广义框架将先前算法解释为同时在两个图上优化割的图划分方法。

Result: 实验表明，该方法在一系列数据集上相比基线方法表现出更优的性能。

Conclusion: 提出的基于谱图划分的通用可解释聚类方法能够有效拟合解释树，为可解释聚类提供了新的解决方案。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [347] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 该论文提出了一个基于范畴对称性感知的学习框架，通过将时间、尺度和传感器层次结构的变化因素融入特征表示结构，使模型在现实扭曲下保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决人类活动识别中传感器信号随上下文、运动和环境变化而漂移的问题，需要模型在世界变化时保持稳定。

Method: 引入范畴对称性感知学习框架，将信号在时间、尺度和传感器层次结构上的变化因素构建到特征表示结构中，使用范畴等变表示理论。

Result: 在UCI人类活动识别基准测试中，该设计将分布外准确率提高了约46个百分点（约3.6倍于基线）。

Conclusion: 抽象对称性原则可以通过范畴等变表示理论转化为日常感知任务中的具体性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [348] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 该论文通过布尔函数分析研究LIF脉冲神经网络的稳定性和鲁棒性，发现宽LIF-SNN分类器具有平均稳定性，其傅里叶谱集中在低频分量，并引入谱简单性概念解释深度网络中的简单性偏差。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在能效计算方面前景广阔，但其理论基础（特别是稳定性和鲁棒性）相比人工神经网络仍有限，需要深入研究。

Method: 使用布尔函数分析框架研究离散时间泄漏积分发放LIF-SNN，重点关注噪声敏感性和分类任务中的稳定性，量化输入扰动对输出的影响。

Result: 主要结果表明宽LIF-SNN分类器具有平均稳定性，这种特性由其傅里叶谱集中在低频分量解释。随机LIF-SNN偏向简单函数，实验验证这些稳定性特性在实践中持续存在。

Conclusion: 研究结果为SNN的稳定性和鲁棒性提供了新的理论见解，通过谱简单性框架连接了分析结果与深度网络中观察到的简单性偏差。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [349] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一个基于能量的统一框架来理解Transformer注意力机制，将标准softmax注意力视为Helmholtz自由能最小化的特例，并基于此框架提出了新的注意力结构变体。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在各种任务中表现出强大的适应性，但其底层机制仍需进一步探索。能量视角长期以来为理解神经计算提供了有价值的原则，本文旨在通过能量视角重新理解基于注意力的Transformer模型。

Method: 提出了一个统一的能量框架，包含三个关键组件：全局能量F*、能量函数Ei和梯度下降形式。将标准softmax注意力视为Helmholtz自由能最小化的特例，并将线性注意力自然地纳入该框架。基于经典梯度下降算法，将原始注意力公式扩展到动量GD、NAG和牛顿法变体。

Result: 实验为基于能量框架设计注意力机制的潜力提供了初步支持。

Conclusion: 能量框架为理解Transformer注意力机制提供了统一视角，并启发了新的注意力结构设计，展示了该框架在注意力机制设计方面的潜力。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [350] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet是一种结合PPG和加速度计信号的多模态神经网络，通过时间与通道注意力模块改进房颤检测，在嘈杂的临床数据中实现三类心律分类（房颤、窦性心律、其他）。


<details>
  <summary>Details</summary>
Motivation: 手腕PPG监测易受运动伪影和生理噪声影响，现有方法多依赖单通道PPG且仅限于二元房颤检测，无法捕捉临床中更广泛的心律失常类型。

Method: 提出RhythmiNet残差神经网络，集成时间与通道注意力模块，联合利用PPG和加速度计信号进行三类心律分类，测试数据按运动强度分层评估鲁棒性。

Result: RhythmiNet相比仅用PPG的基线模型在宏观AUC上提升4.3%，比基于手工HRV特征逻辑回归模型性能提升12%，证明多模态融合和注意力学习的优势。

Conclusion: 多模态信号融合和注意力机制能显著提高在嘈杂真实临床数据中的心律分类性能，为可穿戴设备的心律监测提供了更可靠的解决方案。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [351] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，从容量控制的角度解释归一化方法在深度神经网络中的作用，证明归一化能指数级降低Lipschitz常数，从而平滑损失景观并约束网络容量。


<details>
  <summary>Details</summary>
Motivation: 归一化方法虽然在实践中能稳定优化过程和提升泛化能力，但其理论机制尚未得到充分解释，特别是在深度网络中使用多个归一化层时。

Method: 开发了一个理论框架，通过分析未归一化和归一化DNN的Lipschitz常数来研究归一化的作用机制。

Result: 证明未归一化DNN可能具有指数级大的Lipschitz常数，而插入归一化层能以指数速率降低Lipschitz常数，从而平滑损失景观并约束网络容量。

Conclusion: 归一化通过指数级降低Lipschitz常数，为深度学习中归一化方法的经验成功提供了原则性解释，既促进优化稳定性又增强泛化保证。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [352] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 提出一种使用优化合成数据来估计模型测试误差的方法，在有限标注数据条件下实现准确可靠的模型评估。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估方法需要大量标注测试集，但在很多场景中获取大规模标注数据成本高昂且费时，因此需要在小样本标注条件下进行可靠的模型评估。

Method: 开发了考虑合成数据的泛化边界理论，基于此提出理论驱动的优化合成数据生成方法，用于模型测试误差估计。

Result: 在仿真和表格数据集上的实验结果表明，相比现有基线方法，该方法能够获得更准确可靠的测试误差估计。

Conclusion: 利用优化生成的合成数据可以有效解决有限标注数据条件下的模型评估问题，生成器质量对评估效果有重要影响。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [353] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一个基于流的生成模型，用于推断连续空间切片中细胞微环境的时序轨迹，通过最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。


<details>
  <summary>Details</summary>
Motivation: 理解细胞微环境的时空演化对于解析组织发育和疾病进展至关重要，现有方法在单细胞水平建模细胞演化，忽略了组织中细胞状态的协调发育。

Method: 将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。

Result: 该方法在从胚胎到大脑发育的多种时空数据集中成功恢复了全局空间结构和局部微环境组成。

Conclusion: NicheFlow能够有效推断细胞微环境的时空演化轨迹，为理解组织发育提供新视角。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [354] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种解决多模态学习中模态不平衡问题的新框架，通过互信息量化模态间交互，采用跨模态知识蒸馏和多任务式训练来平衡不同模态的贡献


<details>
  <summary>Details</summary>
Motivation: 多模态学习中的模态不平衡问题在生物数据分析中尤为突出，传统方法难以同时利用模态间协同效应并有效解决模态冲突

Method: 使用互信息量化模态交互，采用两阶段策略：跨模态知识蒸馏预训练阶段用强模态增强弱模态，主训练阶段采用多任务式学习机制动态校准梯度贡献

Result: 该方法有效缓解了模态不平衡问题，显著提升了多模态模型的整体性能

Conclusion: 提出的统一框架通过平衡的多模态学习策略成功解决了模态不平衡问题，为生物数据分析等领域的多模态学习提供了有效解决方案

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [355] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra是一个双头元上下文记忆模块，通过2维递归在时间和变量维度上学习记忆模式，解决了现有时间序列模型缺乏时间归纳偏置、无法捕捉变量间依赖关系以及长序列建模效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer、MLP和线性模型在多元时间序列建模中存在三个主要问题：(1)置换等变性导致缺乏时间归纳偏置；(2)自然设计为单变量设置，无法捕捉时间和变量维度的相互依赖；(3)长序列建模效率低。线性RNN虽然解决了部分问题，但仍受限于单序列且会传播误差。

Method: Hydra采用双头元上下文记忆模块，通过2维递归在时间和变量维度上学习如何记忆更信息丰富的时间序列模式。虽然2维特性使训练递归且不可并行，但提出了2D分块训练算法，在保持有效性的同时将效率提升10倍。

Result: 在时间序列预测、分类和异常检测等多个任务和数据集上的实验结果表明，Hydra相比最先进的基线方法具有优越性能。

Conclusion: Hydra通过创新的2维递归设计和高效的训练算法，成功解决了多元时间序列建模中的关键挑战，在多个任务上展现出卓越性能。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [356] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 提出ProfBO算法，通过MDP先验从相关任务中学习优化轨迹知识，显著减少贝叶斯优化所需的函数评估次数，在真实世界应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在现实应用中（如药物发现、材料设计）因评估成本高昂而难以实用，需要开发能在极少量评估中获得高质量解的方法。

Method: 使用MDP先验建模相关源任务的优化轨迹，将先验嵌入神经网络，采用模型无关元学习快速适应新任务。

Result: 在Covid、Cancer基准测试和超参数调优任务中，ProfBO始终优于最先进方法，用显著更少的评估获得高质量解。

Conclusion: ProfBO算法已准备好实际部署，能够有效解决高成本评估场景下的黑盒优化问题。

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [357] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出SymRegg算法，利用等式图(e-graph)结构避免符号回归中冗余计算，提高搜索效率同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中计算大量冗余表达式(可达总评估数的60%)，需要一种方法来避免这些不必要的计算

Method: 基于e-graph结构的新搜索算法：从e-graph中采样表达式进行扰动，若产生未访问表达式则插入e-graph并生成等价形式

Result: SymRegg能提高搜索效率，在不同数据集上保持一致的准确结果，且只需极简的超参数选择

Conclusion: e-graph结构能有效避免符号回归中的冗余计算，SymRegg算法在效率和准确性方面表现优异

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [358] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文主张在AI智能体设计中应将数据处理能力作为首要任务，提出了实现数据感知智能体的四个关键能力：主动数据获取、复杂数据处理、交互式测试数据合成和持续适应。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体研究主要关注推理能力，但在现实世界部署中，智能体如何持续获取、处理和演进数据的能力被忽视，这限制了智能体的可扩展自主性。

Method: 提出四个核心能力框架：1) 主动数据获取 - 自主收集关键知识或寻求人类输入填补数据空白；2) 复杂数据处理 - 上下文感知和灵活处理多样化数据挑战；3) 交互式测试数据合成 - 从静态基准转向动态生成的交互测试数据；4) 持续适应 - 迭代优化数据和背景知识以适应环境变化。

Result: 构建了一个系统性的数据感知智能体能力框架，为数据中心的AI研究提供了新的方向。

Conclusion: 数据感知能力应成为智能体系统设计的核心优先事项，这是实现可靠现实世界部署的关键，也是数据中心AI的下一个前沿领域。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [359] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期停电预测系统，用于极端天气事件期间预测，通过两阶段特征工程和稳健优化策略，相比基线方法RMSE提升8.4%。


<details>
  <summary>Details</summary>
Motivation: 解决极端天气事件期间短期电力中断的准确预测问题，以支持应急响应和电网恢复工作。

Method: 采用两阶段特征工程（数据清洗+相关性过滤），结合时间嵌入、多尺度滞后特征和天气变量作为外生输入，使用SARIMAX模型并实施分层拟合策略和自动降级机制。

Result: 在密歇根县数据集上，模型达到RMSE 177.2，相比基线方法（RMSE 193.4）提升8.4%。

Conclusion: 所提出的特征工程和稳健优化策略有效提高了极端天气相关停电预测的准确性。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [360] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 评估GAN模型在MIMIC-III数据集上生成合成医疗数据的公平性，发现受保护人口属性存在显著不平衡，提出MedEqualizer框架来改善代表性不足子组的平衡性。


<details>
  <summary>Details</summary>
Motivation: 合成医疗数据可以增强数据可访问性，但需要确保跨受保护属性的公平性，避免在临床研究和决策中出现偏见或误导性结果。

Method: 使用对数差异度量评估子组代表性，提出MedEqualizer模型无关的增强框架，在合成数据生成前丰富代表性不足的子组。

Result: 观察到合成数据中许多子组相对于真实数据存在代表性不足或过度代表的问题，MedEqualizer显著改善了合成数据集中的人口统计学平衡。

Conclusion: MedEqualizer为更公平和具有代表性的医疗数据合成提供了可行路径，能够显著改善合成数据的公平性。

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [361] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 使用COLET数据集，通过基于窗口的时域特征提取和机器学习/深度学习技术对认知负荷进行分类，深度学习模型特别是表格架构在各项指标上优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在健康、心理学和国防应用等领域日益受到关注，需要有效的实时评估方法。

Method: 采用基于窗口的时域分区方法增强特征，然后使用机器学习和深度学习模型进行分类。

Result: 深度学习模型，特别是表格架构，在精确度、F1分数、准确率和分类精度方面都优于传统机器学习方法。

Conclusion: 基于窗口的时域特征提取和深度学习技术对于复杂动态任务中的实时认知负荷评估具有显著效果和潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [362] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: Mono-Forward算法无需反向传播，在MLP架构上超越BP基准，实现更高分类准确率、41%能耗降低和34%训练加速


<details>
  <summary>Details</summary>
Motivation: 挑战反向传播是达到最先进性能必需的传统假设，探索更高效、可持续的替代方案

Method: 从Hinton的Forward-Forward算法演进到Cascaded Forward，最终提出Mono-Forward算法，在相同架构和超参数优化框架下进行公平比较

Result: Mono-Forward在分类准确率上持续超越优化调参的BP基准，能耗降低41%，训练速度提升34%

Conclusion: Mono-Forward成为MLP中实用、高性能且可持续的BP替代方案，同时重新评估了无BP方法的内存效率问题

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [363] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福感的新型公平框架，通过线性规划计算最优公平后处理策略，统一并扩展了多种已知公平定义。


<details>
  <summary>Details</summary>
Motivation: 现有公平性度量缺乏直观的人类中心视角，需要一种既直观又数学严谨的公平框架。

Method: 基于群体从决策结果中获得效用的幸福感概念，通过求解线性规划问题来计算最优公平后处理策略。

Result: 方法高效且可扩展，能够统一和扩展多种已知公平定义，在多样化场景中表现出实际优势。

Conclusion: 幸福感框架为公平性提供了更直观的人类中心视角，同时保持数学严谨性，具有良好的实用性和扩展性。

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [364] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 本文挑战AI研究中的"规模原教旨主义"，提出基于梯度影响模式的资源分配框架，通过只更新高影响力参数和使用梯度范数作为代理指标，可大幅提升AI生命周期效率，减少资源需求数量级。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中无限制的模型规模和计算增长导致了不可持续的环境影响和资源不平等扩大，需要从根本上重新定位LLM开发方向，关注能力-资源比而非单纯能力。

Method: 提出理论框架：基于梯度影响模式指导资源分配决策；在Transformer模型中识别遵循重尾分布的高影响力参数；使用简单梯度范数作为计算高效代理指标；协调参数和数据选择；提出两阶段范式：边际收益预训练和影响力引导适配。

Result: 分析表明：仅更新高影响力参数在性能-资源基础上严格优于全参数调优；梯度范数可有效识别高影响力组件；协调参数和数据选择可实现数量级资源需求减少。

Conclusion: 能力-资源视角将硬件变通方案转变为理论最优策略，通过将资源意识嵌入模型开发、适配和评估过程，可重塑AI进步方向，实现更可持续和公平的未来。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [365] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS是一个双代理架构，通过将推理（教师）与执行（学生）解耦，实现无需梯度更新的持续学习，在推理时通过系统级编排动态调整操作策略。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重新训练，不适合需要实时适应的部署代理。需要一种无需参数更新的持续学习方案。

Method: 采用双代理架构，教师负责推理，学生负责执行，包含持久学习记忆存储经验指导，通过编排层在推理时动态调整监督级别和初始计划选择。

Result: 在微软ExCyTIn-Bench基准测试中，使用GPT-5-mini作为学生，ATLAS达到54.1%成功率，比GPT-5（High）高13%，成本降低86%。跨事件验证显示泛化能力：冻结的指导手册将准确率从28%提升至41%。

Conclusion: 梯度无关持续学习是实现自适应、可部署AI系统的可行路径，系统级编排比参数更新更有效，提供了可用于训练显式世界模型的因果注释轨迹。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [366] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 提出了一种基于Kolmogorov-Arnold网络的温和生成变体神经网络算子模型，用于求解具有随机终止时间的二阶倒向随机微分方程(2BSDEs)无限族问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络算子模型在处理无限族二阶倒向随机微分方程时存在局限性，需要开发更高效的近似方法来解决这类复杂问题。

Method: 利用Kolmogorov-Arnold网络构建生成变体神经网络算子模型，在规则有界欧几里得域上求解具有随机终止时间的2BSDEs无限族。

Result: 证明了对于广泛的2BSDE族，解算子可以通过适当的神经网络算子模型进行近似；识别出一个结构化子类，其神经网络算子近似仅需多项式数量的参数。

Conclusion: 该方法相比一般最坏情况下的神经网络算子保证，显著降低了参数需求，从指数级降低到多项式级，提高了计算效率。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [367] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出了一种新的搜索方向，使一阶和零阶随机在线双层优化算法无需窗口平滑即可实现次线性随机双层遗憾。


<details>
  <summary>Details</summary>
Motivation: 当前在线双层优化方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时无法准确反映系统性能。

Method: 引入新颖搜索方向，减少超梯度估计的oracle依赖，同时更新内外层变量和线性系统解，使用零阶方法估计Hessian、Jacobian和梯度。

Result: 实验验证了在线参数损失调优和黑盒对抗攻击中的有效性。

Conclusion: 提出的框架在保证次线性随机双层遗憾的同时，显著提高了在线双层优化的效率。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [368] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 使用几何不变量理论研究深度线性网络，通过Kempf-Ness定理证明L2正则化器在平衡流形上最小化，将训练动态分解为纤维上的正则化流和平衡流形上的学习流，正则化流可通过矩映射精确求解。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡性提供一个统一的数学框架，并基于该框架从模型简化和贝叶斯原理的角度解释平衡性。

Method: 应用几何不变量理论和Kempf-Ness定理，将训练动态分解为两个梯度流：纤维上的正则化流和平衡流形上的学习流，其中正则化流使用矩映射精确求解。

Result: 建立了L2正则化器在平衡流形上最小化的理论结果，实现了训练动态的分解，并提供了正则化流的精确解。

Conclusion: 该方法为深度学习和线性系统理论中的平衡性概念提供了统一的数学解释框架，有助于从模型简化和贝叶斯角度理解平衡性。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [369] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出一个结合元学习和领域适应的统一框架，使自动调制分类系统能够抵御对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习在单一源域上训练模型，使其能泛化防御未见过的攻击；在线阶段应用领域适应使模型适应新目标域，无需大量标注数据。

Result: 该框架显著提高了调制分类在对抗攻击和环境变化下的准确性。

Conclusion: 该框架为现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [370] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文研究了多处理场景下的提升建模，提出了正交函数适应(OFA)方法，相比现有的结构适应和特征适应方法，在估计能力和鲁棒性方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前多处理提升建模技术通常从二值处理工作改编而来，但在各种数据特征（噪声数据、混合观测数据等）下无法保持有效性。

Method: 提出了基于函数逼近定理的正交函数适应(OFA)方法，将现有模型适应技术分为结构适应和特征适应两类。

Result: 实验结果表明，OFA相比其他基础适应方法能显著提升提升模型性能，并展现出最高的鲁棒性。

Conclusion: OFA方法在多处理提升建模中具有优越的估计能力和鲁棒性，是处理各种数据特征的有效解决方案。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [371] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: 该论文研究了思维链(CoT)是否能增强transformer在所有推理任务中的能力。通过分析CoT transformer的记忆能力，发现CoT并不能在所有推理任务中提升transformer的能力，给出了负面答案。


<details>
  <summary>Details</summary>
Motivation: 探索思维链(CoT)是否能在所有推理任务中增强transformer的能力，这是尚未解决的基本问题。

Method: 将transformer推理视为记忆问题，分析固定精度transformer（有/无CoT）的记忆能力，给出记忆有限推理数据集的充要条件和参数数量界限。

Result: 发现CoT和无CoT transformer记忆有限数据集的充要条件互不包含，参数数量界限均为Θ̄(N)。证明存在CoT无法增强transformer推理能力的任务。

Conclusion: CoT并不能在所有推理任务中增强transformer的能力，给出了负面答案。同时发现某些简单无限数据集无法被有/无CoT的transformer记忆。

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [372] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出一个基于多任务RF信号分类的鲁棒框架，用于共享频谱环境中的发射机识别和协议分类，使用CNN处理信号重叠和环境变化挑战，在POWDER平台数据上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 随着频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、提高频谱利用效率和保障网络安全变得不可或缺。

Method: 设计卷积神经网络(CNN)，采用多通道输入策略提取有意义的信号特征，解决信号特征重叠和环境变化等关键挑战。

Result: 在POWDER平台RF数据上取得显著准确率：协议分类90%，发射基站分类100%，联合分类任务92%。

Conclusion: 该方法在增强现代无线网络中的频谱监测、管理和安全方面具有显著潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [373] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC评估协议，用于确保测试时计算方法的公平评估，不受LLM性能和API成本波动的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效，因此需要一种公平的评估方法。

Method: 设计FEval-TTC协议，标准化少样本提示和答案提取过程，支持跨多个LLM和多种数学及常识推理数据集的评估，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC为测试时计算方法的评估提供了标准化和公平的基准，有助于更可靠的研究比较。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [374] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，通过混合奖励函数和双Q网络来应对现实世界的不确定性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速增长需要优化充电站布局，但现有强化学习方法因确定性奖励系统无法充分应对动态现实条件，导致评估成本高且不反映真实场景。

Method: 集成深度强化学习与基于代理模拟，使用具有双Q网络的混合RL代理来选择最优位置和配置充电端口，采用结合确定性因素和模拟反馈的混合奖励函数。

Result: 在越南河内的案例研究中，该方法相比初始状态将平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展的自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [375] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统化数据集生成和对称感知图神经网络的机器学习框架，用于高效预测建筑风荷载，替代昂贵的传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统风洞测试和大涡模拟(LES)成本高昂，每个LES案例需要至少24小时计算，难以进行大规模参数研究。

Method: 1) 通过符号距离函数插值生成屋顶几何形状，模拟462个不同形状和风向的LES案例构建大规模数据集；2) 开发反射等变图神经网络，保证在镜像几何下的物理一致性预测。

Result: 在插值和外推评估中，WindMiL对表面压力系数的均值和标准差均达到高精度(RMSE ≤ 0.02)，在反射测试中保持96%以上的命中率，而非等变基线模型下降超过10%。

Conclusion: 通过结合系统化数据集和等变代理模型，WindMiL实现了对建筑风荷载的高效、可扩展和准确预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [376] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 变量消除算法通过重塑优化景观，将原始问题中的鞍点转化为简化问题中的局部最大值，从而有效规避鞍点障碍，提升非凸优化的收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解变量消除算法在实践中表现出的优越收敛性和鲁棒性背后的原理，特别是在处理大规模非凸优化中的鞍点问题时。

Method: 基于Hessian惯性和Schur补的严格几何分析，比较原始和简化公式的优化景观，证明变量消除如何重塑临界点结构。

Result: 证明局部最大值在简化景观中是由原始公式中的鞍点创建并直接对应的，在非凸矩阵分解、二维参数神经网络和深度残差网络训练中验证了方法的有效性。

Conclusion: 通过鞍点变换实现景观简化是一个强大的设计原则，可以指导开发更鲁棒和高效的优化算法。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [377] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: KAT-GNN是一个结合临床知识和时序动态的图神经网络框架，用于电子健康记录的风险预测，在冠状动脉疾病和院内死亡率预测任务上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据具有异构性和不规则时序特性，这给临床风险预测带来挑战。需要有效整合临床知识和时序动态来提升预测性能。

Method: 构建模态特定的患者图，通过SNOMED CT本体和共现先验进行知识增强，使用时序感知transformer捕捉纵向动态。

Result: 在CAD预测中AUROC达0.9269±0.0029，在MIMIC-III和MIMIC-IV死亡率预测中分别达到0.9230±0.0070和0.8849±0.0089，显著优于基线方法。

Conclusion: 将临床知识整合到图表示中，结合时序注意力机制，为不同临床任务和数据集的风险预测提供了有效且可泛化的方法。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [378] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的在线鲁棒张量恢复算法，用于智能交通系统中的交通数据质量增强，能够同时处理缺失值和异常值，在保持高恢复精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批量处理方法需要大量计算和存储资源，限制了在持续增长的交通数据量下的可扩展性；现有的在线张量恢复方法在复杂现实场景中性能下降严重，未能充分利用交通数据的内在结构特性。

Method: 将交通数据恢复问题重新表述为流式框架，提出同时利用交通数据全局时空相关性和局部一致性的在线鲁棒张量恢复算法。

Result: 在三个真实交通数据集上的实验结果表明，该方法实现了高恢复精度，同时与最先进的批量方法相比，计算效率提高了三个数量级。

Conclusion: 该方法作为智能交通系统中交通数据质量增强的可扩展且有效的解决方案具有巨大潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [379] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一种对抗性时空注意力网络，用于从多变量EEG信号预测癫痫发作，通过级联注意力块联合建模空间脑连接和时间神经动态，实现高灵敏度、低误报率和个体特异性适应。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中面临关键挑战，需要高灵敏度、低误报率和个体特异性适应能力。现有方法假设固定的发作前持续时间或分别处理时空特征，无法有效捕捉时空模式之间的双向依赖关系。

Method: STAN通过级联注意力块与交替的时空模块联合建模空间脑连接和时间神经动态。采用带梯度惩罚的对抗训练，从明确定义的15分钟发作前窗口学习发作间期和发作前状态的鲁棒区分。

Result: 在两个基准EEG数据集上实现最先进性能：CHB-MIT头皮数据集96.6%灵敏度、0.011次/小时误报；MSSM颅内数据集94.2%灵敏度、0.063次/小时误报。计算效率高（230万参数，45毫秒延迟，180MB内存），适合实时边缘部署。

Conclusion: STAN能够捕捉细微的发作前动态，无需个体化训练即可实现早期检测。该框架为医疗和其他时间序列领域的时空预测提供通用范式，其中个体异质性和可解释性至关重要。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [380] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发了一个轻量级1D CNN模型CaptureNet-Deep，用于自动检测纳米孔蛋白质测序中的捕获阶段，将分析时间从几天缩短到30分钟以内。


<details>
  <summary>Details</summary>
Motivation: 纳米孔蛋白质测序产生长而嘈杂的电流轨迹，其中捕获阶段需要专家手动识别，耗时数天且需要领域专业知识。

Method: 使用轻量级一维卷积神经网络(1D CNN)在降采样信号窗口中检测捕获阶段，并与CNN-LSTM混合模型、基于直方图的分类器和其他CNN变体进行比较。

Result: 最佳模型CaptureNet-Deep在保留测试数据上达到F1分数0.94和精度93.39%，支持低延迟推理并集成到实验仪表板中。

Conclusion: 使用简单可解释的架构可以实现高效的实时捕获检测，轻量级ML模型在测序工作流程中具有更广泛的应用潜力。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [381] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出一种基于归纳偏置的神经控制李雅普诺夫函数框架，通过将李雅普诺夫条件作为指导知识来设计神经网络和控制器，实现端到端学习，相比现有方法具有更高的收敛率和更大的吸引域。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习-验证框架的控制李雅普诺夫函数方法将李雅普诺夫条件作为复杂约束进行优化，难以实现全局收敛，且验证过程过于复杂。

Method: 将李雅普诺夫条件作为归纳偏置，设计神经CLF和基于CLF的控制器，实现稳定优化过程和端到端学习。

Result: 在大量实验案例中，相比现有方法，该方法在CLF学习中获得更高的收敛率和更大的吸引域。

Conclusion: 该方法成功揭示了先前方法在学习过程中成功率下降的原因，并通过归纳偏置方法有效解决了这一问题。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [382] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 本文探索使用数据驱动的Koopman方法来建模无人机轨迹动态，以解决飞行自组织网络(FANETs)中的通信挑战。提出了集中式和分布式两种方法，能够准确预测连接和隔离事件，帮助无人机基于预测调度传输。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在静态无线环境中表现良好，但在高度动态的FANETs环境中存在局限性。需要能够处理网络拓扑不断变化的新方法。

Method: 利用Koopman算子理论，提出集中式和分布式两种方法建模无人机轨迹动态，预测信号干扰加噪声比(SINR)来确保无人机间可靠通信。

Result: 这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机传输调度提供依据。

Conclusion: 数据驱动的Koopman方法能够有效应对FANETs的动态环境挑战，提高网络性能，为下一代AI无线网络提供有力工具。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [383] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过局部敏感哈希将高维梯度压缩为二进制表示，在保护隐私的同时有效检测恶意梯度，显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在信任缺失环境中易受推理攻击和投毒攻击，现有防御方法存在通信计算成本高、检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑的二进制表示，仅通过哈希形式检测过滤恶意梯度。

Result: 实验表明LSHFed在50%参与者为恶意攻击者时仍能保持高模型性能，梯度验证通信量相比全梯度方法降低1000倍。

Conclusion: LSHFed同时增强了联邦学习的聚合鲁棒性和隐私保护，实现了通信效率与安全性的平衡。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [384] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出基于去噪扩散概率模型(DDPM)的云原生网络功能(CNF)放置新框架，将放置问题重新概念化为图到分配任务，通过图神经网络去噪器迭代优化分配矩阵，实现快速可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法（混合整数非线性规划、启发式算法、强化学习）在可扩展性、约束处理和泛化能力方面存在局限，需要新的解决方案来应对5G/6G网络中CNF在云连续体上的放置挑战。

Method: 将CNF放置问题编码为异构图，训练图神经网络去噪器迭代优化噪声CNF到云的分配矩阵，在损失函数中直接整合约束特定损失以学习可行解空间。

Result: 在多种拓扑结构上的广泛评估证实，该模型能持续产生可行解，推理速度比MINLP求解器快数个数量级。

Conclusion: 基于扩散的生成建模在约束网络嵌入问题中具有巨大潜力，为分布式云原生网络功能的实际可扩展编排做出了贡献。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [385] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出MiniFool算法，这是一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体物理中的神经网络分类任务。该算法通过最小化结合χ²检验统计量和目标分数偏差的成本函数来工作，可量化网络决策的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发一种通用的对抗攻击算法来测试神经网络在粒子物理和天体物理分类任务中的鲁棒性，特别是在IceCube中微子观测站的天体物理τ中微子搜索等应用场景中。

Method: 基于最小化成本函数的方法，该函数结合了基于χ²的检验统计量（量化基于实验不确定性的扰动概率）与期望目标分数的偏差。通过攻击参数缩放实验不确定性来测试分类变化。

Result: 研究发现，对于正确分类和错误分类的事件，分类翻转的可能性不同。通过调整攻击参数可以量化网络决策的鲁棒性，并能测试未标记实验数据的分类鲁棒性。

Conclusion: MiniFool算法具有通用适用性，能够有效测试神经网络分类器在粒子物理和天体物理应用中的鲁棒性，为实验数据的可靠性评估提供了有力工具。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [386] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出了一种可验证的分割学习框架，通过集成zk-SNARK证明来确保分割学习中计算结果的正确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能实现数据分离情况下的协同训练，但缺乏验证计算正确性和诚实性的能力。

Method: 在服务器端的前向传播和反向传播中为双方生成zk-SNARK证明和验证，确保双方的可验证性。

Result: 与仅记录更新但不生成零知识证明的区块链系统相比，zk-SNARK测试实现了可验证性和正确性，而区块链虽然轻量但不可验证。

Conclusion: 应用zk-SNARK证明能够有效解决分割学习中的可验证性问题，确保计算过程的正确性和诚实性。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [387] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出一种基于重参数化的多模态强化学习算法，通过距离多样性正则化平衡性能、决策多样性和效率，在多样性关键场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂的多模态决策分布，这限制了其在多样性关键场景中的性能。现有基于扩散或摊销策略的多模态RL方法存在计算不可行的问题。

Method: 首先将现有不可行多模态策略统一框架化，证明可通过重参数化直接使用策略梯度优化；然后提出基于距离的多样性正则化方法，无需显式计算决策概率；在多样性关键领域（多目标达成和生成RL）验证方法有效性。

Result: 在多样性关键场景中表现出优势，特别是在少样本鲁棒性方面；在传统MuJoCo基准测试中也显示出竞争力；实验表明摊销策略是具有强大多模态表达能力和高性能的有前景策略模型类别。

Conclusion: 提出的方法成功解决了多模态RL中性能、多样性和效率的平衡问题，摊销策略被证明是有效的多模态策略表示方法。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [388] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 应用遗忘技术到LeNet神经网络，评估遗忘FGSM攻击的效果，发现能显著提升模型对抗此类攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易受到对抗性攻击，特别是FGSM攻击，需要开发方法来提高模型对这些攻击的鲁棒性。

Method: 使用机器遗忘技术，在LeNet神经网络上应用遗忘方法，从训练数据中移除FGSM攻击添加的扰动。

Result: 遗忘FGSM攻击能显著提高LeNet网络对抗此类攻击的鲁棒性。

Conclusion: 机器遗忘技术是提高神经网络对抗FGSM攻击鲁棒性的有效方法。

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [389] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了首个实域完全原位FFT框架(rdFFT)，通过利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，消除中间缓存使用，实现输入输出内存空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的FFT实现（包括标准FFT和实FFT）无法实现真正的原位计算，特别是实FFT将大小为n的输入映射到大小为n/2+1的复数输出，导致维度不匹配并需要额外的内存分配。

Method: 利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，完全消除中间缓存使用，实现实域完全原位FFT计算。

Result: 在多个自然语言理解任务上的实验证明该方法能有效降低训练内存成本。

Conclusion: 该方法为频域轻量级适配提供了一个有前景的方向。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [390] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出基于图的AlphaEarth卫星嵌入方法，通过建模空间关系和引入模糊标签损失来解决DHS调查坐标位移问题，提高撒哈拉以南非洲地区财富指数的预测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区缺乏准确、细粒度的贫困地图，DHS调查数据虽然质量高但空间覆盖有限且坐标因隐私保护被随机位移，降低了数据质量。

Method: 使用低维AlphaEarth卫星嵌入，通过图结构建模调查点和未标记位置的空间关系，引入概率性模糊标签损失来处理坐标位移问题。

Result: 在37个DHS数据集上的实验表明，相比仅使用图像的基线方法，结合图结构能略微提高预测准确性。

Conclusion: 紧凑的地球观测嵌入在大规模社会经济制图方面具有潜力，图结构方法能够改善财富预测的泛化能力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [391] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了CG-FKAN方法，通过稀疏化和压缩扩展网格来降低联邦学习中KAN模型的通信开销，在通信受限环境下比固定网格KAN降低13.6%的RMSE。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感应用中广泛使用，但可解释性有限。KAN通过可学习的样条函数解决了这一限制，但现有研究忽略了网格扩展带来的通信开销问题。

Method: 提出CG-FKAN方法，在通信预算下通过稀疏化并仅传输必要的系数来压缩扩展网格。

Result: 实验显示CG-FKAN在通信受限环境下比固定网格KAN的RMSE降低达13.6%。

Conclusion: CG-FKAN有效解决了KAN在联邦学习中的通信开销问题，并推导了其近似误差的理论上界。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [392] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 该论文提出了一种新的曲率度量方法——曲率率λ，通过输入空间的高阶导数指数增长率来量化神经网络的功能平滑度，相比传统参数空间度量更易解释且不受参数化影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数空间的锐度度量（如Hessian特征值）计算昂贵、对参数化敏感且难以在功能层面解释，需要一种直接在输入空间定义的曲率度量方法。

Method: 引入曲率率λ作为高阶输入导数的指数增长率，通过log ||D^n f||与n的斜率进行经验估计，并提出了曲率率正则化(CRR)来直接塑造输入空间几何。

Result: 在解析函数和神经网络上的实验表明，λ在训练过程中可预测地演化，CRR与SAM相比能达到相似精度，同时产生更平坦的输入空间几何和更好的置信度校准。

Conclusion: 曲率率λ通过微分动力学将曲率概念基础化，为学习模型的功能平滑度提供了一个紧凑、可解释且参数化不变的描述符。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [393] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率，用于替代计算复杂度高的Ollivier-Ricci曲率，在保持几何表达能力的同时大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Ollivier-Ricci曲率虽然具有很好的几何可解释性，但其极高的计算复杂度限制了在大规模图数据集上的应用。

Method: 使用节点对之间的有效电阻来量化图边上消息传递的难易程度，替代最优传输距离，从而定义有效电阻曲率。

Result: 在多种GNN任务上的实验表明，该方法与Ollivier-Ricci曲率性能相当，同时显著降低了计算开销。

Conclusion: 有效电阻曲率是Ollivier-Ricci曲率的有效替代方案，在保持几何表达能力的同时实现了更高的计算效率。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [394] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 提出了DAMBench，这是第一个大规模多模态基准测试，用于在真实大气条件下评估数据驱动的数据同化模型，解决了现有研究中依赖简化场景和缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法虽然有效，但深度学习提供了更可扩展、高效和灵活的替代方案。然而，现有深度学习数据同化研究存在两个关键限制：(1)依赖过度简化的合成观测场景；(2)缺乏标准化基准进行公平模型比较。

Method: 构建DAMBench基准，整合来自先进预报系统的高质量背景状态和真实世界的多模态观测数据（气象站和卫星图像），所有数据重采样到统一网格并进行时间对齐，提供统一评估协议并基准测试代表性方法。

Result: 建立了严谨的研究基础，促进可重复性、公平比较和扩展到真实世界多模态场景的能力，并展示了如何通过集成真实观测来增强简单基线模型。

Conclusion: DAMBench为未来研究提供了系统性的评估框架，解决了数据同化领域的关键挑战，数据集和代码已公开可用。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [395] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在Intel Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在边缘设备上实现了高效且无灾难性遗忘的持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI设备在开放世界中面临的数据分布变化和新类别出现的挑战，同时克服传统离线训练在功耗受限环境中的局限性，实现高效的在线持续学习。

Method: 采用脉冲神经网络架构，包含三个创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则维持权重归一化、集成神经发生和元可塑性实现容量扩展和遗忘缓解。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN达到与回放方法相当的准确率且无需排练。相比边缘GPU上的最佳OCL方法，速度提升70倍（0.33ms vs 23.2ms），能效提升5600倍（0.05mJ vs 281mJ）。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够突破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [396] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: MS-HGFN是一种多尺度层次图融合网络，通过动态学习股票内部属性和跨股票特征的模式，结合自上而下的门控机制，有效提升股票市场预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图神经网络在股票预测中忽视了两个关键问题：股票内部属性模式对跨股票相关性的影响，以及多尺度采样中对粗粒度和细粒度特征的偏置关注。

Method: 提出MS-HGFN模型，包含层次GNN模块学习不同时间尺度下的内部属性模式和跨股票特征，形成动态图；采用自上而下的门控方法整合多尺度时空特征。

Result: 在美国和中国股票市场的真实数据集实验中，MS-HGFN优于传统和先进模型，预测精度提升达1.4%，且在收益模拟中表现出更强的稳定性。

Conclusion: MS-HGFN通过全面捕捉时空依赖关系和多尺度特征融合，显著提升了股票市场预测的准确性和稳定性。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [397] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的特征提取方法，使用Hadamard矩阵的列或行向量作为不同长度的卷积核，在保持与现有方法兼容的同时提升计算效率、鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA时间序列分类方法计算复杂度高、参数调优和训练周期长，而轻量级方法如ROCKET在核选择和计算开销方面仍有改进空间。

Method: 使用Hadamard矩阵的列或行向量作为不同长度的卷积核进行特征提取，利用核的正交性提升性能。

Result: 在UCR时间序列数据集上，F1分数比ROCKET提升至少5%，训练时间比最快的miniROCKET缩短50%，可在超低功耗嵌入式设备上部署。

Conclusion: 提出的Hadamard卷积变换方法在保持兼容性的同时，显著提升了时间序列分类的计算效率和性能。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [398] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出了并行解耦框架(PDF)，通过利用MLLMs的可控性生成多个并行嵌入，解决了传统SSC范式将丰富多模态输入压缩为单一嵌入的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型受限于SSC范式（单一输入、单一嵌入、对比监督），无法充分利用MLLMs的能力，将多方面的输入压缩为单一嵌入表示。

Method: 使用共享MLLM骨干网络，通过不同的可学习前缀生成多个并行路径，采用互信息最小化(MIM)约束确保并行多样性，并结合每路径对比监督保持语义对齐。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升8.9%(7B)，VLM2Vec-Qwen2VL模型分别提升4.2%(2B)和3.1%(7B)。2B模型仅用一半计算预算就超越基线2.6%。

Conclusion: PDF框架能够生成多样化的并行嵌入，提供鲁棒的语义覆盖和可泛化的嵌入空间，推理时仅需单次前向传播，计算开销可忽略。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [399] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 提出了一种物理信息机器学习框架，通过专门设计的输入空间将领域知识嵌入到机器学习中，显著提高了航空航天复合材料冲击能量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前冲击能量预测方法受限于数据稀疏性、信号噪声、复杂特征依赖关系、非线性动力学、巨大设计空间以及逆问题的不适定性，需要开发更准确可靠的预测方法。

Method: 结合观测偏差和针对性特征选择，从时域、频域和时频域提取特征，通过统计显著性、相关性过滤、降维和噪声鲁棒性的结构化特征选择过程，生成紧凑的能量敏感指标集，然后训练全连接神经网络。

Result: 该方法显著提高了冲击能量预测精度，与传统时间序列技术和纯数据驱动模型相比，误差减少了三倍，在原始和损伤状态下的多种冲击场景中均表现良好。

Conclusion: 所提出的物理信息框架能够产生统计鲁棒且物理意义明确的冲击能量预测，保持可解释性并能够追溯到可测量的结构响应，为复合材料冲击监测提供了有效解决方案。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [400] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 本文重新审视了托普利兹协方差估计问题，通过过参数化梯度下降方法，证明了当使用适度过参数化时，可以从随机初始化实现全局收敛，并提出了一种加速梯度下降变体。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用复杂优化方法来最大化高斯对数似然，而深度学习研究表明简单梯度下降在过参数化模型中具有强大能力，因此重新审视托普利兹协方差估计问题。

Method: 将P×P协方差建模为K个复正弦波的和，通过梯度下降优化参数。研究了K=P时的收敛问题，并提出了适度过参数化(K=2P或4P)和加速梯度下降变体。

Result: 当K=P时梯度下降可能收敛到次优解，但适度过参数化(K=2P或4P)能够从随机初始化实现全局收敛。当频率固定仅优化幅度时，优化景观渐近良性，任何驻点都能恢复真实协方差。数值实验表明该方法在挑战性设置下能达到或超过最先进方法的精度。

Conclusion: 过参数化梯度下降为托普利兹协方差估计提供了一种简单、可扩展且有效的替代方案，能够匹配或超越复杂优化方法的性能。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [401] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理的图链式思维系统，通过分解推理任务、优化LLM服务架构，显著提升了图结构知识推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的图链式思维方法存在准确性低、令牌使用过多、延迟高和吞吐量低的问题，主要由于单代理整体提示、重复上下文重新编码和低效的服务执行。

Method: 将推理分解为分类、推理、动作生成和图检索等专门代理，实现分支和选择性上下文共享；引入图感知的LLM推理机制，包括图特定KV缓存管理、基于优先级的淘汰和流水线执行。

Result: GLM将答案准确性提升高达38%，令牌成本降低95.7%，推理延迟降低90.3%，吞吐量提高15.1倍。

Conclusion: GLM系统通过多代理架构和优化的LLM服务设计，实现了高效的大规模复杂现实世界推理。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [402] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet是一种用于多类别多值处理效应估计的网络架构，通过交叉效应估计模块和动态掩码机制捕捉处理交互，无需限制性结构假设。


<details>
  <summary>Details</summary>
Motivation: 反事实因果推断在多类别、多值处理场景下面临重大挑战，现有方法局限于二元或单一类型处理，存在假设限制、可扩展性不足和评估框架不完善等问题。

Method: 提出XTNet架构，采用交叉效应估计模块和动态掩码机制，通过分解策略将基本效应与交叉处理交互分离，有效建模组合处理空间。

Result: 在合成和真实数据集上的广泛实验表明，XTNet在排序准确性和效应估计质量方面始终优于现有最优基线方法，真实A/B测试结果进一步证实其有效性。

Conclusion: XTNet为复杂干预场景下的多类别多值处理效应估计提供了有效的解决方案，并通过MCMV-AUCC评估指标考虑了处理成本和交互效应。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [403] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出基于卡尔曼滤波的贝叶斯自然梯度下降方法，用于CLIP模型的小样本微调，在保持ID性能的同时提升OOD鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于一阶梯度的方法在CLIP模型微调中存在收敛慢、对超参数敏感、OOD泛化差等问题，而二阶方法能利用损失函数曲率信息提供更有效的更新

Method: 使用卡尔曼滤波器对自然梯度下降进行贝叶斯近似，结合二阶优化和贝叶斯推断的优势，避免计算昂贵的Fisher信息矩阵逆

Result: 在多个图像分类数据集上的实验表明，该方法在ID性能上达到或超过SOTA基线，同时显著提升了OOD鲁棒性

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调，为视觉语言任务提供了更鲁棒高效的学习方法

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [404] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出一个联合优化用户关联和资源分配的统一框架，支持高效的并行推测解码，在移动边缘计算系统中实现可扩展和低延迟的LLM服务。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，需要高效的移动边缘计算解决方案。推测解码虽然前景广阔，但面临通信开销和异步延迟问题。

Method: 使用多智能体深度强化学习算法解决用户关联和资源分配问题，通过Sionna模拟器在现实条件下评估方法。

Result: 实验结果显示，该方法在不影响推理准确性的前提下，端到端延迟最高减少28.0%，平均减少23.7%。

Conclusion: 该方法能够实现移动边缘计算系统中可扩展和低延迟的LLM服务。

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [405] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度波动的边缘环境中联邦学习的模型准确性与公平性之间的权衡，通过实证评估比较了公平性客户端选择算法与随机、贪婪选择在公平性、模型性能和时间方面的表现。


<details>
  <summary>Details</summary>
Motivation: 边缘环境的固有波动性（动态资源可用性和异构客户端能力）对实现高精度和公平的客户端参与提出了重大挑战，需要研究准确性与公平性之间的基本权衡。

Method: 在三个基准数据集（CIFAR10、FashionMNIST和EMNIST）上对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪客户端选择进行了广泛的实证评估。

Result: 结果表明，更公平的客户端选择算法虽然能为客户端提供稍好的参与机会，但在波动环境中会导致全局训练速度变慢。

Conclusion: 这项工作揭示了在波动边缘环境中公平性与性能、公平性与速度之间的权衡，并探索了解决联邦学习中公平客户端选择策略现有缺陷的未来研究方向。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [406] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 提出一种分布式学习方法，通过交换合成数据而非模型参数来处理异构本地模型和数据，基于博弈论原理将本地模型学习制定为合作游戏，证明了指数族本地模型存在唯一纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中处理异构本地模型和数据的挑战，利用生成模型的成功，通过交换合成数据而非共享模型参数来处理模型和数据异质性。

Method: 将本地模型视为"黑盒"，能够从数据学习参数并生成数据；如果本地模型支持半监督学习，可扩展到不同概率空间处理多模态异构数据；基于博弈论原理将本地模型学习制定为合作游戏。

Result: 证明了指数族本地模型存在唯一纳什均衡，提出的学习方法收敛到该均衡；在标准基准视觉数据集上的图像分类和条件生成任务中展示了方法的优势。

Conclusion: 通过交换合成数据处理分布式学习中的异构性问题是有效的，基于博弈论的合作学习方法在异构模型和数据场景下具有良好性能。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [407] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出了HyperNQ，第一个基于超图神经网络的QLDPC解码器，通过利用超边捕捉高阶稳定子约束，在伪阈值区域下将逻辑错误率比BP提高84%，比GNN策略提高50%。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要有效的纠错策略来减轻噪声和退相干。QLDPC码通过支持恒定速率编码和稀疏奇偶校验结构，成为可扩展量子纠错的有前景解决方案。但传统BP解码在短循环存在时收敛性差，而GNN仅限于Tanner图上的成对交互，无法捕捉高阶相关性。

Method: 提出HyperNQ，首个基于超图神经网络的QLDPC解码器，利用超边捕捉高阶稳定子约束，采用两阶段消息传递方案，在伪阈值区域进行评估。

Result: 在伪阈值标记以下，HyperNQ将逻辑错误率比BP提高84%，比基于GNN的策略提高50%，表现出优于现有最先进解码器的性能。

Conclusion: HyperNQ通过超图神经网络捕捉高阶稳定子约束，实现了高度表达性和紧凑的解码，显著提升了QLDPC码的解码性能。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [408] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了网络化专家混合系统（NMoE），通过客户端协作推理和联邦学习框架解决边缘设备训练大型AI模型的资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在移动边缘计算中的计算资源和训练数据需求与边缘设备有限的存储和计算能力之间存在冲突，这给在边缘训练和部署LAMs带来了重大挑战。

Method: 引入网络化专家混合系统（NMoE），客户端根据邻居的专业知识分发任务并聚合返回结果；提出结合监督学习和自监督学习的联邦学习框架，平衡个性化与泛化，同时保持通信效率和数据隐私。

Result: 通过广泛实验证明了所提出的NMoE系统的有效性，为NMoE训练算法提供了见解和基准。

Conclusion: NMoE系统能够有效解决边缘设备训练大型AI模型的资源限制问题，通过协作推理和联邦学习实现高效部署。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [409] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试平台，通过比较15种不同算法，结合物理和统计特征转换，以及贝叶斯优化超参数调优，实现了跨化学体系的异常检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和飞机等应用中至关重要，未检测到的异常可能引发安全隐患或昂贵的停机时间。

Method: 开发了OSBAD开源基准测试平台，包含15种统计、距离和机器学习算法；提出物理和统计特征转换工作流将集体异常分解为点异常；使用贝叶斯优化管道进行自动化超参数调优。

Result: 在涵盖液态和固态化学的数据集上验证了OSBAD的跨化学泛化能力，能够识别不同电化学系统中的不规则性。

Conclusion: OSBAD为开发安全、可扩展和可转移的电池分析异常检测工具建立了统一基础，强调了物理和统计特征工程以及概率超参数调优在安全关键能源系统中可信数据驱动诊断的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [410] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC是一种后训练方法，通过动态规则验证解决开放生成任务中评估规则过多的问题，使用LLM作为批评器动态识别最可能的失败模式，联合优化生成器和批评器。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样且隐含的评估规则，但规则数量过多导致验证成本过高且评估不完整，使得基于规则的强化学习后训练难以扩展。

Method: 使用LLM作为批评器动态识别最可能的失败模式，通过外部验证器验证，联合优化生成器和批评器，减少所需验证次数。

Result: 实验表明RLAC在文本生成中提高了事实准确性，在代码生成中提高了正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态批评器比固定批评器更有效，RLAC有潜力将RL后训练扩展到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [411] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种随机初始化门控稀疏适配器方法，通过全秩适配器、ReZero门控和迭代幅度剪枝来减少语言模型微调中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，提供不施加秩约束的稀疏自适应替代方案。

Method: 从随机初始化的全秩适配器开始，使用ReZero类似的门控机制，并通过迭代幅度剪枝进行稀疏化。

Result: 在Textual MNIST任务上，RIGSA相比4-bit QLoRA显示出更少的遗忘，特别是在GSM8k任务上表现更好，但与随机掩码性能相当。

Conclusion: RIGSA提供了一种有效的稀疏自适应方法，在减少灾难性遗忘方面优于QLoRA，为参数高效微调提供了新的选择。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [412] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够捕捉现实随机过程中的记忆效应、长程依赖性和反常扩散现象。


<details>
  <summary>Details</summary>
Motivation: 现实随机过程存在记忆效应、时间相关性、长程依赖性和反常扩散现象，这些在标准扩散或桥模型中由于使用布朗运动而无法捕捉。

Method: 利用最近提出的分数布朗运动马尔可夫近似(MA-fBM)，构建FDBM框架，保持分数布朗运动的非马尔可夫特性同时实现可处理的推理。扩展到Schrödinger桥问题并推导出学习非配对数据转换的原则性损失函数。

Result: 在蛋白质构象预测任务中，FDBM相比布朗运动基线获得了更低的Cα原子位置均方根偏差(RMSD)；在非配对图像翻译任务中获得了更低的Fréchet Inception距离(FID)。

Conclusion: FDBM在两个任务中都优于布朗运动基线，证明了其在捕捉现实随机过程复杂特性方面的有效性。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [413] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出了一种个性化核心集加权联邦学习方法，通过仅使用代表性数据点而非完整客户端数据进行训练更新，在保持最优泛化误差的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端训练整个数据集带来的计算负担问题，通过核心集选择减少通信和计算开销。

Method: 基于个性化核心集的加权联邦学习框架，每个客户端仅使用核心集代表性数据点进行训练更新，理论分析了泛化误差的上下界。

Result: 在多个基准数据集上相比随机采样方法有显著性能提升，在医疗数据集上优于基于子模优化的子集选择方法。

Conclusion: 智能选择训练样本能有效提升联邦学习性能，提出的方法在保持理论最优性的同时实现了实际性能增益。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [414] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出一种基于物理信息的神经场模型，使用多尺度傅里叶特征编码从稀疏噪声超声数据中估计血流，无需真实监督即可实现去噪和修复。


<details>
  <summary>Details</summary>
Motivation: 超声血流分析对疾病诊断很重要，但超声图像存在深度衰减问题，且现有EchoPIV技术在测量血流速度方面存在局限性。物理信息机器学习可以增强在噪声或不完整数据情况下的准确性和鲁棒性。

Method: 采用物理信息神经场模型结合多尺度傅里叶特征编码，从稀疏噪声超声数据中重建血流场，无需真实监督。

Result: 该模型在合成和真实数据集上均实现了低均方误差的去噪和修复效果，与参考流场和真实流量测量结果一致。

Conclusion: 将其他成像模态中有效的方法应用于超声血流重建，证明了物理信息神经场在医学血流重建中的潜力。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [415] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 提出了一种基于度量学习的无秩张量分解框架，用判别性相似度优化替代传统重建目标，在多个领域优于基线方法，特别适合小样本场景。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法基于重建和固定秩约束，难以捕捉高维数据的语义结构，需要一种能直接反映语义相似度的分解方法。

Method: 基于度量学习的无秩张量分解框架，通过优化三元组损失和多样性、均匀性正则化，学习数据驱动的嵌入空间，其中距离直接反映语义相似度。

Result: 在多个数据集（人脸识别、脑连接分析、模拟数据）上优于PCA、t-SNE、UMAP和传统张量分解方法，聚类指标显著提升，且在小训练集上表现优于基于Transformer的方法。

Conclusion: 度量学习为张量分析提供了新范式，优先考虑语义相关性而非像素级保真度，在数据稀缺场景下具有计算优势。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [416] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 该论文提出了一种基于域对抗ConvNeXt自编码器(A-CNT)的方法，用于在室内环境变化下实现鲁棒的恶意干扰源定位，解决了传统UWB定位方法在环境布局变化时性能严重下降的问题。


<details>
  <summary>Details</summary>
Motivation: UWB定位虽然能提供厘米级精度，但容易受到干扰攻击，且在室内环境布局变化时定位性能会严重下降。现有ML/DL方法在单个房间内定位恶意干扰源以及在变化的室内布局中定位干扰源的研究较少。

Method: 引入了两个新的UWB数据集（原始和修改的房间配置），提出了域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层来对齐跨域的CIR衍生特征，以减轻环境变化导致的性能下降。

Result: 在源数据集上，Random Forest获得最高F1-macro分数0.95，XGBoost获得最低平均欧几里得误差20.16cm。但在修改的房间布局中，XGBoost误差增加十倍至207.99cm。A-CNT框架将平均欧几里得误差降至34.67cm，比非对抗迁移学习提高77%，比最佳基线提高83%。

Conclusion: 域对抗特征对齐能够在环境变化的情况下实现鲁棒且可迁移的室内干扰源定位，为智能建筑中的资产跟踪和入侵检测提供了更安全的解决方案。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [417] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 该论文研究了科学机器学习中数据保真度与计算成本之间的权衡，通过重新构建经典缩放定律，将数据集轴分解为计算预算和数据集组成，揭示了计算-性能缩放行为，并展示了预算依赖的最优保真度混合。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习通常受限于通过数值模拟生成训练数据的高昂成本，但通过调整建模假设和近似，可以权衡模拟保真度和计算成本，这是其他领域所没有的特性。

Method: 使用低保真度和高保真度的雷诺平均纳维-斯托克斯（RANS）模拟来研究神经代理模型中的数据保真度与成本权衡，重新构建经典缩放定律，将数据集轴分解为计算预算和数据集组成。

Result: 实验揭示了计算-性能缩放行为，并展示了在给定数据集配置下预算依赖的最优保真度混合。

Conclusion: 这是首个针对多保真度神经代理数据集的经验缩放定律研究，为科学机器学习中计算高效的数据集生成提供了实际考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [418] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在顺序微调时的灾难性遗忘问题，无需同时访问所有任务数据，保持基础能力的同时提升专业任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在顺序微调新任务时会出现灾难性遗忘，传统多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型（2B和8B参数）上实现新任务集成，同时保留预训练获得的基础知识。

Result: 路由方法在ChartQA、MMBench、DocVQA等通用基准上保持性能，同时提升专业任务准确率，无需所有任务数据并发访问。

Conclusion: 路由机制具有良好可扩展性和鲁棒性，特别适用于语义相关的新任务，并能实现跨模态知识迁移，优于现有持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [419] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 该论文提出时间特征分析作为新的可解释性方法，通过将语言模型表示分解为可预测部分和残差部分，更好地捕捉语言的时间动态特性，相比稀疏自编码器在处理时序结构方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但语言模型表示具有丰富的时间动态特性（如概念维度增长、上下文相关性和非平稳性），这与稀疏自编码器的时间独立性先验相冲突。

Method: 引入时间特征分析，将给定时间的表示分解为两部分：可从上下文推断的可预测分量，以及捕捉上下文无法解释的新信息的残差分量。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象慢变信息和新颖快变信息，而现有稀疏自编码器在上述任务中均存在显著缺陷。

Conclusion: 研究结果强调了在设计稳健可解释性工具时，需要与数据匹配的归纳偏置。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [420] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 该研究结合可解释机器学习和符号建模，通过Kolmogorov Arnold Networks (KANs)将水库水温预测的黑盒模型转化为透明解析表达式，在保持高预测精度的同时揭示物理驱动机制。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对水资源管理、生态系统健康和气候韧性至关重要，但单纯预测无法揭示控制物理过程。需要将预测精度与解释能力相结合，理解水库热动力学。

Method: 使用集成和神经网络模型（RF、XGBoost、MLP）进行预测，然后应用SHAP量化物理驱动因素的贡献，最后开发KANs将数据驱动洞察转化为紧凑解析表达式。

Result: 模型预测精度高（最佳RMSE=1.20°C，R²=0.97），KANs生成的10个渐进复杂方程从R²=0.84（单预测因子）提升到R²=0.92（10个预测因子），深度是关键次要预测因子，降水影响有限。

Conclusion: 该框架通过KANs和可解释机器学习将黑盒模型转化为透明替代模型，在保持预测准确性的同时增强对水库热动力学的理解，平衡了简单性与准确性。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [421] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 本文提出了一个终身表示学习框架，通过多任务经验风险最小化算法和任务规避维度概念，解决了在线学习中利用共享结构加速学习的问题。


<details>
  <summary>Details</summary>
Motivation: 终身学习需要学习者在面对连续任务时，识别并利用任务间的共享结构来加速学习。与多任务学习不同，终身学习要求在线地利用现有知识并持续收集部分信息。

Method: 提出了一个简单的算法，使用多任务经验风险最小化作为子程序，并引入了任务规避维度这一新概念来建立样本复杂度界限。

Result: 该方法适用于涉及一般函数类的广泛学习问题，并在噪声下的分类和回归任务中得到了具体实例化。

Conclusion: 所提出的终身表示学习框架和算法能够有效处理在线学习中的共享结构利用问题，具有广泛的应用前景。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [422] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种坐标上升算法，通过最大似然估计在监督方式下学习动态状态估计中的动态和测量模型，包括神经网络参数和噪声协方差矩阵，然后结合非线性卡尔曼滤波器进行状态估计。


<details>
  <summary>Details</summary>
Motivation: 传统动态状态估计方法通常假设模型已知，但在实际应用中动态和测量模型往往未知或难以精确建模，需要从数据中学习这些模型参数。

Method: 使用坐标上升算法进行最大似然估计，学习高斯假设下的动态和测量模型的神经网络参数以及噪声协方差矩阵，训练后结合非线性卡尔曼滤波器进行状态估计。

Result: 算法能够有效学习动态和测量模型参数，包括神经网络权重和噪声统计特性，为后续状态估计提供准确的模型基础。

Conclusion: 该方法为动态状态估计提供了一种从数据中学习模型参数的可行方案，结合了神经网络建模和传统滤波技术的优势。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [423] [Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images](https://arxiv.org/abs/2511.00702)
*Alberto Di Biase*

Main category: cs.GR

TL;DR: 该论文探索了将医学成像中的扩散张量成像（DTI）和纤维束追踪技术应用于艺术绘画渲染的方法，通过结构张量提供局部方向信息来放置画笔笔触。


<details>
  <summary>Details</summary>
Motivation: 将医学成像中的纤维追踪技术应用于艺术绘画渲染，探索跨领域的技术应用，模仿人类艺术家的绘画过程。

Method: 使用纤维束追踪算法，基于结构张量（而非梯度）提供更好的局部方向信息，来放置画笔笔触。

Result: 该方法在肖像和一般图像上进行了演示，成功实现了类似人类艺术家绘画过程的笔触放置。

Conclusion: 这项研究展示了扩散张量成像技术在图像绘画渲染中的跨领域应用潜力，建立了纤维追踪与画笔笔触放置之间的平行关系。

Abstract: Doctors and researchers routinely use diffusion tensor imaging (DTI) and
tractography to visualize the fibrous structure of tissues in the human body.
This paper explores the connection of these techniques to the painterly
rendering of images. Using a tractography algorithm the presented method can
place brush strokes that mimic the painting process of human artists,
analogously to how fibres are tracked in DTI. The analogue to the diffusion
tensor for image orientation is the structural tensor, which can provide better
local orientation information than the gradient alone. I demonstrate this
technique in portraits and general images, and discuss the parallels between
fibre tracking and brush stroke placement, and frame it in the language of
tractography. This work presents an exploratory investigation into the
cross-domain application of diffusion tensor imaging techniques to painterly
rendering of images. All the code is available at
https://github.com/tito21/st-python

</details>


### [424] [Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning](https://arxiv.org/abs/2511.00898)
*Heng Zhang,Jing Liu,Jiajun Wu,Haochen You,Lubin Gan,Yuling Shi,Xiaodong Gu,Zijian Zhang,Shuai Chen,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: DuoGLM是一个无需训练的双视角框架，通过局部关系感知模板和全局拓扑角色推理，解决LLM在图学习中结构重要节点性能下降的问题，在零样本节点分类和跨域迁移中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图拓扑编码为静态特征，但缺乏将拓扑模式转换为基于角色的解释的推理支架，导致在桥接节点和中心节点等结构重要节点上出现系统性性能下降。

Method: 提出DuoGLM框架：局部视角构建关系感知模板捕获节点与邻居的语义交互；全局视角执行拓扑到角色推理，生成结构位置的功能描述。两个互补视角提供显式推理机制。

Result: 在8个基准数据集上的实验显示，DuoGLM在零样本节点分类中准确率提升14.3%，在跨域迁移中AUC提升7.6%，显著优于现有方法。

Conclusion: 显式角色推理对于LLM的图理解是有效的，双视角框架能够区分拓扑相似但语义不同的节点。

Abstract: Large Language Models have emerged as a promising approach for graph learning
due to their powerful reasoning capabilities. However, existing methods exhibit
systematic performance degradation on structurally important nodes such as
bridges and hubs. We identify the root cause of these limitations. Current
approaches encode graph topology into static features but lack reasoning
scaffolds to transform topological patterns into role-based interpretations.
This limitation becomes critical in zero-shot scenarios where no training data
establishes structure-semantics mappings. To address this gap, we propose
DuoGLM, a training-free dual-perspective framework for structure-aware graph
reasoning. The local perspective constructs relation-aware templates capturing
semantic interactions between nodes and neighbors. The global perspective
performs topology-to-role inference to generate functional descriptions of
structural positions. These complementary perspectives provide explicit
reasoning mechanisms enabling LLMs to distinguish topologically similar but
semantically different nodes. Extensive experiments across eight benchmark
datasets demonstrate substantial improvements. DuoGLM achieves 14.3\% accuracy
gain in zero-shot node classification and 7.6\% AUC improvement in cross-domain
transfer compared to existing methods. The results validate the effectiveness
of explicit role reasoning for graph understanding with LLMs.

</details>


### [425] [G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning](https://arxiv.org/abs/2511.00911)
*Heng Zheng,Haochen You,Zijun Liu,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: 提出了G2rammar框架，为文本属性图显式编码结构和语义语法，通过两阶段学习提升语言模型对图结构的理解能力


<details>
  <summary>Details</summary>
Motivation: 现有方法将图结构线性化为标记序列时忽略了语法这一关键要素，而语法在自然语言中定义了单词的句法角色和功能。图中节点同样具有不同的结构角色（如枢纽、桥接点等），但当前方法缺乏对这些结构或语义角色的语法标注

Method: 提出双语语法框架G2rammar，包含结构语法（通过中心性和邻域模式表征拓扑角色）和语义语法（通过文本信息性捕捉内容关系）。采用两阶段学习：结构语法预训练后接语义语法微调

Result: 在真实世界数据集上的广泛实验表明，G2rammar通过为语言模型提供理解图结构所需的语法上下文，始终优于竞争基线方法

Conclusion: 显式编码结构和语义语法能够显著提升语言模型在图结构推理方面的能力，G2rammar框架为文本属性图的建模提供了更有效的语法感知方法

Abstract: Text-attributed graphs require models to effectively integrate both
structural topology and semantic content. Recent approaches apply large
language models to graphs by linearizing structures into token sequences
through random walks. These methods create concise graph vocabularies to
replace verbose natural language descriptions. However, they overlook a
critical component that makes language expressive: grammar. In natural
language, grammar assigns syntactic roles to words and defines their functions
within sentences. Similarly, nodes in graphs play distinct structural roles as
hubs, bridges, or peripheral members. Current graph language methods provide
tokens without grammatical annotations to indicate these structural or semantic
roles. This absence limits language models' ability to reason about graph
topology effectively. We propose \textbf{G2rammar}, a bilingual grammar
framework that explicitly encodes both structural and semantic grammar for
text-attributed graphs. Structural grammar characterizes topological roles
through centrality and neighborhood patterns. Semantic grammar captures content
relationships through textual informativity. The framework implements two-stage
learning with structural grammar pre-training followed by semantic grammar
fine-tuning. Extensive experiments on real-world datasets demonstrate that
G2rammar consistently outperforms competitive baselines by providing language
models with the grammatical context needed to understand graph structures.

</details>


### [426] [An Adjoint Method for Differentiable Fluid Simulation on Flow Maps](https://arxiv.org/abs/2511.01259)
*Zhiqi Li,Jinjin He,Barnabás Börcsök,Taiyuan Zhang,Duowen Chen,Tao Du,Ming C. Lin,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: 提出了一种基于双向流映射的可微分流体模拟伴随求解器，通过共享流映射实现前向模拟和反向梯度计算的统一，具有低内存和高精度的特点。


<details>
  <summary>Details</summary>
Motivation: 传统伴随方法需要区分中间数值步骤并存储中间变量，内存消耗大且计算复杂。本文旨在开发一种能够精确计算梯度、内存效率高的可微分流体模拟方法。

Method: 利用双向流映射，在正向传递中传输流体冲量变量，在反向传递中传播伴随变量。引入长-短时间稀疏流映射表示来演化伴随变量，避免存储中间变量。

Result: 在192^3分辨率下仅需6.53GB内存，同时保持涡度跟踪的高精度，能够精确识别、预测和控制涡旋动力学。

Conclusion: 该方法通过共享流映射实现了高效准确的可微分流体模拟，为需要精确涡旋动力学分析的应用提供了新的可能性。

Abstract: This paper presents a novel adjoint solver for differentiable fluid
simulation based on bidirectional flow maps. Our key observation is that the
forward fluid solver and its corresponding backward, adjoint solver share the
same flow map as the forward simulation. In the forward pass, this map
transports fluid impulse variables from the initial frame to the current frame
to simulate vortical dynamics. In the backward pass, the same map propagates
adjoint variables from the current frame back to the initial frame to compute
gradients. This shared long-range map allows the accuracy of gradient
computation to benefit directly from improvements in flow map construction.
Building on this insight, we introduce a novel adjoint solver that solves the
adjoint equations directly on the flow map, enabling long-range and accurate
differentiation of incompressible flows without differentiating intermediate
numerical steps or storing intermediate variables, as required in conventional
adjoint methods. To further improve efficiency, we propose a long-short
time-sparse flow map representation for evolving adjoint variables. Our
approach has low memory usage, requiring only 6.53GB of data at a resolution of
$192^3$ while preserving high accuracy in tracking vorticity, enabling new
differentiable simulation tasks that require precise identification,
prediction, and control of vortex dynamics.

</details>
